{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Aerospace Engineers\n",
    "### **Homework 1: Simple Linear Regression**\n",
    "\n",
    "In this homework, we will get started with Python and analyze a simple dataset. We will study a *Convection-Duffsion-Reaction* problem which considers how a premixed flame moves and reacts within a chamber. What we're interested in is the maximum temperature observed in that chamber under different conditions. The details of this scenario aren't critical to understanding and completing this homework. \n",
    "\n",
    "### General Homework Policies \n",
    "* You must not import any python libraries unless the question explicitly asks you to do so. \n",
    "* Some exercises may have a free response question at the end. We are not looking for anything more than a short paragraph for these questions. In some cases, just a sentence will suffice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 1: Loading and Initial Exploration of the Dataset\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Load the `CDRSamples.csv` dataset into your Jupyter notebook using the `pandas` library, which is widely used throughout the data-science world. This will load this `.csv` (which stands for comma-separated values) table into your notebook as a \"DataFrame\" object. This is a useful structure for manipulating spreadsheets programmatically. To receive full credit for this exercise, you must: \n",
    "\n",
    "- Import the `pandas` library and name it `pd` so we don't have to type out the whole name each time we use it. \n",
    "- Assign the `CDRSamples.csv` file as a Pandas Dataframe to a variable called `df`. \n",
    "- Print the number of rows contained in this dataframe. \n",
    "- Use a `for` loop to print out the range (max-min) of each column (You might find the `.min()` and `.max()` methods helpful)\n",
    "- Display the first 5 rows of the dataset using the `.head()` method on the `df` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE \n",
    "\n",
    "# Import the \"pandas\" library and name it \"pd\" so we don't have to call the whole name each time we use it\n",
    "\n",
    "# Assign the 'CDRSamples.csv' file as a Pandas Dataframe to a variable called \"df\"\n",
    "\n",
    "# Print the number of rows in the dataframe \n",
    "\n",
    "# Use a for-loop to print out the range of each column \n",
    "\n",
    "\n",
    "# 4.) Display the first 5 rows of the dataset using the .head() method on the \"df\" variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 2: Separating Input and Output Data\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "As we can see from the first few rows of the dataset, each row represents a unique selection of our input variables and the corresponding maximum temperature associated with those input variables. These inputs will be called \"features\". Features are observable traits of the system that we feed in as inputs to a machine learning model. The input features we'll be using are: \n",
    "\n",
    "* **A** and **E** are paremeters of the Arrhenius Equation (scaled for numerical stability), which determines the rate of reaction's dependence on temperature. \n",
    "* **Ti** is the temperature at the inlet of the chamber. \n",
    "* **To** is the temperature at the outlet of the chamber. \n",
    "* **phi** is the fuel:oxidizer ratio of the premixed inflow. \n",
    "\n",
    "The output we'll be using is: \n",
    "* **T_max** is the maximum temperature observed in the chamber. \n",
    "\n",
    "Our goal of this exercise is to use linear regression to predict the maximum temperature in the chamber, **T_max**, given we know **A, E, Ti, To** and **phi**. To get credit for this exercise you must: \n",
    "\n",
    "- Create a new DataFrame called `X_df` that only contains the input features (**A, E, Ti, To** and **phi**). \n",
    "- Create a new DataFrame named `Y_df` of only the **T_max** column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.) Create a new DataFrame named `X_df` of only the input features of the dataframe\n",
    "\n",
    "# 2.) Create a new DataFrame named `Y_df` of only the `T_max` column. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 3: Plotting Relationships Between Inputs and Outputs\n",
    "**10 Points**\n",
    "\n",
    "\n",
    "Visualizing relationships between variables is an extremely important part of data analysis. When using Python, the most common plotting package is `matplotlib`. In this exercise, we will create a simple scatterplot between the Fuel:Oxidzer Ratio (**phi**) and the maximum temperature. To receive full credit for this exercise, you must: \n",
    "\n",
    "- Import the pyplot package from matplotlib and name it `plt` \n",
    "- Plot a scatterplot showing the relationship between the **phi** and **T_max** values. Fuel:Oxidizer Ratio should be on the X-Axis and Maximum Temperature should be on the Y-Axis. Adjust the size of the points of the scatterplot to 0.5. \n",
    "- Add a grid to the plot \n",
    "- Add appropriate X and Y Labels to the plot. \n",
    "- Add an appropriate title to the plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyplot package from matploblib and name it 'plt' \n",
    "\n",
    "# Plot a scatterplot visualizing the relationship between the Fuel:Oxidizer Ratio and Maximum Temperature values \n",
    "\n",
    "# Add a grid, axes labels and a title to the plot \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 4: Quantifying Feature-Output Relationships \n",
    "**10 Points**\n",
    "\n",
    "We are interested in how much each feature is *corelated* with our output-of-interest, the maximum temperature (**T_max**). Recall that the Pearson Correlation Coefficient, $R$, is a measurement ranging from $\\pm 1$ which determines the extent to which two variables are linearly correlated with one another. An $R$ value of 1.0 indicates a perfect positive correlation i.e. as one variable increases, the other variable increases proportionally. An $R$ value of -1.0 indicates a perfect negative correlation which means as one variable increases, the other decreases. An $R$ value of zero means the variables share no linear relationship with each other. The Pearson Correlation Coefficient between two random variables $X$ and $Y$ is computed using the following: \n",
    "\n",
    "$$ R = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X) \\text{Var}(Y)}} \\approx \\frac{\\sum_{i=1}^N (x_i - \\mu_x) (y_i - \\mu_y)}{\\sqrt{\\left( \\sum_{i=1}^N (x_i - \\mu_x)^2 \\right) \\left(\\sum_{i=1}^N (y_i - \\mu_y)^2 \\right)}}$$ \n",
    "\n",
    "\n",
    "To get full-credit for this exercise, you must: \n",
    "\n",
    "* Import the `numpy` library and rename it `np`. \n",
    "* Use a `for` loop to print out the feature name and its pearson correlation coefficient ($R$) for each of the input features with the maximum temperature (**T_max**). Do not use the `np.corrcoef` function. *Hint: you may find the `df.columns` attribute and the `np.var` and `np.cov` functions useful* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy library and rename it 'np' \n",
    "\n",
    "# Loop through the features of X_df and print out the feature name and correlation coefficient for each \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Free Response:** Consider these correlation coefficients. Which feature will be most helpful in predicting maximum temperature? Which feature will be the least helpful? Why? \n",
    "\n",
    "*Your response here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 5: Least-Squares Regression with Numpy\n",
    "**10 Points**\n",
    "\n",
    "With this in mind, in this question, we will solve for a set of weights that solve the following least-squares regression problem: \n",
    "\n",
    "\n",
    "$$ \\min_{\\beta} ||X \\beta - Y||_2^2 $$ \n",
    "\n",
    "Where $X \\in \\mathbb{R}^{N \\times d}$, $\\beta \\in \\mathbb{R}^{d}$ and $Y \\in \\mathbb{R}^{N}$. $N$ is the number of observations we have and $d$ is the number of features we have for each observation. The optimal $\\beta$ value, i.e. the weights that optimally map the features of $X$ to the output $Y$ can be computed with the \"normal equations\" (we'll discuss this problem in much more detail in class, this exercise is just to get you familiar with matrix operations using NumPy): \n",
    "\n",
    "$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top Y $$ \n",
    "\n",
    "**NOTE:** *Let's say we want to compute $A^{-1}B$. Using `np.linalg.inv(A) @ B` is significantly more expensive than using `np.linalg.solve(A,B)`. This is because we don't actually need to invert a matrix all the way to solve a linear system. It's good programming practice to get in the habit of using the `solve()` function wherever you can in place of the `inv()` function. In MATLAB the `solve()` function is the same as `A\\B`.*  \n",
    "\n",
    "In this exercise, we will compute $\\hat{\\beta}$ using NumPy. To receive credit for this question, you must: \n",
    "\n",
    "- Convert the `X` and `Y` dataframes into 2D Numpy Arrays (matrices) called `X` and `Y`\n",
    "- Compute `beta_hat` using the second equation above. Use `np.linalg.solve()` to get avoid inverting the formed matrix and use `@` for matrix multiplication. \n",
    "- Print `beta_hat`\n",
    "\n",
    "*NOTE: Make sure your matrix dimensions are correct when you multiply $X^\\top X$. This should result in a 5x5 matrix. If you accidentally multiply $X X^\\top$, this will result in a 120,000 x 120,000 matrix which may exceed the memory of your machine.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_df and Y_df Dataframes into Numpy Arrays called X and Y\n",
    "\n",
    "# Compute beta_hat using the normal equations \n",
    "\n",
    "# Print beta_hat \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 6: Function to Quantify Prediction Error\n",
    "**10 Points**\n",
    "\n",
    "\n",
    "A function is a reusable block of code that takes a set of *parameters* and executes some set of instructions based on those parameters. For example, if we didn't want to convert between Farenheit and Celsius by hand each time, we might write a function called `to_celsius(farenheit_temp)` that does it for us. The name of the function, `to_celsius` takes in a variable called `farenheit_temp` and then returns the conversion so you don't have to manually calculate it each time. This can be written in python with the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.333333333333336\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def to_celsius(farenheit_temp):\n",
    "    return (farenheit_temp - 32) * (5/9)\n",
    "\n",
    "print(to_celsius(83))\n",
    "print(to_celsius(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A crucial part of machine learning is developing a metrics that quantify the performance of a model. In the case of regression, we are interested in how \"close\" the model's predictions come to the true values. One way to check this error is with *Mean-Relative Error*. This is computed with the following equation: \n",
    "\n",
    "$$ MRE = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{|\\hat{y}_i - y_i|}{y_i} $$ \n",
    "\n",
    "where $N$ is the total number of datapoints, $\\hat{y}_i$ is the model's prediction for a specific input, and $y_i$ is the true output at that input. If we multiply MRE by 100, we get **Mean Percent Error**. \n",
    "\n",
    "To get credit for this exercise, you must: \n",
    "\n",
    "- Compute a `Y_hat` variable with the following expression: $ X \\hat{\\beta} = \\hat{Y}$; this is the model's best guess at maximum temperature given the features of $X$. \n",
    "- Define a function called `compute_mre` which takes two parameters, `Y_pred` and `Y_true`, which are numpy vectors of the same length and returns the mean relative error between them.\n",
    "- Call the function on your `Y_hat` vector you computed in the first step and the `Y` vector. Print the **Mean Percent Error** between these two vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a Y_hat variable using your beta_hat variable from the previous sections \n",
    "\n",
    "# Define a function compute_mre that takes Y_pred and Y_true as parameters and returns the MRE between them: \n",
    "\n",
    "# Print the Mean Percent Error of the model's predictions, Y_hat and the true values stored in the vector Y \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Free Response:** In your own words, interpret this value. What does this say about how good or bad our linear regression is? \n",
    "\n",
    "*Your response here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 7: Visualizing Model Performance\n",
    "**10 Points**\n",
    "\n",
    "Oftentimes, it is useful to visualize the performance of a machine learning model to intuitively gauge how well the model has predicted the target values. \n",
    "\n",
    "* Import the `matplotlib.pyplot` module and rename it `plt` \n",
    "* Use `matplotlib` to plot a scatterplot of the `Y_hat`, or our model's predicted values on the X-Axis and `Y`, the true output values on the Y-Axis. Make the size of the points 0.5. \n",
    "* Add a grid, appropriate axis labels and a title to your plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the matplotlib.pyplot module and rename it 'plt' \n",
    "\n",
    "# Plot a scatterplot of Y_hat on the X-Axis and Y on the Y-Axis with appropriate sizing and labeling  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Free Response:** Describe how well the linear regression model is able to predict the maximum temperature. If the model performs perfectly (i.e. it 100% matches the true output values), how would this scatterplot look? If the model performs poorly (i.e. encounters high error in its predictions), how would it look?\n",
    "\n",
    "*Your response here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 8: Cross-Validation\n",
    "**10 Points**\n",
    "\n",
    "The model's mean squared error is a good metric to assess model performance, but by itself, it doesn't mean much. What we are interested in is how well the model *generalizes* to unseen data. This is the process of **Cross-Validation**. We have provided some code to take the `X_df` and `Y_df` and split them into \"Train\" and \"Test\" sets. The `test_size=0.20` argument means we will hold out 20% of our data from the training process to evaluate the model on unseen data. We will train our model on the training data, and then evaluate how well our model performs on the \"Test\" data. To get credit for this exercise, you must: \n",
    "\n",
    "* Compute a new `beta_hat` only using the training dataframes, like we did in **Exercise 5**. \n",
    "* Use the training `beta_hat` to compute `Yhat_train` and `Yhat_test` arrays. (Like we did in **Exercise 6**). \n",
    "* Use your `compute_mre` function in **Exercise 6** to compute variables called `train_mre` and `test_mre`, the Mean Relative Error of the training data and the MRE of the testing data. Print out the **Mean Percent Error** of the training and testing predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE PROVIDED; DO NOT MODIFY ---------------------\n",
    "\n",
    "# Importing Scikit-Learn library's train_test_split function \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the X_df and Y_df into training and testing pairs with 20% of the data being held out for testing \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_df, Y_df, test_size=0.20, random_state=42)\n",
    "\n",
    "# Converting these from pandas dataframes into numpy arrays \n",
    "X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values\n",
    "\n",
    "## BEGIN YOUR CODE: ---------------------------------\n",
    "\n",
    "# Compute a new beta_hat using only the training data \n",
    "\n",
    "# Compute Yhat_train and Yhat_test arrays (i.e. model's predictions on X_train and X_test)\n",
    "\n",
    "# Use the MRE function from Exercise 5 to compute train_mre and test_mre \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Free Response:** What do the Mean Percent Error values for the train and test sets tell you? Has our model done a good job of generalizing to unseen data? \n",
    "\n",
    "*Your response here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 9: Cross-Validation with Polynomial Features\n",
    "**10 Points**\n",
    "\n",
    "\n",
    "Because the model fit the data pretty well, we are now going to examine what happens when we add too many features. The provided code does a few things. First, we are only using 100 total datapoints. Second, we're going to add polynomial features up to order 3, using the `sklearn` library. This means that instead of simply having our parameters A, E, Ti, To, and Tmax as features, we will use every combination of these up to cubic nonlinearity (e.g. $AE^2$, $T_i^3$, $A T_o T_{max}$, etc). With nonlinear features, we are now able to capture nonlinear relationships between our features. \n",
    "\n",
    "To get credit for this exercise, please repeat the steps of **Exercise 8** on this new dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE PROVIDED; DO NOT MODIFY ---------------------\n",
    "\n",
    "# Importing Scikit-Learn functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "\n",
    "# Defining how many datapoints to use in our analysis \n",
    "n_datapoints = 100\n",
    "\n",
    "# Splitting the X_df and Y_df into training and testing pairs with 20% of the data being held out for testing \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_df.loc[:n_datapoints,:], Y_df.loc[:n_datapoints,:], test_size=0.20, random_state=42)\n",
    "\n",
    "# Converting these from pandas dataframes into numpy arrays \n",
    "X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values\n",
    "\n",
    "# Generating polynomial features up to order 3 on the training/testing inputs \n",
    "poly_features = PolynomialFeatures(degree=3)\n",
    "X_train = poly_features.fit_transform(X_train)\n",
    "X_test = poly_features.transform(X_test)\n",
    "\n",
    "## BEGIN YOUR CODE: ---------------------------------\n",
    "\n",
    "# Compute a new beta_hat using only the training data \n",
    "\n",
    "# Compute Yhat_train and Yhat_test arrays (i.e. model's predictions on X_train and X_test)\n",
    "\n",
    "# Use the compute_mre function from Exercise 5 to compute train_mre and test_mre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Free Response:** How does the nonlinear model compare to the linear model? What do the MRE Values for the train and test-set tell us? Explain what you think might be going on here. (Hint: What dimension is `beta_hat` now?)\n",
    "\n",
    "*Your response here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 10: Conceptual Wrap-Up \n",
    "**10 Points** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Free Response:** If you've completed the homework up to this point, you should have successfully trained a machine learning model on a relatively large dataset with over 100,000 points (woohoo!). In what ways is a linear regression model useful? In what ways is it limited or could be improved? Be specific based on the theory we covered in class. \n",
    "\n",
    "*Your response here*\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
