\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=2.0cm, left=2cm, right=2cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
\usepackage{gensymb,enumerate}  
\usepackage{hyperref,color,enumerate,enumitem}
\usepackage{bm}
\usepackage{listings}
\usepackage{csquotes}
\usepackage{float}
\usepackage{multicol}
\usepackage[style = ieee]{biblatex}

\bibliography{CSE6740References}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}

\setlength{\parindent}{20pt}
\setlength{\parskip}{0.05in}

\pagestyle{fancyplain}
\headheight 35pt
\lhead{ AIM 4803 \\ \small Georgia Tech }
\chead{ \large \textbf{Machine Learning for AE \\ } Homework II }
\rhead{Due Feb 21st 11:59pm EST}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corrolary}
\newtheorem{dfn}{Definition}
\newtheorem{cnj}{Conjecture}
\newtheorem{prp}{Proposition}

\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\Var}[1]{\mathbb{V}\left[#1\right]}
\newcommand{\Corr}[2]{\text{Corr}\left[#1, #2\right]}
\newcommand{\Cov}[2]{\text{Cov}\left[#1, #2\right]}
\newcommand{\OneN}{\left[\mathbf{1}\right]_{1/N}}
\newcommand{\One}{\left[\mathbf{1} \right]}
\newcommand{\Tr}{\text{Tr}}

\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}

\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bef}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bi}{\mathbf{i}}
\newcommand{\bj}{\mathbf{j}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bl}{\mathbf{l}}
\newcommand{\bem}{\mathbf{m}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}


\newcommand{\R}{\mathbb{R}}
\newcommand{\bPhi}{\mathbf{\Phi}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bkappa}{\boldsymbol{\kappa}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bYt}{\Tilde{\bY}}
\newcommand{\xnew}{x_{\text{new}}}
\newcommand{\znew}{z_{\text{new}}}
\newcommand{\bznew}{\mathbf{Z}_{\text{new}}}
\newcommand{\bPsi}{\boldsymbol{\Psi}}
\newcommand{\bmu}{\boldsymbol{\mu}}

\begin{document}

\section*{Homework Instructions}

\begin{itemize}
    \item This homework contains some theoretical problems that you will need to work out algebraically. The important thing is that you \textbf{show all steps} you took to arrive at your answer. 
    \item Exercises 1-3 are contained in this document and Exercises 4-6 are in the \texttt{Homework1.ipynb} starter-code provided on Canvas. 
    \item Please turn in two files on the Canvas assignment: 
    \begin{enumerate}
        \item \texttt{Homework2\textunderscore FIRSTNAME\textunderscore LASTNAME.pdf} - a file containing your answers to the theoretical questions. 
        \item \texttt{Homework2\textunderscore FIRSTNAME\textunderscore LASTNAME.ipynb} - a file containing your answers to the programmatic questions. 
    \end{enumerate}
    \item For the theoretical questions, the use of \LaTeX (the .tex file for this document will be provided so you can use this template if you'd like) is strongly encouraged for this assignment, but not required. You have a Georgia Tech account on \url{http://www.overleaf.com} where you can edit and compile LaTeX documents. This is a very useful (and satisfying) skill to create a nice technical documents and is the standard in technical publishing. If you do not want to use \LaTeX, you may use Microsoft Word or similar software, just make sure your work is submitted in \texttt{.pdf} format. Handwritten responses will not be accepted. 

    \item You must present the answers in order and the submitted documents must be legible and well-organized. 

    \item As always, if you collaborate with others or use online resources, this is allowed, just please outline how you used them. And, of course, directly copying the responses of any entity other than yourself is not allowed. 
\end{itemize}

\vfill 

\pagebreak 

\section{Pseudoinverses and the Normal Equations \\ {\large (15 Points)}}


In class, we have extensively covered the minimization of the following loss-function to find the optimal least-squares regression weights $\beta$:

\begin{align}
    \mathcal{L}(\beta) = || X \beta - Y||_2^2
\end{align}

The normal equations to find the minimum-norm $\beta$ when $X$ is not full-rank is given by: 

\begin{align}
    \hat{\beta} = (X^\top X)^\dagger X^\top Y
\end{align}

where $(\cdot)^\dagger$ indicates a pseudoinverse. Show that by substituting $X$'s SVD into the modified normal equations, the following is true:  

\begin{align}
    \hat{\beta} = (X^\top X)^\dagger X^\top Y = X^\dagger Y
\end{align}

\textit{Hints:} 
\begin{itemize}
    \item \textit{For a matrix $A \in \R^{m \times n}$, its pseudoinverse is given by $ A^\dagger = V \Sigma^{-1} U^\top $}
    \item \textit{For a unitary or sub-unitary matrix, $A \in \R^{m \times m}$, $A^\top A = I $. }
    \item \textit{For a diagonal matrix, $A \in \R^{m \times m}$, $A^\top = A$. }
\end{itemize}

\vfill \pagebreak 

\section{Normal Equations for Ridge Regression\\ \large (20 Points) }

Now let's introduce a new penalty into the least-squares loss-function that also considers the 2-norm of $\beta$ itself: 

\begin{align}
    \mathcal{L}(\beta) = || X \beta - Y||_2^2 + \lambda || \beta||_2^2
\end{align}

Now our loss-function penalizes both \enquote{size} of the model error and the \enquote{size} of $\beta$. Show that the $\beta$ which minimizes this new loss-function is given by:

\begin{align}
    \beta_\lambda = \left( X^\top X + \lambda I \right)^{-1} X^\top Y
\end{align}

\textit{Hints:} 

\begin{itemize}
    \item \textit{Start by expanding the loss-function (Recall that for a vector $v$, $||v||_2^2 = v^\top v $).}
    \item \textit{Once you have the loss-function entirely in terms of matrix-vector multiplication, find its gradient with respect to $\beta$. }
    \item \textit{Once you have $\nabla \mathcal{L}(\beta)$, set it equal to zero and solve for $\beta$.} 
\end{itemize}

\vfill \pagebreak 

\section{The Achilles Heel of Linear Regression \\ \large (10 Points)}

In this exercise, we will see how small changes in our $X$ and $Y$ matrices can have \textit{huge} consequences on our computed linear regression weights. This can be the result of noise or corruption in our training data. Let's examine a simple 2x2 example of how regularization works. Consider the following $X \beta = Y$ system: 

$$ \begin{bmatrix} 1 & 100 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} \beta_1 \\ \beta_2 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix} $$ 

Our matrix $X$ is rank-deficient which means there are infinitely many solutions to this system. The minimum-norm solution to this system is: 

$$ \beta^\dagger = X^\dagger Y = \begin{bmatrix} 1/1001 \\ 100 / 1001 \end{bmatrix} \approx \begin{bmatrix} 0.001 \\ 0.1 \end{bmatrix} $$ 

Now let's consider \textit{perturbing} the matrix $X$ by some small, positive $\varepsilon$. 

$$ \begin{bmatrix} 1 & 100 \\ 0 & \varepsilon \end{bmatrix} \begin{bmatrix} \beta_1 \\ \beta_2 \end{bmatrix} = \begin{bmatrix} 1 \\ \varepsilon \end{bmatrix} $$ 

With the addition of this tiny $\varepsilon$, this system is now full-rank which means there is only one solution. \textbf{By hand}, solve for $\beta_\varepsilon$ (the solution to the perturbed system). How does this $\beta_\varepsilon$ compare to the original $\beta^\dagger$? What does this say about the stability of linear regression in the presence of small perturbations?


\end{document}
