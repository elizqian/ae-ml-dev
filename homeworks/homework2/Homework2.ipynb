{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Aerospace Engineers\n",
    "### **Homework 2: SVD, Regularization and the Pseudoinverse**\n",
    "\n",
    "\n",
    "### General Homework Policies \n",
    "* You must not import any python libraries unless the question explicitly asks you to do so. \n",
    "* Some exercises may have a free response question at the end. We are not looking for anything more than a short paragraph for these questions. In some cases, just a sentence will suffice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 4: Regularization to Combat Instability from Perturbation\n",
    "**10 Points**\n",
    "\n",
    "Now we'll explore regularization as a stratgegy to combat the behavior observed in **Exercise 3**. We have provided some code and set $\\varepsilon=0.01$. We've defined a `lambda_vals` list which contains logarithmically spaced values of $\\lambda$ from $10^{-9}$ to $10^5$. To get credit for this exercise: \n",
    "\n",
    "* Compute the SVD of X using `np.linalg.svd(X)`. \n",
    "* Loop through the `lambda_vals` vector of regularization parameters to try out. \n",
    "* For each loop iteration, solve for $\\beta_\\varepsilon$ using the regularized Normal Equations (do not use `np.linalg.inv()` ): \n",
    "$$ \\beta_{\\varepsilon} = (X^\\top X + \\lambda I)^{-1} X^\\top Y $$ \n",
    "* For each loop iteration, compute the error between $X \\beta_\\varepsilon$ and $Y$. Append this value to the `errors` list. \n",
    "$$ \\text{error} = ||X \\beta_{\\varepsilon} - Y||_2^2 $$ \n",
    "\n",
    "* For each loop iteration, compute the 2-norm of $\\beta_{\\varepsilon}$ and append it to the `norms` list. \n",
    "* Make a plot with the error values on the y-axis and the norms of $\\beta$ on the x-axis using the `plt.loglog` function for logarithmically scaled x and y axes. Add appropriate axis labels, a grid, and a title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE PROVIDED; DO NOT MODIFY ---------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# We have set the epsilon to 0.01 \n",
    "eps = 0.01\n",
    "# The perturbed X \n",
    "X = np.array([\n",
    "    [1, 100], \n",
    "    [0, eps]\n",
    "])\n",
    "\n",
    "# The perturbed Y \n",
    "Y = np.array([\n",
    "    1, eps \n",
    "])\n",
    "\n",
    "lambda_vals = np.logspace(-9, 5, 100)\n",
    "\n",
    "errors = []\n",
    "norms = []\n",
    "\n",
    "## BEGIN YOUR CODE: ---------------------------------\n",
    "\n",
    "# Compute the SVD of X \n",
    "\n",
    "# Loop through the values in lambda_vals \n",
    "for lambda_val in lambda_vals:\n",
    "    # Solve for beta using the pseudoinverse definition above \n",
    "\n",
    "    # Compute the 2-norm of the residual X beta - Y \n",
    "\n",
    "    # Append the error to the errors list and the 2-norm of beta to the norms list\n",
    "\n",
    "# Plot the 2-norms of beta on the x-axis and the 2-norms of the residuals on the y-axis on a log-log plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Free Response:** Explain the trend in this plot. What does this tell us about how we should choose our $\\lambda$ to get as close to the original $\\beta^\\dagger$ as we can?   \n",
    "\n",
    "*Your response here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 5: Overfitting Revisited \n",
    "\n",
    "**20 Points**\n",
    "\n",
    "We can also use regularization when we have an extremely expressive model. Recall the 1-D Bernoulli equation: \n",
    "\n",
    "$$ P_1 + \\frac{1}{2} \\rho v_1^2 + \\rho g h_1 = P_2 + \\frac{1}{2} \\rho v_2^2 + \\rho g h_2 $$ \n",
    "\n",
    "Let's say we wish to approximate the pressure $P_2$ and we have experimental measurements of $P_1, \\rho, v_1$, and $v_2$. We have given you some starter code to generate the data you need. Do not modify this code. You've been given an `X_df` and `Y_df` which contain the input data and output data. To get full credit for this exercise, you must: \n",
    "\n",
    "* Load in the `BernoulliData.csv` file into your notebook as a pandas dataframe. \n",
    "\n",
    "* Create an `X_df` containing the `P1`, `rho`, `v1`, `v2` columns. Create a `Y_df` containing the `P2` column. \n",
    "\n",
    "* Split the `X_df` and `Y_df` dataframes into train and test sets with 80% of the training data used to train a linear regression model. Name these new arrays `Xtrain`, `Xtest`, `Ytrain`, `Ytest`. You may copy the code we used in Homework 1. Set the `random_state=42` for repeatability. \n",
    "\n",
    "* Import the `PolynomialFeatures` class from the `sklearn` library. Use this module to transform your data to have degree-3 polynomial features. You my also use the code from Homework 1. \n",
    "\n",
    "* Define a function called `mean_absolute_error` which takes in a `Ytrue` and a `Yhat` and computes the Mean Absolute Error between them. \n",
    "\n",
    "* Compute a `beta_hat` using the normal equations (no regularization)\n",
    "\n",
    "* Compute a `Yhat_train` vector to store the model's predictions for the `Xtrain` data. \n",
    "\n",
    "* Compute a `Yhat_test` vector to store the model's predictions for the `Xtest` data. \n",
    "\n",
    "* Print out the 2-norm of `beta_hat`.\n",
    "\n",
    "* Print out the MAE between `Ytrain` and `Yhat_train` using your `mean_absolute_error` function. \n",
    "\n",
    "* Print out the MAE between `Ytest` and `Yhat_test` using your `mean_absolute_error` function. \n",
    "\n",
    "* Plot a scatterplot with `Yhat_train` and `Ytrain` on the X and Y axes, respectively. Color these points blue. On the same plot, plot a scatterplot of `Yhat_test` and `Ytest` on the X and Y axes, respectively. Add appropriate axis labels, a grid, **a legend**, and a title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Load the 'BernoulliData.csv' file into your workspace as a pandas dataframe \n",
    "\n",
    "# Create an `X_df` containing the `P1`, `rho`, `v1`, `v2` columns. Create a `Y_df` containing the `P2` column. \n",
    "\n",
    "\n",
    "# Split the X_df and Y_df into Xtrain, Xtest, Ytrain, Ytest numpy arrays \n",
    "\n",
    "# Use PolynomialFeatures from sklearn.preprocessing to create degree-3 polynomial features \n",
    "\n",
    "# Create a function called mean_absolute_error() to compute the MAE between some Ytrue and Yhat vectors\n",
    "\n",
    "# Compute a beta_hat using ordinary least-squares/the normal equations without any regularization \n",
    "\n",
    "# Use this beta_hat to make predictions on the training and testing inputs. Call these predictions Yhat_train and Yhat_test\n",
    "\n",
    "# Print out the 2-norm of beta_hat \n",
    "\n",
    "# Print out the training and testing MAE for the model \n",
    "\n",
    "# Make a scatterplot containing the Yhat_train and Yhat_test on the X-Axis and Ytrain and Ytest on the Y-Axis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Free Response:** In your own words, interpret the MAE for the training and testing data. How did our simple linear regression model perform? Why? Be specific. What does the 2-norm of $\\hat{\\beta}$ tell us? \n",
    "\n",
    "*Your response here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 6: Regularization to Combat Overfitting\n",
    "\n",
    "**20 Points**\n",
    "\n",
    "Now that we know our model isn't performing well on unseen data, let's employ regularization to fix it. To get full credit for this exercise, you must: \n",
    "\n",
    "* Define a `lambda_val` variable as our regularization parameter. \n",
    "\n",
    "* Compute a new `beta_hat` using the regularized normal equations i.e. $\\hat{\\beta} = (X^\\top X + \\lambda I)^{-1} X^\\top Y $. \n",
    "\n",
    "* Compute a new `Yhat_train` vector to store the model's predictions for the `Xtrain` data. \n",
    "\n",
    "* Compute a new `Yhat_test` vector to store the model's predictions for the `Xtest` data. \n",
    "\n",
    "* Print out the new 2-norm of `beta_hat`.\n",
    "\n",
    "* Print out the new MAE between `Ytrain` and `Yhat_train` using your `mean_absolute_error` function. \n",
    "\n",
    "* Print out the new MAE between `Ytest` and `Yhat_test` using your `mean_absolute_error` function. \n",
    "\n",
    "* Plot a scatterplot with `Yhat_train` and `Ytrain` on the X and Y axes, respectively. Color these points blue. On the same plot, plot a scatterplot of `Yhat_test` and `Ytest` on the X and Y axes, respectively. Add appropriate axis labels, a grid, **a legend**, and a title. \n",
    "\n",
    "* Repeat this process using different values of `lambda_val` until the training MAE is roughly equal to the test MAE (within $\\pm 3$ of each other is fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a new beta_hat using the regularized normal equations \n",
    "\n",
    "# Use this beta_hat to make predictions on the training and testing inputs. Call these predictions Yhat_train and Yhat_test\n",
    "\n",
    "\n",
    "# Print out the 2-norm of beta_hat \n",
    "\n",
    "# Print out the training and testing MAE for the model \n",
    "\n",
    "# Make a scatterplot containing the Yhat_train and Yhat_test on the X-Axis and Ytrain and Ytest on the Y-Axis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Free Response:** \n",
    "* **What value of `lambda_val` did you end up choosing?** \n",
    "* **Is this larger or smaller than you expected?** \n",
    "* **How does our training MAE with regularization compare to our training MAE without regularization?** \n",
    "* **What is the 2-norm of the regularized $\\hat{\\beta}$? How does it compare to the unregularized $\\hat{\\beta}$?** \n",
    "* **What ingredient of the regression problem does regularization change? Give justification using the theory from class.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your response here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
