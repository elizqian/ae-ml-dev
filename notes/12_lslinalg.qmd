---
title: "LLS: Linear Algebra Perspective"
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"

---

{{< include _macros.qmd >}}

We now take a deeper dive into the structure of linear least squares problems. We will see how theoretical concepts from linear algebra can be used to describe and explain properties of learned linear regression models, and we will introduce fundamental algorithms from computational linear algebra that are used to solve linear least squares problems.

The intended learning outcomes of these notes are that students should be able to:


## Mathematical problem formulation
We briefly recap the mathematical problem formulation that we consider. Given a data set $\{(z_i,y_i)\}_{i=1}^N$, where $z_i\in\R^d$ and $y_i\in\R$ are paired inputs and outputs, we choose a feature mapping $x: \R^d\to\R^n$, and define the matrix $X\in\R^{N\times n}$ and vector $Y\in\R^N$ as follows:

$$
X = \begin{pmatrix}
- &x^\top(z_1) & - \\
 & \vdots & \\
- & x^\top(z_N) & -
\end{pmatrix}, 
\qquad 
Y = \begin{pmatrix}
y_1 \\ \vdots \\ y_N
\end{pmatrix}.
$$ {#eq-def-X-y}

The least squares problem is then to solve (as introduced previously):

$$
\beta^* = \arg\min_{\beta\in\R^n} \frac1N\|X \beta - Y\|^2.
$$ {#eq-ls-min}

The minimization above is somewhat cumbersome to write, so you'll sometimes see the following expression used as shorthand for the above minimization (remember that the $1/N$ factor doesn't really matter for finding the minimum):

$$ 
X\beta = Y
$$ {#eq-ls-matrix}

This notation is particularly common in the linear algebra context which is the focus of these notes.

## Well-posedness

In math, the term "well-posedness" refers to the existence and uniqueness of solutions to a problem. 
We say a mathematical problem is "well-posed" if it has a unique solution. Suppose for a moment that $N = n$ so that $X$ is square. Recall from linear algebra that the square linear system $X\beta=Y$ has a unique solution if and only if $X$ is invertible, and this solution is given by $\beta=X^{-1}Y$. 

Linear least-squares problems consider the general case where $X$ is non-invertible. This happens in our linear regression case when $X$ is rectangular, but $X$ can also be square and non-invertible. There are essentially two ways $X$ can fail to be invertible:

1. $X$ can have column rank less than $n$, i.e., the number of linearly independent columns in $X$ is less than the number of columns $n$. In this case, we say $X$ is "column rank-deficient" or $X$ "does not have full column rank".

2. $X$ can have row rank less than $N$, i.e., the number of linearly independent rows in $X$ is less than the number of rows $N$. In this case, we say $X$ is "row rank-deficient" or $X$ "does not have full row rank".

These are not mutually exclusive: $X$ can be simultaneously column rank-deficient and row rank-deficient. However, these different types of rank deficiencies have different consequences for well-posedness the existence and uniqueness of solutions, and different implications for learning linear regression models. 

### Overdetermined least-squares problems

Let's start with the row rank-deficient case. If $X$ has fewer linearly independent rows than the total number of rows, this means that some of the rows of $X$ are redundant.
This means that some of the linear equations represented by rows of the linear system $X\beta=Y$ have redundant left-hand sides. 
Because redundant equations place more constraints on the solution than can be simultaneously satisfied, this case is called the *overdetermined* case.

One way overdetermined linear systems can arise is if we have multiple copies of the same input in the data set, i.e., suppose $z_1 = z_2$. Note that having multiple copies of the same input datum usually does not mean that the associated output data are also copies of each other (so you can still have $y_1 \neq y_2$): for example, you might take multiple experimental measurements with the same configuration and get different measured values due to sensor noise or external factors that you don't account for in your inputs. If this happens, your data set contains "contradictory" information, because $x(z_1)^\top\beta=y_1$ and $x(z_2)^\top\beta= x(z_1)^\top\beta=y_2$ cannot simultaneously be true. 

What does row rank-deficiency mean for solving $X\beta=Y$? In most cases this means that there is no choice of $\beta$ that will exactly solve the equation --- the equality cannot simultaneously hold for all rows of the system. That means *no solution exists* for the linear system. 

Since no solution exists in the sense of $X\beta=Y$ holding exactly for all rows, we will define a different way of thinking about solutions. Let's define the *residual* of the system as follows:

$$
r(\beta) = X\beta-Y.
$$ {#eq-res-def}

For a given parameter $\beta$, the residual $r(\beta)$ is a measure of the model misfit over the training data set.
Note that if $X\beta=Y$ holds with equality for all rows, then the residual is zero. In the overdetermined case where $X\beta=Y$, since we can't have a zero residual, we aim for the next best thing, which is minimizing the size of the residual (recall that the norm is a measure of the size of a vector)[^1]:

[^1]: Note that the second equality follows because squaring the norm doesn't change where the minimum is because $a^2$ is monotone increasing in $a$ for non-negative $a$. 

$$
\beta^* = \arg\min_{\beta\in\R^n} \|r(\beta)\| = \arg\min_{\beta\in\R^n}\|r(\beta)\|^2
$$ {#eq-ls-res-min}

With the residual defined in @eq-res-def, the expression @eq-ls-res-min recovers our original expression for the formulation of a least-squares problem[^2], @eq-ls-min. In this sense, the least-squares formulation "fixes" the lack of existence of a solution to the linear system due to row redundancy. For this reason, if you see $X\beta=Y$ written for a non-invertible system, the way you should understand that is in the sense of solving @eq-ls-min[^3].

[^2]: up to a multiplying constant anyway, which I've emphasized before really doesn't matter when it's inside a minimization.

[^3]: see, for example, the documentation for the numpy function [<tt>linalg.lstsq</tt>](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html) or the [scipy version](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html#scipy.linalg.lstsq)

::: {.callout-note}
explain that row redundancy can also happen if not literally the same thing
:::


### Underdetermined least-squares problems

We now turn our attention to the column rank-deficient case. If $X$ has fewer linearly independent columns than the total number of columns, this means that some of the columns of $X$ are redundant. Recall that $X\beta\in\R^N=\sum_{j=1}^n \beta_j x_j$, where $x_j\in\R^N$ is the $j$th column of $X$. If $X$ has redundant columns, this means that there are different ways to weight the columns of $X$ using the coefficients in $\beta$ that lead to the same value for $X\beta$. This means that requiring $X\beta=Y$ will not fully specify what $\beta$ is, so we call this the *underdetermined* case. 

One way underdetermined systems arise is if there are more unknown coefficients in $\beta$ than there data points to constrain the problem. This can happen if we choose a very large set of features ($n$ large) but only have a small amount of data ($N<n$).

What does column rank-deficiency mean for solving $X\beta=Y$?
This means that *the solution is not unique*. This non-uniqueness comes from the fact that column rank-deficiency of $X$ means that $X$ has a nonzero kernel (aka nullspace). Recall that this means there exists some nonzero $v\in\R^n$ satisfying $Xv = 0$. Let $r^*$ be the minimum possible residual, 

$$
r^* = \min_{\beta\in\R^n}\|r(\beta)\|.
$$

Suppose we find some $\beta^*$ that leads to this minimum residual, so that $r^* = X\beta^* - Y$. Well, if $v$ lies in the kernel of $X$, then we can add any multiple of $v$ to $\beta^*$ and get the same minimum residual! That is, if $\beta^*$ is *a* solution to @eq-ls-res-min, then so is $\beta^* + av$ for any $a\in\R$. This means there are infinitely many solutions. To make underdetermined problems well-posed, we have to decide on a way of choosing a single unique solution out of the many possible minimizers of @eq-ls-res-min. Deciding on a method of choosing a single unique solution out of many possible solutions is called *regularization* and is a topic we will explore in more depth later. In the meantime, the most common and vanilla way of regularizing is to specify that we want to find what's called the "minimum-norm" solution: i.e., out of all possible solutions, we take the smallest one. That is,

$$
\beta^* = \arg\min_{\beta\in\R^n} \|\beta\| \quad \text{subject to } \|X\beta - Y\| = r^*.
$$




::: {.callout-note}
explain that col redundancy can also happen even if $N\geq n$ if features are redundant.
:::

feature selection 

multiple solutions 

low-rank or near-low-rank-ness

## QR decomposition

algorithm, cost 

## SVD 

for data reduction, and for regularization

## Exercises

- some scenarios to classify is under or over-determined or both




<!-- ## Using Nonlinear Features to Improve Model Performance -->

<!-- We know the underlying formula for drag-force is: 

$$ F_d = \frac{1}{2} \rho v^2 C_d A  $$  -->

<!-- While this function doesn't depend on the angle of attack, we do see how the features are polynomially related to one another. Naturally, it is going to be difficult for a linear combination of features to replicate this. However, if we take the log of both sides, something interesting happens: 

$$ \ln(F_d) = \ln(\frac{1}{2} \rho v^2 C_d A) = \ln(\frac{1}{2}) + \ln(\rho) + 2\ln(v) + \ln(C_d) + \ln(A) $$

When our features and outputs are log-scaled, we see a friendly linear-combination of log-scaled versions features emerge! Let's alter our inputs and outputs accordingly, so our problem becomes: 

$$ \mathbf{X} = \begin{bmatrix} 1 & \ln \alpha_1 & \ln \rho_1 & \ln v_1 \\ 1 & \ln \alpha_2 & \ln \rho_2 & \ln v_2 \\ & & \vdots & \\ 1 & \ln \alpha_N & \ln \rho_N & \ln v_N \end{bmatrix} , \mathbf{Y} = \begin{bmatrix} \ln F_{d1} \\ \ln F_{d2} \\ \vdots \\ \ln F_{d3} \end{bmatrix} $$  -->




<!-- We have nearly a 5x reduction in MSE just by log-scaling our features! If we choose the right features, linear regression can be an extremely powerful method for approximating complex functions.  -->

---