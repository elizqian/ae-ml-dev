---
title: "LLS: Computational Considerations"
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"

---

{{< include _macros.qmd >}}

introduce FLOPs for matmat, matvec, etc.

introduce LU and QR, give pseudocode algorithms, matrix expressions, cost of each algorithm, give python code and show nonzero residual for LU and conditioning error for QR?

explain machine precision both double and single 

introduce the SVD in math, singular values vs rank/conditioning, explain the concept of conditioning in terms of being essentially non-invertible from the computer's point of view, cond/rank commands

show that normal equations are terribly conditioned.

pseudoinverse and truncated SVD for regularization

SVD + PCA??


## Algorithms for solving linear systems
We now turn our attention to the algorithms for solving equations of the form $X\beta = Y$ for invertible as well as over- or underdetermined systems. For all of these cases, the best practice is to use functions that are part of standard linear algebra packages to solve these systems. In python, both the <tt>numpy</tt> and <tt>scipy</tt> libraries have <tt>linalg</tt> packages that contain various useful linear algebra functions. Both are based on the LAPACK library, which efficiently implements many linear algebra computations in Fortran.
Popular deep learning packages like PyTorch and TensorFlow have their own linear algebra packages that build on other libraries that leverage GPU computing capabilities.

<!-- add a sentence saying what things we will introduce -->

### Invertible linear systems
If $X$ is invertible (and therefore square), the python functions we use to solve the system $X\beta=Y$ are <tt>numpy.linalg.solve</tt> and <tt>scipy.linalg.solve</tt>. 

These packages solve $X\beta = Y$ using *LU factorization*, which is a topic that should have been covered in your linear algebra class. Here's a quick recap if you've forgotten[^4]: any square matrix $X$ can be written (sometimes after appropriate re-ordering of the rows/columns) as the product of a lower triangular matrix $L$ with an upper triangular matrix $U$, that is, $X=LU$. Here's some pseudocode for LU decomposition (without pivoting, that is, assuming no re-ordering of the rows/columns of $X$ are necessary):

[^4]: if you want more of a refresher, revisit your favorite linear algebra textbook. My favorite is [@strang2022introduction].

**LU decomposition: pseudocode**

1. Initialize $L = I$ (Identity matrix).
2. `for` $j = 1$ to $n$:
   - `for` $i = 1$ to $j$:
     - $s_1 = 0$
     - `for` $k = 1$ to $i$:
       - $s_1 = s_1 + U_{kj} L_{ik}$
     - $U_{ij} = X_{ij} - s_1$
   - `for` $i = (j + 1)$ to $n$:
     - $s_2 = 0$
     - `for` $k = 1$ to $j - 1$:
       - $s_2 = s_2 + U_{kj}  L_{ik}$
     - $L_{ij} = \frac{X_{ij} - s_2}{U_{jj}}$
3. `return` $L$, $U$

Once the LU factorization of $X$ is found, then we can solve for $\beta$ in two steps:


1. Solve $L\alpha = Y$ for $\alpha$.

2. Solve $U\beta = \alpha$ for $\beta$.

This may look like we've just overcomplicated things by turning our one problem of solving $X\beta = Y$ into two problems, but the triangular structure of $L$ and $U$ lets us solve these two new systems relatively efficiently. To see this, consider solving $L\alpha = Y$ for a three-dimensional example:

$$
L = \begin{pmatrix}1 & & \\ 2 & 1 & \\ 
2 & 1 & 2 
\end{pmatrix}, \qquad Y = \begin{pmatrix}1 \\ 2 \\ 0\end{pmatrix}.
$$

Recall that this represents a system of three linear equations:

$$
\begin{aligned}
1 \alpha_1 &             & = 1\\
2 \alpha_1 &+ 1 \alpha_2  &= 2 \\
2 \alpha_1 &+ 1 \alpha_2 + 2 \alpha_3 & = 0
\end{aligned}
$$

We can immediately read off the solution of the first equation, $\alpha_1 = 1$ because there is only one unknown variable in it. From there we substitute that into the second equation to get $2 + 1\alpha_2 = 2$, so we again one unknown variable, which we can easily solve to get $\alpha_2 = 0$. From there we now substitute our values for $\alpha_1$ and $\alpha_2$ into the third equation, yielding $2 + 2\alpha_3 = 0$, which again has one unknown variable, so we can easily solve to get $\alpha_3 = -1$. This yields our overall solution $\alpha = (1, 0, -1)^\top$.

The process we have just described is called *forward substitution* and is a standard algorithm for solving lower triangular systems. It generalizes easily to triangular systems of arbitrary dimension $n\times n$ --- we just keep substituting in the results of previous rows into the next row, as follows:

**Forward substitution: pseudocode**

1. $\alpha_1 = Y_1 / L_{11}$.
2. `for` $i$ = 2 to $n$:
   - $s = 0$
   - `for` $j$ = 1 to $(i - 1)$:
     - $s = s + L_{ij}\alpha_j$
   - $\alpha_i = (Y_i - s) / L_{ii}$
3. `return` $\alpha$

Now that we have solved $L\alpha = Y$ for $\alpha$ using forward substitution, we need to solve $U\beta = \alpha$ for $\beta$, where $U$ is upper triangular rather than lower triangular. This is done using *backward substitution*, which is a very similar algorithm to forward substitution. That gives us our final solution $\beta$.

::: {.callout-note}
## Exercise
Without looking it up, can you write pseudocode for a backward substitution algorithm for solving upper triangular systems?
:::

### Least squares problems
If $X$ is non-invertible, the python functions we use to solve $X\beta = Y$ are <tt>numpy.linalg.lstsq</tt> and <tt>scipy.linalg.lstsq</tt>. These functions can use a number of different algorithms to solve the least squares problem. A common choice is based on *QR decomposition*, which should also have been covered in your linear algebra class. 

Here's a reminder of the essential facts of QR decomposition: if $X\in\R^{N\times n}$ has full column rank, then it can be written as the product of an orthogonal matrix $Q\in\R^{N\times n}$ with an upper triangular matrix $R\in\R^{n\times n}$. Analogous to solving invertible systems, once we have factored $X=QR$ we can then solve the least squares problem in two steps:

1. Solve $Q\alpha = Y$ for $\alpha$.

2. Solve $R\beta = \alpha$ for $\beta$.

Recall that orthogonal matrices $Q$ satisfy $Q^\top Q=I$, so this special structure lets us find $\alpha$ via a matrix multiplication, which is computationally more efficient than a full LU decomposition. Then, since $R$ is upper triangular, we can solve for $\beta$ using backward substitution.

If $X$ is low-rank, there are a couple of options, but the standard one is based on the the singular value decomposition, which deserves its own set of notes, so we will defer discussion of this until later. 
