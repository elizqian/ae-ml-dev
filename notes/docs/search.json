[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AE 4803 AIM: Course Notes",
    "section": "",
    "text": "Preface\nThis is the class website where all relevant notes and class information will be stored for AE4803 AIM. To get started, click the Introduction button to the left.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What’s in a name? Artificial intelligence, machine learning, data science, and the scope of this course.\nI googled “artificial intelligence” (often abbreviated AI) and the first result comes from Google’s new “AI overview” feature1:\nBecause I closed the tab with my first search and wanted to go back to the webpage to copy the text for the image alt-text above, I repeated my search, and got a somewhat different answer:\nThese are probably both reasonable definitions for casual conversation, but that’s not what we’re here for. Instead, we’re here to really learn deeply about what AI is, and for that we’re going to need precision – that is, of our definitions.\nI’m not going to provide a definition of AI for now and instead I’m going to throw two more terms into the mix that you may have heard. The first is “machine learning” (often abbreviated ML). Rather than get an AI definition for this too, I decided to go to the dictionary Merriam-Webster, which provides the following primary definition (Merriam-Webster Dictionary 2024):\nAnd finally I’ll add the term “data science” (curiously, rarely abbreviated DS). I wanted to give you a Merriam-Webster definition here, but the term isn’t in their dictionary as of writing this on October 28, 2024. So instead I’m going to use Cambridge Dictionary’s definition (Cambridge Dictionary 2024):\nClearly, different people/entities may have different ideas of what AI, ML, and data science may mean. Some people may use these terms to describe generative tools like ChatGPT, GitHub copilot, and Dall-E (or similar products developed by other entities). Others use these terms to refer to more purpose-built algorithms like AlphaGo (for playing Go) or GraphCast (for weather prediction).2 Many academics use these terms to describe the study of the underlying mathematical and programming ideas on which such products are built.\nMy goal is not to give you a single definition of any of these terms and then to argue that my definition is more correct than any other definition. The point I want to make is that it’s worth being clear about how we define these terms in any given context, whether it be in a textbook, a news article, or perhaps a spirited discussion between friends. To that end, I now want to make clear what it is that we will and will not cover in this class, which is a “foundations”-level course in the College of Engineering’s AI minor.\nThe focus of this class will be the mathematical and programming foundations of scientific machine learning, which I define as the study of algorithms which use scientific data to define computational tools that perform useful scientific tasks. The main scientific task that we will focus on in this course is the task of predictive simulation, which seeks to predict the behavior or outcomes of scientific or engineering systems. Predictive simulation is a key task in engineering disciplines like design and control, where we seek to predict design outcomes and control responses in order to make decisions about design parameters and control inputs. The class of machine learning methods that we will focus on in this class will therefore be regression methods, which use data to define relationships between inputs and outputs that allow us to take a specified input and issue a prediction for the output.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#whats-in-a-name-artificial-intelligence-machine-learning-data-science-and-the-scope-of-this-course.",
    "href": "01_intro.html#whats-in-a-name-artificial-intelligence-machine-learning-data-science-and-the-scope-of-this-course.",
    "title": "1  Introduction",
    "section": "",
    "text": "Exercise\n\n\n\nConsider the two different definitions of “AI” above carefully. In what (if any) senses are they (a) exactly the same, (b) similar, (c) somewhat different, (d) very different? What consequences might these differences have (a) developing AI algorithms, (b) evaluating AI impacts, (c) creating AI policies?\n\n\n\n\na computational method that is a subfield of artificial intelligence and that enables a computer to learn to perform tasks by analyzing a large dataset without being explicitly programmed\n\n\n\nthe use of scientific methods to obtain useful information from computer data, especially large amounts of data\n\n\n\n\n\n\n\nExercise\n\n\n\nFor both the terms “machine learning” and “data science”, find an alternative definition from a source that is not a generative AI. What differences exist between the new definitions you’ve found and the ones I’ve cited above?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#regression-methods-an-overview",
    "href": "01_intro.html#regression-methods-an-overview",
    "title": "1  Introduction",
    "section": "1.2 Regression methods: an overview",
    "text": "1.2 Regression methods: an overview\n\n1.2.1 Motivation and problem description\nWe use the notation \\(z\\in \\mathbb{R}^d\\) to denote a real-valued vector of \\(d\\) input variables, and the notation \\(y \\in\\mathbb{R}\\) to denote a real-valued scalar output variable. Our goal is to be able to predict the value of \\(y\\) if we know the value(s) of the input variable \\(z\\). Some examples:\n\nin aerodynamic modeling, \\(z\\) could contain airfoil geometry parameters like camber and thickness, and \\(y\\) could represent the lift coefficient. Being able to predict \\(y\\) from \\(z\\) enables engineers to choose more aerodynamically efficient designs.\nin orbital dynamics, \\(z\\) could represent orbital parameters like altitude and inclination, and \\(y\\) could represent the total time a satellite spends outside the sun’s shadow in a given time period. Being able to predict \\(y\\) from \\(z\\) enables engineers to determine if a satellite’s solar panels will generate enough power to support the satellite’s mission.\nin chemical process engineering, \\(z\\) could represent conditions within a reactor like temperature and chemical mixture properties and \\(y\\) could represent the reaction rate. Being able to predict \\(y\\) from \\(z\\) enables engineers to design more efficient reactors.\n\n\n\n\n\n\n\nExercise\n\n\n\nPropose your own example scenario where it would be useful to predict real-valued outputs from inputs, drawing on your own experience, e.g. in personal projects, previous coursework, work/internship/co-op experiences, or extracurriculars. What would \\(z\\) and \\(y\\) represent? What does predicting \\(y\\) from \\(z\\) enable in your scenario?\n\n\nMathematically, predicting \\(y\\) from \\(z\\) amounts to defining a function \\(f\\) that takes in a \\(d\\)-dimensional input and outputs a scalar. Mathematical notational shorthand for this is \\(f:\\mathbb{R}^d\\to\\mathbb{R}\\). This function \\(f\\) is often called a model. The question at hand is: how do we choose \\(f\\)? There are many ways to do so:\n\nAt a baseline, you could just make up a model: let’s just say \\(y = f(z) = \\|z\\|^2\\). This is mathematically valid, but it’s probably a bad model for the three example scenarios above, because this model probably issues predictions that are very different from the true outputs.\nAlternatively, you could develop a model based on physical principles (a “physics-based” model) – if you have taken classes in (aero/thermo)dynamics, then you would have learned some ways to calculate \\(y\\) from \\(z\\) in the above examples, e.g. based on potential flow theory, rigid body dynamics, or the Arrhenius equation. These models are likely to be more accurate than our made-up model above, although they are often imperfectly accurate because they make simplifying assumptions. One drawback of physics-based models is that they may be computationally expensive (some computational fluid dynamics simulations require many thousands of hours of supercomputing time). Additionally, fully physics-based models may not even exist in some applications (e.g., there are many aspects of plasma physics for which we currently lack a complete theoretical understanding).\nFinally, our focus will be on using data to define a model (a “data-driven” model or a “learned” model). We assume that we have a data set consisting of \\(N\\) pairs of input and output data, \\((z_i,y_i)\\) for \\(i = 1,\\ldots,N\\). To define our model, we want to choose an \\(f\\) so that the predicted output \\(f(z_i)\\) is close to the output data \\(y_i\\) for all the data in our data set. This is sometimes called “fitting” the model to data. The advantages of data-driven models are that they may be significantly cheaper than physics-based models, and they can be fit to experimental data even when we lack scientific theory for developing physics-based models. The disadvantages are that data-driven models require data – and the amount of data that would be “enough” to ensure that the learned model is accurate or useful is highly dependent on the application.\n\nLearning how to fit models to data and assess their accuracy and usefulness is the focus of this course.\n\n\n1.2.2 Mathematical setting and problem formulation\nHot take: Machine learning methods, at their core, are simply very complicated calculators.\nOur goal is to understand the calculations that these methods are carrying out, which is going to enable us to understand and assess both the successes and failures of the methods. This means we want to be precise about the mathematical problem formulation, that is, the characterization of the math problem that is being solved by the methods.\nFormulating a regression problem requires three ingredients:\n\nPaired input and output data: let \\(N\\) be the number of pairs, and let \\(z_1,\\ldots,z_N\\) denote the \\(N\\) input data and \\(y_1,\\ldots,y_N\\) denote the \\(N\\) corresponding outputs. It is common to notate the set of all these data pairs as \\(\\{(z_i,y_i)\\}_{i=1}^N\\).\nThe definition of a parametrized model class – more details on this in a moment, but you should think of this as a set of possible functions that map \\(z\\) to \\(y\\).\nA method for choosing a model from within the chosen model class – in regression problems this is most frequently done by solving an appropriate optimization problem to pick the “best” possible model from within the class.\n\nLet’s focus on the second ingredient, the parametrized model class. We use the mathematical term “class” in a similar way to the mathematical term “set”. For example, perhaps you will have heard before that the numbers \\(1,2,3,\\ldots\\) all the way to infinity form the “set of natural numbers” (often denoted \\(\\mathbb{N}\\)). As another example, \\(\\{H,T\\}\\) can denote the set of possible outcomes of flipping a two-sided coin (H for heads, T for tails). Note that it is common to use curly braces \\(\\{\\cdot\\}\\) to enclose a set (notice where it appears in our shorthand notation for the data set!). Similarly, we can define a set of possible functions that map \\(z\\) to \\(y\\), for example (assuming \\(d=1\\) so that \\(z\\) is a scalar for the moment):\n\\[\n\\mathcal{F}:= \\{z^2, \\sin(z), \\exp(z)\\}\n\\]\nIn the above expression, the notation \\(:=\\) indicates that \\(\\mathcal{F}\\) is being defined as the expression that follows \\(:=\\), that is, \\(\\mathcal{F}\\) is the set of three possible functions: \\(f(z)=z^2\\), \\(f(z)=\\sin(z)\\), and \\(f(z)=\\exp(z)\\). You see that it might be cumbersome to write the “\\(f(z)=\\)” part of every function, so that part often gets dropped.\nBut so far I have still only been saying “set” — what distinguishes a “class” of potential models from a “set” of potential models? Usually we use the term “class” to denote a set of functions that share certain properties or functional forms, for example (still assuming \\(z\\) is scalar for the moment) the class of all quadratic functions:\n\\[\n\\{c_0 + c_1 z + c_2 z^2 : c_0,c_1,c_2\\in\\mathbb{R}\\}\n\\qquad(1.1)\\]\nThe way to read this notation above is as the set of all functions of the form \\(f(z) = c_0 + c_1 z + c_2 z^2\\) that can be obtained when \\(c_0,c_1,c_2\\) are allowed to take on any real values.3 We call this the class of all quadratic functions because by varying \\(c_0,c_1,c_2\\) we always get a quadratic function (noting that with our mathematics hats on, linear functions are just special cases of quadratic functions where the second-order coefficient is zero). Thus, functions within the class share a functional form, unlike the set \\(\\mathcal{F}\\) we defined above. However, we could use the functions in the set \\(\\mathcal{F}\\) to construct an alternative model class:\n\\[\n\\{a z^2 + b\\sin(z) + c\\exp(z) : a,b,c\\in\\mathbb{R}\\}\n\\qquad(1.2)\\]\nor, allowing \\(z\\) to be a \\(d\\)-dimensional vector again, we could consider the following model class:\n\\[\n\\{\\max(0,a^\\top (Wz + b)): W\\in\\mathbb{R}^{d\\times d}, \\,a,b\\in\\mathbb{R}^d\\}\n\\qquad(1.3)\\]\n\n\n\n\n\n\nExercise\n\n\n\nHow would you read and understand the two classes of functions notated above?\n\n\nNotice that the notation we have been using to define function classes follows a pattern: within the brackets, before the colon comes the form of the function, followed by a list of some variables that appear in the function definition, together with a specification of what values those variables are allowed to take on. We call the post-colon variables parameters, and the set of all values that the parameters are allowed to be chosen from is called the parameter space. Any of the classes of functions we have defined above can be described as a parametrized model class, because they define sets of functions with a shared form, where the individual functions within the set arise from varying the values of the parameters.\nLet me describe some common notational conventions that I want you to be familiar with.45 It is common to use a single lowercase letter (usually a Greek one) to denote the set of all parameters – e.g., \\(\\beta = \\{c_0,c_1,c_2\\}\\) or \\(\\theta = \\{a,b,W\\}\\), and the corresponding capital letter to denote the corresponding parameter space, i.e., \\(B = \\mathbb{R}^3\\) and \\(\\Theta = \\mathbb{R}\\times \\mathbb{R}\\times \\mathbb{R}^{d\\times d}\\). Where we might write \\(y = f(z)\\) for a general abstract model \\(f\\), we will typically write \\(y = f(z;\\beta)\\) or \\(y = f(z;\\theta)\\) to indicate a model parametrized by \\(\\beta\\) or \\(\\theta\\) (note that we separate inputs from parameters using a semicolon).\n\nIf you’ve heard the term “model architecture” used to describe a machine learning model, what this means is the specification of the parametrized model class. As the term “architecture” suggests, choosing the model class is a design choice, and different model classes have varying advantages and disadvantages in terms of best possible performance, difficulty to train (more on that shortly), computational cost, and more. We’ll cover many possible choices of model class, and explore their pros and cons, throughout the course.\nNow we will turn our attention to the third ingredient, the method of selecting a model from within the chosen class. Because parametrized models are defined by their parameters, the choice of model amounts to choosing the parameter values. Again, you could just make up the parameter values (choose your favorite numbers), but this is unlikely to match the data well and unlikely to yield accurate predictions. In some cases you could also derive the parameter values based on physical principles (this is possible when models in the chosen class have some physical interpretation). But our focus will be on choosing parameters that define a model that “best matches” the available data. A basic and standard version of this parameter selection task is to solve the following minimization problem:\n\\[\n\\theta^* = \\arg\\min_{\\theta\\in\\Theta}\\frac 1N \\sum_{i=1}^N (f(z_i;\\theta)-y_i)^2\n\\]\nNote that for the above expression to be well-defined, we need our first two ingredients – that is, we need our data set, and we need to have already chosen the parametrized model class, which gives us an expression for the parametrized function \\(f(z;\\theta)\\). Once we have those ingredients, this expression defines \\(\\theta^*\\) as the parameter that minimizes the average square error of the learned model over the available data. In this sense, this parameter \\(\\theta^*\\) defines a learned model \\(f(z;\\theta^*)\\) that “best fits” the data (it is better than all other models in its class).\nAs we progress through this course, we will learn more about how we can solve the above optimization (it is different for different model architectures), and about alternative ways to select the parameter based on alternative minimization problems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#a-foundation-for-learning-about-ml-beyond-parametrized-regression",
    "href": "01_intro.html#a-foundation-for-learning-about-ml-beyond-parametrized-regression",
    "title": "1  Introduction",
    "section": "1.3 A foundation for learning about ML beyond parametrized regression",
    "text": "1.3 A foundation for learning about ML beyond parametrized regression\nI want to emphasize that this class is not and indeed cannot (realistically) be an exhaustive introduction to machine learning, artificial intelligence, or data science. Even the community of professionals who describe themselves as working in “scientific machine learning” is quite broad in scope and would include folks working on topics that I do not intend to cover in this class. There are many topics within AI/ML/data science we will not cover (beyond perhaps at most a very surface level), including generative modeling, decision trees, unsupervised learning methods like clustering, and many more. The philosophy of this course is to empower you to learn about these other methods through future coursework or independent study, by providing a foundation in what I term the `three pillars of artificial intelligence’:\n\nMathematics: all machine learning methods are fundamentally solving mathematical problems at their core. The first pillar of AI is to be able to mathematically define the problem one seeks to solve.\nComputation: once a mathematical problem is defined, an ML method must define an algorithm that carries out the calculations to solve the problem. The second pillar of AI is to define an algorithm that (approximately) solves the mathematical problem.\nHuman intelligence: this is the crucial third pillar of AI. Human intelligence is vital for defining mathematical problem formulations, designing algorithms, and assessing the results and iteratively updating the mathematical problem formulation and computational algorithm to achieve desired outcomes.\n\nWe will seek to develop a firm foundation in these three pillars of AI throughout the course.\n\n\n\n\nCambridge Dictionary. 2024. “Data Science.” 2024. https://dictionary.cambridge.org/dictionary/english/data-science.\n\n\nMerriam-Webster Dictionary. 2024. “Machine Learning.” 2024. https://www.merriam-webster.com/dictionary/machine%20learning.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#course-structure-and-intended-learning-outcomes",
    "href": "01_intro.html#course-structure-and-intended-learning-outcomes",
    "title": "1  Introduction",
    "section": "1.4 Course structure and intended learning outcomes",
    "text": "1.4 Course structure and intended learning outcomes\n\nlearning outcomes\nproblem solving review - meant to develop your skills in demonstrating human intelligence\n\n\n\n\nCambridge Dictionary. 2024. “Data Science.” 2024. https://dictionary.cambridge.org/dictionary/english/data-science.\n\n\nMerriam-Webster Dictionary. 2024. “Machine Learning.” 2024. https://www.merriam-webster.com/dictionary/machine%20learning.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#footnotes",
    "href": "01_intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "It seems apt to use AI to define AI, and I’m quoting it for illustrative purposes here, but I do want to point out that Google’s AI Overview is not at all transparent about how these responses are generated and thus does not meet the standards for being cited as a source in most publishing venues.↩︎\nthis paragraph lists examples that came to me most quickly and thus reflects to some extent my own cognitive biases based on the media I’ve consumed. I welcome suggestions of other examples of AI/ML/data science to include that are less well-known – Email me!↩︎\nFollowing the same notational convention, we could write the data set as \\(\\{(z_i,y_i) : i =1,\\ldots,N\\}\\), which we would read as the set of all pairs \\((z_i,y_i)\\) for \\(i = 1,\\ldots,N\\), but the notation \\(\\{(z_i,y_i)\\}_{i=1}^N\\) is more compact and more common. I don’t make the rules. 🤷‍♀️↩︎\nThese are common conventions, but the ML community is comprised of a diverse group of scientists and engineers across disciplines, and it’s very common to read things that use entirely different notation. This is unfortunately the reality of things, and one of the reasons I am going to push you in this class to be comfortable describing the underlying concepts precisely using different notations - that is, you are going to develop flexibility in re-assigning meaning to different symbols as we go along (I hope).↩︎\nI’m making an effort to explicitly define notation when I introduce it, and to give examples of how to read and understand it, but it’s easy for me to miss things, so please speak up or write us a message if you have a question.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "11_ls.html",
    "href": "11_ls.html",
    "title": "2  Linear Least Squares Problems",
    "section": "",
    "text": "2.1 Classifying regression problems as linear vs nonlinear\nWe begin our exploration of scientific machine learning methods in the fundamental setting of linear regression problems. The intended learning outcomes of these notes are that students should be able to:\nWe have previously introduced regression problems in a general way: recall that the three ingredients of (parametrized) regression problems are (1) paired input and output data, (2) the choice of a parametrized model class, and (3) a method for choosing a model from within that class. The classification of a regression problem as linear or nonlinear depends solely on ingredient (2), the parametrized model class: if the models in that class depend linearly on the model parameters, the regression problem is a linear regression problem.\nThe classification of regression problems as linear or nonlinear depends solely on the dependence of the functions in the parametrized model class on the parameters. That is, we can define functions that are linear in \\(\\theta\\) while being nonlinear in \\(z\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "11_ls.html#classifying-regression-problems-as-linear-vs-nonlinear",
    "href": "11_ls.html#classifying-regression-problems-as-linear-vs-nonlinear",
    "title": "2  Linear Least Squares Problems",
    "section": "",
    "text": "Check your knowledge: Do you remember what it means for a function to depend linearly on a variable?\n\n\n\n\n\nThe function \\(f(z;\\theta): \\mathbb{R}^d\\times\\Theta\\to\\mathbb{R}\\) is said to be linear in the parameters \\(\\theta\\) if, for all \\(a,b\\in\\mathbb{R}\\) and all \\(\\theta_1,\\theta_2\\in\\Theta\\), the following holds: \\(f(z; a\\theta_1 + b\\theta_2) = af(z;\\theta_1) + bf(z;\\theta_2)\\).\nNote that when we say a function is “linear”, we have to specify in what. That is, we can also say \\(f(z;\\theta)\\) is linear in the inputs if, for all \\(a,b\\in\\mathbb{R}\\) and all \\(z_1,z_2\\in\\mathbb{R}^d\\), the following holds: \\(f(az_1 + bz_2;\\theta) = af(z_1;\\theta)+bf(z_2;\\theta)\\).\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nConsider the model classes (Equation 1.1)-(Equation 1.3) introduced previously. Are these model classes linear or nonlinear in the parameters? In the inputs?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "11_ls.html#mathematical-problem-formulation",
    "href": "11_ls.html#mathematical-problem-formulation",
    "title": "2  Linear Least Squares Problems",
    "section": "2.2 Mathematical problem formulation",
    "text": "2.2 Mathematical problem formulation\nWe are now going to introduce an abstract mathematical problem formulation that can be used to describe many specific instances of linear regression problems. This is a theme of the course and throughout computational mathematics and engineering: abstraction using the language of mathematics lets us isolate the core essence of the problem we’re solving and develop powerful algorithms that can solve specific applications of those problems across a wide range of disciplines. I’ll introduce the abstract formulation first, and follow it up with some specific examples.\nIngredient 1 (the data set): let \\(\\{(z_i,y_i)\\}_{i=1}^N\\) be a given data set of paired inputs \\(z_i\\in\\mathbb{R}^d\\) and outputs \\(y_i\\in\\mathbb{R}\\).\nIngredient 2 (the parametrized model class): let \\(x:\\mathbb{R}^d\\to\\mathbb{R}^n\\) be a function that maps the \\(d\\)-dimensional input to an \\(n\\)-dimensional feature vector. For a fixed \\(x\\), we will consider the following parametrized model class:\n\\[\n\\mathcal{F}_\\beta := \\{ x(z)^\\top \\beta : \\beta\\in\\mathbb{R}^n\\}\n\\qquad(2.1)\\]\nRecall that Equation 2.1 is read as “\\(\\mathcal{F}_\\beta\\) is defined to be the set of all functions \\(f(z;\\beta) = x(z)^\\top\\beta\\) for all \\(\\beta\\in\\mathbb{R}^n\\).” This is an abstract way to define the model class for any linear regression problem, as we will describe in more detail shortly.\nIngredient 3 (the method of choosing the parameters): let \\(\\beta^*\\) be given by\n\\[\n\\begin{aligned}\n\\beta^* &= \\arg\\min_{\\beta\\in\\mathbb{R}^n} \\frac1N \\sum_{i=1}^N (f(z_i;\\beta) - y_i)^2 \\\\\n&= \\arg\\min_{\\beta\\in\\mathbb{R}^N} \\frac1N \\sum_{i=1}^N (x(z_i)^\\top\\beta - y_i)^2.\n\\end{aligned}\n\\qquad(2.2)\\] Then, we define the learned model to be \\(f(z;\\beta^*) = x(z)^\\top\\beta^*\\). We call \\(\\beta^*\\) defined this way the “optimal regression parameters” or just the “optimal parameters”, because they are the result of solving an optimization problem. Note that the objective of this optimization function is defined by the data, and represents the average squared error of the model over the data set.\nTaken together, the three ingredients I have defined above define a linear least squares problem, which is a subclass of linear regression problems. We’ll now give several examples of specific instances of linear least squares problems to illustrate how broadly applicable this abstract framework is.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "11_ls.html#examples",
    "href": "11_ls.html#examples",
    "title": "2  Linear Least Squares Problems",
    "section": "2.3 Examples",
    "text": "2.3 Examples\n\n2.3.1 Example 1: Aerodynamic drag prediction\n\nAn important task in aerodynamic design is predicting lift and drag forces on a body moving through air. Let’s consider a simplified problem where we are given an fixed airfoil design and our goal is to predict the drag force \\(F_d\\) on the airfoil as a function of three parameters which describe its flight conditions:\n\nThe angle of attack \\(\\alpha\\)\nThe density of the fluid \\(\\rho\\)\nThe freestream velocity of the air \\(v\\)\n\nTo put this problem in our abstract framework, we define \\(z = (\\alpha,\\rho,v)^\\top\\) to be a three-dimensional input, and take the output to be the drag force \\(y= F_d\\). In order to define a regression problem, we require the existence of a data set \\(\\{(z_i,y_i)\\}_{i=1}^N\\). Note that in this case \\(z_i = (\\alpha_i,\\rho_i,v_i)\\). We assume that this data set is given.\nIn linear regression problems, defining the model class amounts to choosing a set of regression features by defining \\(x\\). A simple choice takes the inputs themselves to be features: \\(x^{(1)}(z) = z = (\\alpha,\\rho,v)^\\top\\). This leads to a class of parametrized models with a three-dimensional unknown parameter vector \\(\\beta\\in\\mathbb{R}^3\\). The models in this class have the following form:\n\\[\nf^{(1)}(z;\\beta) = x^{(1)}(z)^\\top\\beta = (\\alpha,\\rho,v) \\beta = \\beta_1\\alpha + \\beta_2\\rho + \\beta_3 v.\n\\]\nThere are many other possible choices. For example, consider \\(x^{(2)}(z) = (\\alpha,\\rho, v, \\alpha^2, \\rho^2, v^2, \\alpha\\rho, \\alpha v, \\rho v)^\\top\\). This leads to a parametrized model class with a nine-dimensional unknown parameter vector \\(\\beta\\in\\mathbb{R}^9\\):\n\\[\n\\begin{aligned}\nf^{(2)}(z;\\beta) &= x^{(2)}(z)^\\top\\beta = (\\alpha,\\rho, v, \\alpha^2, \\rho^2, v^2, \\alpha\\rho, \\alpha v, \\rho v)\\beta \\\\\n&= \\beta_1\\alpha + \\beta_2\\rho + \\beta_3 v + \\beta_4\\alpha^2 + \\beta_5\\rho^2 + \\beta_6 v^2 + \\beta_7\\alpha\\rho + \\beta_8\\alpha v + \\beta_9 \\rho v\n\\end{aligned}\n\\]\nFor either the above choices of features \\(x^{(1)}(z)\\) or \\(x^{(2)}(z)\\), we could then define and solve the minimization Equation 2.2 to find the optimal regression parameters and define our learned model \\(f^{(1)}(z;\\beta^*)\\) or \\(f^{(2)}(z;\\beta^*)\\).\n\n\n\n\n\n\nUnderstanding notation\n\n\n\nNote that we use \\(\\beta\\) to denote the unknown parameters in both \\(f^{(1)}\\) and \\(f^{(2)}\\) above despite \\(\\beta\\) referring to different quantities in the definition of the different functions. This is a common notational shortcut — while we could use the notation \\(\\beta^{(1)}\\in\\mathbb{R}^3\\) and \\(\\beta^{(2)}\\in\\mathbb{R}^9\\) to specify the different \\(\\beta\\) for the different functions, this can be cumbersome if we are considering many different options for the choice of features \\(x(z)\\), and it’s standard to just use \\(\\beta\\), where the definition of \\(\\beta\\) is implied by the context. One of the challenges in learning about machine learning and computational mathematics more generally is getting used to similar notation meaning different things in different contexts. That’s one of the things that we’ll practice in this course.\n\n\n\n\n2.3.2 Example 2: Monitoring the Maximum Displacement of a Bridge\nLeast-squares regression is a powerful tool for estimating unknown quantities in many scientific and engineering disciplines beyond Aerospace. Let’s instead say we are interested in accurately estimating the maximum displacement at the midpoint of a bridge under various input-conditions measured by sensors along the bridge. Suppose we can accurately gather the following inputs:\n\nLoad applied to the bridge, \\(L\\)\nTemperature at the midpoint of the bridge, \\(T\\)\nDominant frequency in the bridge, \\(f\\)\n\nWe are interested in quickly estimating the Maximum Displacement (mm), \\(D\\), from these measurements. As we can see, the context of our problem has changed entirely, but we can still fit it into our existing framework by defining the input vector \\(z=(L, T, f)^\\top\\) and output \\(y=D\\). If we take \\(N\\) measurements throughout the year under various environmental conditions, we can again form some training dataset \\(\\{(z_i, y_i)\\}_{i=1}^N\\) which we can use to define some feature-set \\(x(z)\\) and then solve for the optimal weights that map \\(x(z)\\) to \\(y\\) identically to how we did in Example 1.\n\n\n2.3.3 Example 3: Detecting Cardiac Arrhythmia using Health-Monitoring Devices\nMany health issues are often difficult to detect without extensive testing. However, with the increase in biometric data available from devices such as smartphones and smart-watches, it has become increasingly feasible to predict health conditions by leveraging this information. \\(V O_2\\) max is a measure of the maximum rate at which a person can use oxygen during exercising and is considered a standard measure of cardiovascular fitness. However, to measure this experimentally requires an extensive laboratory test that measures the volume of oxygen a patient breathes during exercising. Let’s now suppose we have developed a smart-watch that gathers the following health-data from its user:\n\nMedian daily Step-Count \\(s\\)\nResting Heart Rate \\(r\\)\nRespiratory Rate \\(b\\)\n\nFrom these measurements, we are interested in accurately estimating the \\(VO_2\\) max of a patient. Let’s say we have a large sample of many patients of various ages and demographics who have worn our smart-watch for a long period of time. For the \\(i\\) th patient, suppose we have \\(s_i\\), \\(r_i\\) and \\(b_i\\) collected from the device and we have measured each patient’s actual \\(VO_2\\) max in a laboratory setting, which we denote \\(y_i\\). We then form a training dataset, \\(\\{(z_i, y_i)\\}_{i=1}^N\\) and solve the least-squares regression problem using the discrete values for \\(y\\) exactly how we did in Example 1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "11_ls.html#solving-linear-least-squares-problems",
    "href": "11_ls.html#solving-linear-least-squares-problems",
    "title": "2  Linear Least Squares Problems",
    "section": "2.4 Solving linear least squares problems",
    "text": "2.4 Solving linear least squares problems\nLinear least squares problems are special because they have closed form solutions: that is, we can write an analytical expression for the optimum parameters \\(\\beta^*\\) in terms of the data. To do so, we are going to define a feature data matrix \\(X\\in\\mathbb{R}^{N\\times n}\\) and an output data vector \\(Y\\in\\mathbb{R}^N\\) as follows:\n\\[\nX = \\begin{pmatrix}\n- &x^\\top(z_1) & - \\\\\n& \\vdots & \\\\\n- & x^\\top(z_N) & -\n\\end{pmatrix},\n\\qquad\nY = \\begin{pmatrix}\ny_1 \\\\ \\vdots \\\\ y_N\n\\end{pmatrix},\n\\] where the rows of \\(X\\) and the elements of \\(Y\\) correspond to input-output pairs in the data set. Note that each column of \\(X\\) corresponds to a different feature defined by an element of the vector-valued function \\(x\\).\nUsing this notation, Equation 2.2 can be rewritten as\n\\[\n\\beta^* = \\arg\\min_{\\beta\\in\\mathbb{R}^n} \\frac1N\\|X \\beta - Y\\|^2\n\\qquad(2.3)\\]\nThis is the minimization of a multivariate function (because \\(\\beta\\) is a vector). Recall from calculus that to find minimizers of multivariate functions we first seek critical points that satisfy the first-order necessary conditions for optimality: that is, we look for points where the derivative of the objective function is 0. Let \\(\\mathcal{L}(\\beta) = \\frac1N\\|X\\beta-Y\\|^2 = \\frac1N(X\\beta - Y)^\\top (X\\beta-Y)\\). Then,\n\\[\n\\frac{\\partial\\mathcal{L}}{\\partial \\beta} = \\frac2N(X^\\top X\\beta - X^\\top Y)\n\\]\nSetting the derivative equal to 0 yields the standard normal equations1:\n\\[\nX^\\top X\\beta^* = X^\\top Y\n\\]\nIf \\((X^\\top X)\\) is invertible, then the unique solution to the normal equations is given by\n\\[\n\\beta^* = (X^\\top X)^{-1} X^\\top Y\n\\]\nand this choice of \\(\\beta\\) defines our final learned model: \\(f(z;\\beta^*) = x(z)^\\top\\beta^*\\).\n\n2.4.1 Using Python for Least-Squares Regression\nNow let’s explore how we can actually solve for our optimal weights, \\(\\beta^*\\), given some training data with code. First, we need to import the libraries we need to work with matrices/vectors (numpy), plot results (matplotlib), work with dataframes (pandas), and easily perform more math than base python allows like \\(\\pi\\), sines, cosines, and square-roots (math).\n\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport pandas as pd\nimport math\n\nNow let’s say we’ve imported some pandas dataframe called df that contains a set of empirical data from Example 1 (Assume it comes from experimental trials or simulations). We can print the first few rows of the dataset using the df.head() method:\n\ndf.head()\n\n\n\n\n\n\n\n\nalpha\nrho\nvelocity\nf_drag\n\n\n\n\n0\n16.854305\n1.319114\n79.481130\n9425.204935\n\n\n1\n42.782144\n1.203298\n50.263709\n13722.379248\n\n\n2\n32.939727\n1.320528\n57.690388\n12863.441979\n\n\n3\n26.939632\n0.696729\n49.251769\n3644.076532\n\n\n4\n7.020839\n1.227098\n19.524299\n186.612122\n\n\n\n\n\n\n\nLet’s define our input features as \\(x(z) = (\\alpha, \\rho, v)^\\top\\). We can now form \\(X\\) (our training inputs) and \\(Y\\) (our training outputs) matrices by extracting the input features and outputs from the dataframe:\n\n# The hstack method concatenates column vectors into \n# an N x d matrix like [1 alpha rho velocity]\nX = np.hstack((np.ones((N, 1)), df[['alpha', 'rho', 'velocity']].values))\nY = df['f_drag'].values\n\nWe can solve for \\(\\beta^*\\) using the np.linalg.lstsq() function which efficiently solves the least-squares equation \\(X \\beta = Y\\). The rcond keyword argument specifies how “well-conditioned” we want our result to be, achieved by cutting off the smallest singular values of \\(X\\) (don’t worry too much about this for now). Setting it to None assumes \\(X\\) is well-conditioned i.e. far from being rank-deficient. By default, the lstsq() function returns a list of the optimal weights, \\(\\beta^*\\), the residuals of the training data (\\(X \\beta - Y\\)), the rank of \\(X\\), and the singular values of \\(X\\). For now, we’re only interested in \\(\\beta^*\\), so we use [0] to select this value.\n\nbeta_star = np.linalg.lstsq(X, Y, rcond=None)[0]\n\nWe are now able to compute the model’s predictions at the training inputs:\n\nY_hat = X @ beta_star\n\nLet’s now plot \\(\\hat{{Y}}\\) and \\({Y}\\) to examine how closely correlated the two are. A straight line with slope 1 would mean the linear regression model exactly matched all of the training outputs:\n\nplt.figure(figsize=(8,4))\nplt.scatter(Y, Y_hat)\nplt.grid()\nplt.xlabel(\"True Drag Force\")\nplt.ylabel(\"Model Predicted Drag Force\")\n\nText(0, 0.5, 'Model Predicted Drag Force')\n\n\n\n\n\n\n\n\n\nThis plot shows that there is clearly a strong, but nonlinear relationship between the true and model-predicted drag-force. We observe that the linear regression model under-predicts the drag-force when the true force is very low or very high and the model over-predicts the drag-force when the true force is around 5000N. This shows that when the relationship between the input features and the output is nonlinear, linear regression may provide good estimates on average, but choosing features to better capture this nonlinearity may provide much more accurate outputs over the entire input domain.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "11_ls.html#assessing-the-learned-models",
    "href": "11_ls.html#assessing-the-learned-models",
    "title": "2  Linear Least Squares Problems",
    "section": "2.5 Assessing the learned models",
    "text": "2.5 Assessing the learned models\n\n2.5.1 Common Performance Metrics\nOnce we have trained a model (in the linear regression case, we have computed the optimal \\(\\beta\\) that maps our training inputs to our training outputs), it’s important to have some quantitative way to measure how “well” our model is able to do this. There are many, many ways to measure model performance. For regression problems, the popular sklearn python package for machine learning has thirteen different metrics to assess model performance. We will outline some of the most prominent metrics in this section.\nA natural way to assess model performance is to measure the error between the model’s predictions and actual outputs. One common way to do this is computed is with mean squared error:\n\\[\n\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2\n\\]\nwhere \\(\\hat{y}_i\\) is the model’s best approximation of \\(y_i\\), using inputs \\(x_i\\). Because of how linear regression is formulated, the estimator \\(\\beta^*\\) will be the set of weights that minimize the MSE of the training data. If \\(X\\) is full-rank, \\(\\beta^*\\) will be unique. However, many data-scientists dislike the MSE because it penalizes higher errors more than lower errors and in the context of physical experiments the squared error doesn’t have easily interpretable units. For instance, in Example 1 we sought to predict drag-force in Newtons. Using the MSE would produce an error with units of (Newtons)\\(^2\\) which doesn’t have an intuitive physical interpretation. For this reason, many scientists use the mean absolute error:\n\\[\n\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^N |\\hat{y}_i - y_i|\n\\]\nIn Example 1, this would give us an error with units of Newtons that we can interpret as “the linear regression model deviates from the true outputs by ___ Newtons on average”. If the output data significantly varies in magnitude over the domain of the input, then we may expect MSE and MAE to dominate in places where the magnitude of \\(y\\) is large. To combat this problem, we might choose to use mean relative error to assess the performance of our model. This scales the error at each input/output pair according to how large the output is:\n\\[\n\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N \\frac{|\\hat{y}_i - y_i|}{|y_i|}\n\\]\nAt the end of 2.4, we plotted each training output, \\(y_i\\), on the x-axis of a plot and the model’s best guess (\\(\\hat{y}_i\\)) on the y-axis. If the model was able to perfectly match each \\(y_i\\), then we should expect a perfect line with a slope of 1 (\\(\\hat{y}=y\\)). Using the pearson correlation coefficient, often denoted \\(R\\) or the coefficient of determination, \\(R^2\\), we can assess how accurately the model is able to match a given set of outputs. We can compute this as:\n\\[\nR = \\frac{\\sum (\\hat{y}_i -\\bar{\\hat{y}})(y_i - \\bar{y})}{\\sqrt{\\sum (\\hat{y}_i -\\bar{\\hat{y}})^2 \\sum (y_i - \\bar{y})^2}}\n\\]\nWhen \\(\\hat{y}\\) perfectly matches the true outputs, \\(y\\), we get a pearson correlation coefficient of \\(R=R^2=1.0\\). Anything lower indicates an imperfect match between the model’s predictions and the true outputs.\n\n\n2.5.2 Cross-Validation\nUp to now, we have only discussed model performance as it relates to the data used to train the model. However, the entire reason we go to the trouble of training machine learning models is so they can be used to make accurate predictions on unseen data. Famous LLMs like ChatGPT are so valuable because we can ask them questions they have never seen before and they can still provide coherent, useful answers.\nOftentimes, when machine learning models are highly expressive, (i.e. can model complex functional relationships) they tend to overfit to their training data. This happens when the model is so good at matching its training data, it does not generalize well to data it hasn’t seen before. This can be especially problematic when our training data contains some sort of noise or uncertainty and our trained model begins to conform to the noise present in the training data.\nA straightforward method to assess how much a model is overfitting its training data is to form a train and test set. Usually around 80% of the training data is used to train the model and about 20% is kept out of the training process and used to evaluate the model’s ability to generalize to data it hasn’t seen before. A model that generalizes well should exhibit approximately the same performance on its testing data as its training data. When a model shows significantly worse performance on its test dataset, this indicates that the model might be overfitting its training data.\nLet’s refer back to Example 1, when we were trying to accurately predict the force of drag on an airfoil. Say we break up the training data (X and Y) into (Xtrain, Ytrain) and (Xtest, Ytest); one dataset to train the model and one to evaluate it. Suppose we receive the following values for Mean Absolute Error on the training and test predictions:\n\nTraining MAE: 61.456 N\nTesting  MAE: 595.328 N\n\nThis shows that the model exhibits significantly lower error on its training data, and much higher on data it hasn’t seen before. This is typically a dead giveaway of overfitting.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "11_ls.html#exercises",
    "href": "11_ls.html#exercises",
    "title": "2  Linear Least Squares Problems",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\n\nRecall that the loss-function for least-squares regression is given by \\(\\mathcal{L}(\\beta) = \\frac 1N|| X \\beta - Y||_2^2\\). Show that the gradient of \\(\\mathcal{L}\\) with respect to \\(\\beta\\) is given by \\(\\nabla_\\beta \\mathcal{L} = \\frac2N \\left( X^\\top X \\beta - X^\\top Y\\right)\\)\nHint: you may find the following identities from the Matrix Cookbook (Petersen and Pedersen 2012) useful. For \\(v,u\\in\\mathbb{R}^n\\) and \\(A\\in\\mathbb{R}^{n\\times n}\\), the following hold:\n\n\\(\\nabla_v (v^\\top A v) = 2Av\\)\n\\(\\nabla_v (v^\\top u) = \\nabla_v(u^\\top v) = u\\)\n\nFor an extra-thorough learning experience, try deriving the above identities from the definitions of matrix-vector and dot products.\n\nConsider a matrix \\(A \\in \\mathbb{R}^{n \\times m}\\). Which of the following are necessary conditions for \\(A\\) to be invertible? Which are sufficient?\n\n\\(A\\) is a square matrix, i.e. \\(n = m\\)\n\nThe columns of \\(A\\) span \\(\\mathbb{R}^{n}\\)\nThe columns of \\(A\\) are linearly independent\n\\(A\\) has no linearly dependent columns\nThe null-space of \\(A\\) is empty\nThe only solution of \\(A X = 0\\) is \\(X = 0\\). \n\n\n\n\nShow that \\(X^\\top X\\) is invertible if and only if \\(X\\) has full column-rank. \n\n\n\nExplain why \\(\\beta^*\\) is the unique solution to the normal equations if \\((X^\\top X)\\) is invertible. If \\((X^\\top X)\\) is not invertible, describe all solutions to the normal equations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "11_ls.html#further-reading",
    "href": "11_ls.html#further-reading",
    "title": "2  Linear Least Squares Problems",
    "section": "2.7 Further reading",
    "text": "2.7 Further reading\nLinear Algebra Crash Course (?sec-linalg)\nThe Matrix Cookbook (Petersen and Pedersen (2012))\n\n\n\n\n\nPetersen, Kaare Brandt, and Michael Syskind Pedersen. 2012. “The Matrix Cookbook.” https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "11_ls.html#footnotes",
    "href": "11_ls.html#footnotes",
    "title": "2  Linear Least Squares Problems",
    "section": "",
    "text": "note that the constant factor \\(\\frac2N\\) drops out. It’s common to play fast and loose with multiplicative constants in minimizations – other sources define the minimization objective with a \\(\\frac12\\) multiplier, which leads to the derivative not having the factor of 2 in front and leads to the same critical point. Or you may see the objective function defined without the \\(1/N\\) in front, which still leads to the same critical point.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "12_lslinalg.html",
    "href": "12_lslinalg.html",
    "title": "3  LLS: A linear algebra perspective",
    "section": "",
    "text": "3.1 Mathematical problem formulation\nWe now take a deeper dive into the structure of linear least squares problems. We will see how theoretical concepts from linear algebra can be used to describe and explain properties of learned linear regression models.\nThe intended learning outcomes of these notes are that students should be able to:\nWe briefly recap the mathematical problem formulation that we consider. Given a data set \\(\\{(z_i,y_i)\\}_{i=1}^N\\), where \\(z_i\\in\\mathbb{R}^d\\) and \\(y_i\\in\\mathbb{R}\\) are paired inputs and outputs, we choose a feature mapping \\(x: \\mathbb{R}^d\\to\\mathbb{R}^n\\). We seek to find coefficients \\(\\beta\\in\\mathbb{R}^n\\) so that the model\n\\[\nf(z;\\beta) = x(z)^\\top\\beta = \\sum_{j=1}^n \\beta_j x_j(z)\n\\]\naccurately predicts the output \\(y\\). To do this, we define the matrix \\(X\\in\\mathbb{R}^{N\\times n}\\) and vector \\(Y\\in\\mathbb{R}^N\\),\n\\[\nX = \\begin{pmatrix}\n- &x^\\top(z_1) & - \\\\\n& \\vdots & \\\\\n- & x^\\top(z_N) & -\n\\end{pmatrix},\n\\qquad\nY = \\begin{pmatrix}\ny_1 \\\\ \\vdots \\\\ y_N\n\\end{pmatrix},\n\\qquad(3.1)\\]\nand write the least-squares problem as follows:\n\\[\nX\\beta = Y\n\\qquad(3.2)\\]\nThis is a different notation than the minimization (Equation 2.2) introduced earlier, but is also fairly common. One way to think of the expression Equation 3.2 is to note that our goal is to find \\(\\beta\\) so that \\(X\\beta\\) is as close to \\(Y\\) is possible – where ideally we would have \\(X\\beta = Y\\). But Equation 3.2 has a more precise and technical meaning. The goal of these notes is to explain that precise technical definition and its implications for model learning.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLS: A linear algebra perspective</span>"
    ]
  },
  {
    "objectID": "12_lslinalg.html#well-posedness-of-xbeta-y",
    "href": "12_lslinalg.html#well-posedness-of-xbeta-y",
    "title": "3  LLS: A linear algebra perspective",
    "section": "3.2 Well-posedness of \\(X\\beta = Y\\)",
    "text": "3.2 Well-posedness of \\(X\\beta = Y\\)\nIn math, the term “well-posedness” refers to the existence and uniqueness of solutions to a problem. We say a mathematical problem is “well-posed” if it has a unique solution. Existence and uniqueness of solutions may seem like abstract theoretical concepts, but as we will soon see, they have very practical consequences for the accuracy and reliability of the models we learn.\n\n3.2.1 Well-posedness of square linear systems\nSuppose for a moment that \\(N = n\\). Then the square linear system \\(X\\beta = Y\\) describes a system of \\(n\\) linear equations for \\(n\\) unknowns (the elements of the unknown coefficient vector \\(\\beta\\in\\mathbb{R}^n\\)). The problem of solving \\(X\\beta = Y\\) for \\(\\beta\\) is well-posed if and only if \\(X\\) is invertible. In this case, the solution exists and is uniquely given by \\(\\beta=X^{-1}Y\\).\nRemember that in our setting, \\(X\\) is a matrix of feature data (one row per data point), and \\(Y\\) is a vector with the corresponding output data. For square invertible \\(X\\), the existence of a solution to \\(X\\beta=Y\\) means that there is a choice of \\(\\beta\\) so that the learned model will interpolate the data, i.e., the model will satisfy\n\\[\nf(z_i;\\beta) = y_i \\quad \\text{for all }i = 1,\\ldots,N.\n\\]\nThe fact that the solution is unique means there is only one choice of \\(\\beta\\) that perfectly matches the data. Imagine for a moment if there were more than one choice of \\(\\beta\\) that perfectly matched the data: how would you choose which \\(\\beta\\) to use for predicting model outputs at new inputs?\nWhat if \\(X\\) is not invertible? This is clearly the case in the general linear regression problem we have set up where \\(N\\neq n\\) and \\(X\\) is rectangular, but \\(X\\) can also be non-invertible and square. There are essentially two ways \\(X\\) can fail to be invertible:\n\n\\(X\\) can have column rank less than \\(n\\), i.e., the number of linearly independent columns in \\(X\\) is less than the number of columns \\(n\\). In this case, we say \\(X\\) is “column rank-deficient” or \\(X\\) “does not have full column rank”.\n\\(X\\) can have row rank less than \\(N\\), i.e., the number of linearly independent rows in \\(X\\) is less than the number of rows \\(N\\). In this case, we say \\(X\\) is “row rank-deficient” or \\(X\\) “does not have full row rank”.\n\nThese are not mutually exclusive: \\(X\\) can be simultaneously column rank-deficient and row rank-deficient. However, these different types of rank deficiencies have different consequences for well-posedness the existence and uniqueness of solutions, and different implications for learning linear regression models.\n\n\n3.2.2 Overdetermined least-squares problems\nLet’s start with the row rank-deficient case. If \\(X\\) has fewer linearly independent rows than the total number of rows, this means that some of the rows of \\(X\\) are redundant. This means that some of the linear equations represented by rows of the linear system \\(X\\beta=Y\\) have redundant left-hand sides. Because redundant equations place more constraints on the solution than can be simultaneously satisfied, this case is called the overdetermined case.\nOne way overdetermined linear systems can arise is if we have multiple copies of the same input in the data set, i.e., suppose \\(z_1 = z_2\\). Note that having multiple copies of the same input datum usually does not mean that the associated output data are also copies of each other (so you can still have \\(y_1 \\neq y_2\\)): for example, you might take multiple experimental measurements with the same configuration and get different measured values due to sensor noise or external factors that you don’t account for in your inputs. If this happens, your data set contains “contradictory” information, because \\(x(z_1)^\\top\\beta=y_1\\) and \\(x(z_2)^\\top\\beta= x(z_1)^\\top\\beta=y_2\\) cannot simultaneously be true.\nWhat does row rank-deficiency mean for solving \\(X\\beta=Y\\)? In most cases this means that there is no choice of \\(\\beta\\) that will exactly solve the equation — the equality cannot simultaneously hold for all rows of the system. That means no solution exists for the linear system. This means there is no choice of \\(\\beta\\) that can perfectly match all the data.\nInstead of looking for \\(\\beta\\) that yields a perfect match to the data, the least-squares problem formulation looks to minimize the misfit to the data. Let’s define the residual of the system as follows:\n\\[\nr(\\beta) = X\\beta-Y.\n\\qquad(3.3)\\]\nFor a given parameter \\(\\beta\\), the residual norm \\(\\|r(\\beta)\\|\\) is a measure of the model misfit over the training data set. Note that if \\(X\\beta=Y\\) holds with equality for all rows, then the residual (norm) is zero. In the overdetermined case where \\(X\\beta=Y\\), since we can’t have a zero residual, we aim for the next best thing, which is minimizing the size of the residual (recall that the norm is a measure of the size of a vector)1:\n\\[\n\\beta^* = \\arg\\min_{\\beta\\in\\mathbb{R}^n} \\|r(\\beta)\\| = \\arg\\min_{\\beta\\in\\mathbb{R}^n}\\|r(\\beta)\\|^2\n\\qquad(3.4)\\]\nWith the residual defined in Equation 3.3, the expression Equation 3.4 recovers our original expression for the formulation of a least-squares problem2, Equation 2.2. In this sense, the least-squares formulation “fixes” the lack of existence of a solution to the linear system due to row redundancy. For this reason, if you see \\(X\\beta=Y\\) written for a non-invertible system3, the way you should understand that is in the sense of solving Equation 2.2.\n\n\n\n3.2.3 Underdetermined least-squares problems\nWe now turn our attention to the column rank-deficient case. If \\(X\\) has fewer linearly independent columns than the total number of columns, this means that some of the columns of \\(X\\) are redundant. Recall that \\(X\\beta\\in\\mathbb{R}^N=\\sum_{j=1}^n \\beta_j x_j\\), where \\(x_j\\in\\mathbb{R}^N\\) is the \\(j\\)th column of \\(X\\). If \\(X\\) has redundant columns, this means that there are different ways to weight the columns of \\(X\\) using the coefficients in \\(\\beta\\) that lead to the same model predictions. This means that requiring \\(X\\beta=Y\\) will not fully specify what \\(\\beta\\) is, so we call this the underdetermined case.\nOne way underdetermined systems arise is if there are more unknown coefficients in \\(\\beta\\) than there data points to constrain the problem. This can happen if we choose a very large set of features (\\(n\\) large) but only have a small amount of data (\\(N&lt;n\\)).\nWhat does column rank-deficiency mean for solving \\(X\\beta=Y\\)? This means that the solution is not unique. This non-uniqueness comes from the fact that column rank-deficiency of \\(X\\) means that \\(X\\) has a nonzero kernel (aka nullspace). Recall that this means there exists some nonzero \\(v\\in\\mathbb{R}^n\\) satisfying \\(Xv = 0\\). Let \\(r^*\\) be the minimum possible residual,\n\\[\nr^* = \\min_{\\beta\\in\\mathbb{R}^n}\\|r(\\beta)\\|.\n\\]\nSuppose we find some \\(\\beta^*\\) that leads to this minimum residual, so that \\(r^* = X\\beta^* - Y\\). Well, if \\(v\\) lies in the kernel of \\(X\\), then we can add any multiple of \\(v\\) to \\(\\beta^*\\) and get the same minimum residual! That is, if \\(\\beta^*\\) is a solution to Equation 3.4, then so is \\(\\beta^* + av\\) for any \\(a\\in\\mathbb{R}\\). This means there are infinitely many solutions. To make underdetermined problems well-posed, we have to decide on a way of choosing a single unique solution out of the many possible minimizers of Equation 3.4.\nDeciding on a method of choosing a single unique solution out of many possible solutions is called regularization and is a topic we will explore in more depth later. In the meantime, the most common and vanilla way of regularizing is to specify that we want to find what’s called the “minimum-norm” solution: i.e., out of all possible solutions, we take the smallest one. That is,\n\\[\n\\beta^* = \\arg\\min_{\\beta\\in\\mathbb{R}^n} \\|\\beta\\| \\quad \\text{subject to } \\|X\\beta - Y\\| = r^*.\n\\qquad(3.5)\\]\n\n\n\n3.2.4 Summary of linear least-squares problem formulation\nFor \\(X\\) and \\(y\\) defined as in Equation 3.1, to solve \\(X\\beta=Y\\) means to find the minimum-norm least-squares solution as described in Equation 3.5. This is generally what is happening under the hood when you use any scientific computing or machine learning software that has a built-in least-squares function, including\n\nscipy.linalg.lstsq\nnumpy.linalg.lstsq\nMATLAB’s “backslash operator” \\ also known as mldivide\n\nIn a later chapter we will introduce some of the basic algorithms that are used to find (minimum-norm) least-squares solutions that these functions use. For now you should focus on understanding the precise mathematical definition of the least squares solution given in Equation 3.5.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLS: A linear algebra perspective</span>"
    ]
  },
  {
    "objectID": "12_lslinalg.html#feature-selection-expressivity-accuracy-and-well-posedness",
    "href": "12_lslinalg.html#feature-selection-expressivity-accuracy-and-well-posedness",
    "title": "3  LLS: A linear algebra perspective",
    "section": "3.3 Feature selection: expressivity, accuracy, and well-posedness",
    "text": "3.3 Feature selection: expressivity, accuracy, and well-posedness\n\nYou may wonder why you should bother understanding what the mathematical solution is when there are built-in functions that just give it to you. One reason is that you can debug machine learning code much more effectively if you understand what the code is supposed to be doing theoretically. Another reason is that considerations of well-posedness have big implications for your job as an intelligent human designer of machine learning models.\nFor linear regression models, your job as an intelligent human is to decide on the feature mapping \\(x:\\mathbb{R}^d \\to\\mathbb{R}^n\\), which determines what kinds of terms appear in your learned model. Examples of features can include:\n\nmonomials, e.g., \\(1, z, z^2, z^3, \\ldots\\),\nFourier features, e.g. \\(\\sin(\\pi z), \\cos(\\pi z), \\sin(2\\pi z),\\cos(2\\pi z), \\ldots\\),\nphysics-based features based on problem knowledge,\nor any other function of \\(z\\) that you choose.\n\n\n\nThe choice of feature mapping \\(x\\) determines the expressivity of the model class. The term “expressivity” describes the set of functions that the model class can represent. A model class is considered highly expressive if it can represent many different types of functions. For example, recall that any continuous function can be arbitrarily well-approximated by an infinite sum of polynomial terms. Thus, the features \\(x_j : z \\mapsto z^j\\) for \\(j = 0,1,2,\\ldots, n-1\\) would be highly expressive if \\(d\\) is large. Fourier-based features have a similar approximation result – if our features are based on truncating a Fourier series at some frequency, the more features we have, the more expressive the model class would be.\nGenerally, highly expressive model classes are better able to match the training data than less expressive model classes. This is because there are more degrees of freedom (regression coefficients) that we can tweak to lower the mismatch between the model prediction and the training data. However, the more features we have, the greater the risk of overfitting, that is, achieving a very low error on the training data with a model that will not perform well on unseen (test) data. Overfitting is especially a risk for underdetermined problems, where there are more features than data points, but can also happen even when the problem is well-determined or over-determined.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLS: A linear algebra perspective</span>"
    ]
  },
  {
    "objectID": "12_lslinalg.html#exercises",
    "href": "12_lslinalg.html#exercises",
    "title": "3  LLS: Solutions",
    "section": "3.4 Exercises",
    "text": "3.4 Exercises\n\nsome scenarios to classify as under or over-determined or both",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLS: Solutions</span>"
    ]
  },
  {
    "objectID": "12_lslinalg.html#footnotes",
    "href": "12_lslinalg.html#footnotes",
    "title": "3  LLS: A linear algebra perspective",
    "section": "",
    "text": "Note that the second equality follows because squaring the norm doesn’t change where the minimum is because \\(a^2\\) is monotone increasing in \\(a\\) for non-negative \\(a\\).↩︎\nup to a multiplying constant anyway, which I’ve emphasized before really doesn’t matter when it’s inside a minimization.↩︎\nsee, for example, the documentation for the numpy function linalg.lstsq or the scipy version↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLS: A linear algebra perspective</span>"
    ]
  },
  {
    "objectID": "13_lsnla.html",
    "href": "13_lsnla.html",
    "title": "4  LLS: Computational Cost",
    "section": "",
    "text": "4.1 Measuring computational cost\nIn this section we will introduce fundamental algorithms from computational linear algebra that are used to solve linear systems and linear least squares problems. We will also introduce and discuss some of the key properties and theoretical considerations when using these algorithms.\nThe intended learning outcomes of these notes are that students should be able to:\nA key property of any algorithm is its computational cost. The two types of cost we are interested in are time and memory: the former describes how long an algorithm takes to run, and the latter describes how many bytes of RAM we must have available in order to run the computation. For both of these measures, we are interested in understanding how the time and memory costs change as the dimensions of the problem (the number of data \\(N\\) or the number of features \\(d\\)) change. These properties are often referred to as the (time or memory) complexity of the algorithm, or as the (time or memory) cost scaling of the algorithm.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLS: Computational Cost</span>"
    ]
  },
  {
    "objectID": "20_ugbo.html",
    "href": "20_ugbo.html",
    "title": "5  Unconstrained Gradient Based Optimization (UGBO)",
    "section": "",
    "text": "6 Problem Motivation\nIn this lecture, we will be focusing on the following optimization problem,\n\\[\\min_x f(x)\\]\nwhere \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) is our objective function (or loss function), and \\(x\\in\\mathbb{R}^n\\) is a vector of parameters (variables) which we can change. Minimization problems arise in numerous physical and real-world circumstances, including:\nFor this lecture, our approach for solving the above minimization problem will be a gradient-based one. This means that we will make use of information from the gradient of \\(f\\) (first-order information), and the curvature of \\(f\\) (second-order information).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Unconstrained Gradient Based Optimization (UGBO)</span>"
    ]
  },
  {
    "objectID": "20_ugbo.html#what-are-we-looking-for",
    "href": "20_ugbo.html#what-are-we-looking-for",
    "title": "5  Unconstrained Gradient Based Optimization (UGBO)",
    "section": "7.1 What are we looking for?",
    "text": "7.1 What are we looking for?\nTo start, we first need to motivate some fundamental definitions regarding the nature of the solution we are looking for.\nDefinition: We say that a point \\(x^\\star\\) is a global minimizer of \\(f\\) if: \\[f(x^\\star)\\leq f(x) \\text{ for all }x\\in \\mathcal{D}\\subseteq\\mathbb{R}^n.\\]\nAlthough a global minimizer is the most desireable outcome from our minimization problem, finding a global minimizer is often quite challenging. This is because a global minimizer may not always exist, and because we often only possess local information about the function \\(f\\). This motivates the following defintion:\nDefinition: We say that a point \\(x^\\star\\) is a local minimizer of \\(f\\) if there exists some open set \\(\\mathcal{B}\\), that contains \\(x^\\star\\), such that: \\[\nf(x^\\star)\\leq f(x) \\text{ for all }x\\in\\mathcal{B}.\n\\] If the inequality becomes strict, then \\(x^\\star\\) is referred to as a strict local minimizer. To illustrate the above definitions, let us look at the function \\(f(x) = 2 + \\cos(x) + \\frac{1}{2} \\cos\\left(5x - \\frac{1}{2}\\right)\\) over the domain \\(\\mathcal{D} = [0, 3.7]\\). In this domain, the function has two local minimizers and one global minimizer, as illustrated in the plot below.\n\n\n\n\n\n\n\n\n\nWe now have a way to understand and characterize our solutions. However, the above definitions suggest that we need to examine all points that are next to \\(x^\\star\\) in order to properly characterize it. However, if our function is smooth, i.e., its higher order derivatives exist and are continuous, then we can classify \\(x^\\star\\) by examining the derivatives of \\(f\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Unconstrained Gradient Based Optimization (UGBO)</span>"
    ]
  },
  {
    "objectID": "20_ugbo.html#gradient",
    "href": "20_ugbo.html#gradient",
    "title": "5  Unconstrained Gradient Based Optimization (UGBO)",
    "section": "7.2 Gradient",
    "text": "7.2 Gradient\nRecall that the gradient of a function \\(f(x)\\), denoted \\(\\nabla f(x)\\), is the column vector of partial derivaties with respect to each parameter: \\[\n\\nabla f(x) =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_1}\\\\\n\\vdots\\\\\n\\frac{\\partial f}{\\partial x_i}\\\\\n\\vdots\\\\\n\\frac{\\partial f}{\\partial x_n}\n\\end{bmatrix}\n\\] Each element of \\(\\nabla f(x)\\) captures the rate of change of the function \\(f\\) with respect to the parameter \\(x_i\\). The gradient of a function helps us identify local minimizers and allows us to state (without proof) the first fundamental result of gradient-based optimization.\nTheorem (first-order necessary conditions): If \\(x^\\star\\) is a local minimizer and \\(f\\) is continuously differentiable in an open nieghborhood of \\(x^\\star\\), then \\(\\nabla f(x^\\star) = 0\\).\nA point \\(x^\\star\\) is called a stationary point if \\(\\nabla f(x^\\star) = 0\\). The above theorem guarantees that all local minimizers must be stationary points. However, the theorem cannot guarantee that all stationary points are local minimizers. This is because the conditions are necessary but not sufficient. As a counterexample, a stationary point may be a local maximizer instead of a local minimizer. In order to obtain necessary and suffient conditions, we need to move one derivative higher.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Unconstrained Gradient Based Optimization (UGBO)</span>"
    ]
  },
  {
    "objectID": "20_ugbo.html#curvature",
    "href": "20_ugbo.html#curvature",
    "title": "5  Unconstrained Gradient Based Optimization (UGBO)",
    "section": "7.3 Curvature",
    "text": "7.3 Curvature\nThe curvature of a function \\(f\\) is the rate of change of the gradient, and tells us whether the slope is stationary, increasing, or decreasing. To compute the curvature of \\(f\\), we need to take a partial derivate of every component of the gradient with respect to each coordinate direction. This leads to an \\((n\\times n)\\) matrix of second partial derivatives called the Hessian:\n\\[\nH(x) = \\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}\n\\]\nIf our function \\(f\\) has continuous second partial derivatives, then the order of differentiation does not matter and the matrix \\(H\\) becomes symmetric. The Hessian allows us to establish two more (stronger) fundamental results of gradient-based optimization. Before stating them, let us recall that a matrix \\(A\\) is positive definite if \\(x^\\top A x &gt; 0\\) for all \\(x\\neq 0\\) and positive semidefinite if \\(x^\\top A x \\geq 0\\) for all \\(x\\).\nTheorem (second-order necessary conditions): If \\(x^\\star\\) is a local minimizer of \\(f\\) and the Hessian \\(H\\) exists and is continuous in an open neighborhood of \\(x^\\star\\), then \\(\\nabla f(x^\\star) = 0\\) and \\(H\\) is positive semidefinite.\nTheorem (second-order sufficient conditions): Suppose that \\(H\\) is is continuous in an open neighborhood of \\(x^\\star\\) and that \\(\\nabla f(x^\\star) =0\\) and \\(H\\) is positive definite. Then, \\(x^\\star\\) is a strict local minimizer of \\(f\\).\nIt is important to remark that the second-order sufficient condition gurantee a stronger result when compared to the second-order necessary condtions. Namely, the second-order sufficient conditions lead to a strict local minimizer. Furthermore, we should also note that the sufficient condtions are not necessary; it is possible for a point \\(x^\\star\\) to be a strict local minimizer but fail to satisfy the sufficient conditions. As an example, consider \\(f(x) = x^6\\), where \\(x^\\star = 0\\) is a strict local minimizer, yet the Hessian evaluates to zero at \\(x^\\star = 0\\) and is therefore not positive definite.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Unconstrained Gradient Based Optimization (UGBO)</span>"
    ]
  },
  {
    "objectID": "20_ugbo.html#a-complete-example",
    "href": "20_ugbo.html#a-complete-example",
    "title": "5  Unconstrained Gradient Based Optimization (UGBO)",
    "section": "7.4 A complete example",
    "text": "7.4 A complete example\nHere, we work our way through an example problem where we can see how the concepts from above can be applied to help us identify and characterize critical points. Consider the function:\n\\[\nf(x_1, x_2) = 0.5x_1^4 + 2x_1^3 + 1.5x_1^2 + x_2^2 - 2x_1x_2\n\\]\nLet’s plot the contours of \\(f\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function\ndef f(x1, x2):\n    return 0.5 * x1**4 + 2 * x1**3 + (3/2) * x1**2 + x2**2 - 2 * x1 * x2\n\n# Create the contour plot\nx = np.linspace(-4, 2, 1000)\ny = np.linspace(-4, 2, 1000)\nX, Y = np.meshgrid(x, y)\nZ = f(X, Y)\n\nplt.figure(figsize=(9.5, 5))\ncontour = plt.contour(X, Y, Z, levels=75)\nplt.xlabel(r'$x_1$')\nplt.ylabel(r'$x_2$')\n\nText(0, 0.5, '$x_2$')\n\n\n\n\n\n\n\n\n\nTo start, we need to identify all critical points of \\(f\\). We can do that by solving for all points such that \\(\\nabla f(x) = 0\\), i.e.,\n\\[\n\\nabla f(x_1, x_2) =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2}\n\\end{bmatrix} =\n\\begin{bmatrix}\n2x_1^3 + 6x_1^2 + 3x_1 - 2x_2 \\\\ 2x_2 - 2x_1\n\\end{bmatrix} =\n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}\n\\]\nWe can do that by using fsolve() from scipy.optimize.\n\nfrom scipy.optimize import fsolve\n\n# Define the first partial derivatives\ndef df_dx1(x1, x2):\n    return 2 * x1**3 + 6 * x1**2 + 3 * x1 - 2 * x2\n\ndef df_dx2(x1, x2):\n    return 2 * x2 - 2 * x1\n\n# Define the system of equations\ndef equations(vars):\n    x1, x2 = vars\n    return [df_dx1(x1, x2), df_dx2(x1, x2)]\n\n# Solve for critical points\ninitial_guesses = [(0, 0), (-1, -1), (-10, -10)]\ncritical_points = [fsolve(equations, guess) for guess in initial_guesses]\n\nprint(critical_points)\n\n[array([0., 0.]), array([-0.17712434, -0.17712434]), array([-2.82287566, -2.82287566])]\n\n\nNow, we need to classify each critical point. To do so, let’s compute the Hessian:\n\\[\nH(x_1, x_2) =\n\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n6x_1^2 + 12x_1 + 3 & -2 \\\\\n-2 & 2\n\\end{bmatrix}\n\\]\nWe now need to evaluate the Hessian at each one of the critical points and check whether is it positive semidefinite. There are a number of ways we can check this, but we will be examining the eigenvalues of the Hessian and checking whether they are all positive.\n\n# Define the Hessian\ndef Hessian(x1, x2):\n    return np.array([[6 * x1**2 + 12 * x1 + 3, -2], [-2, 2]])\n\neigs = [np.linalg.eigvals(Hessian(pt[0], pt[1])) for pt in critical_points]\n\nprint(eigs)\n\n[array([4.56155281, 0.43844719]), array([-0.5227962 ,  3.58554227]), array([17.20040482,  1.73684911])]\n\n\nBased on these results, we know that the first and third critical points are local minimizers. The second critical point is classifies as a saddle point because the Hessian is not positive semidefinite. To find out which of the local minimizers is the global minimzer, we evaluate the function at each of these points. Let’s go ahead and label the points accordingly.\n\n# Create the contour plot\nx = np.linspace(-4, 2, 1000)\ny = np.linspace(-4, 2, 1000)\nX, Y = np.meshgrid(x, y)\nZ = f(X, Y)\n\nplt.figure(figsize=(9.5, 5))\ncontour = plt.contour(X, Y, Z, levels=75)\nplt.xlabel(r'$x_1$')\nplt.ylabel(r'$x_2$')\n\n# Plot and label the critical points\nlabels = [\"local minimum\", \"saddle point\", \"global minimum\"]\n\nfor (x, y), label in zip(critical_points, labels):\n    plt.scatter(x, y, label=f'{label} ({x:.2f}, {y:.2f})', s=100)\n    if label == \"saddle point\":\n        plt.text(x, y, f' {label}', fontsize=12, ha='right', weight='bold')\n    else:\n        plt.text(x, y, f' {label}', fontsize=12, ha='left', weight='bold')\n\nplt.grid(False)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Unconstrained Gradient Based Optimization (UGBO)</span>"
    ]
  },
  {
    "objectID": "21_ugbo2.html",
    "href": "21_ugbo2.html",
    "title": "6  Unconstrained Gradient Based Optimization (UGBO) II - Steepest Descent",
    "section": "",
    "text": "7 Problem Motivation\nSo far, we have developed analytical tools that allow us to find and classify critical points of functions. However, this analytical approach to finding minima is often infeasible, esecially when the objective function is a result of a numerical model. To combat this, we turn to interative algorithms that prgressively work towards finding a minimum while only relying on function and gradient evaluations. The main idea is that we will start from some initial guess \\(x_0\\) and produce a sequence of points \\(x_1, x_2, x_3, \\ldots, x_j, \\ldots\\), eventually converging to some local minimizer \\(x^\\star\\).\nThere are two major classes of iterative optimization methods that produce the aforementioned sequence of points: line search methods and trust region methods. In this course, we will focus on the former, but we will provide some resources for those interested in the latter. Line search methods consist of three main steps that occur at each iteration:\nThe whole process is summarized in the diagram below.\nflowchart LR\n  A(Starting guess) --&gt; B(Search direction)\n  B --&gt; C(Step size)\n  C --&gt; D(Is this a minimum?)\n  D --&gt;|Yes| E(Converged)\n  D --&gt;|No| F(Update values)\n  F --&gt; B\nSteps 1 and 2 can be understood as two separate subproblems in the overall optimization scheme. In this note, we will examine the first of these subproblems by introducing the steepest descent algorithm, and seeing how the subproblem of determining a suitable step size arises as a natural consequence.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Unconstrained Gradient Based Optimization (UGBO) II - Steepest Descent</span>"
    ]
  },
  {
    "objectID": "21_ugbo2.html#motivation",
    "href": "21_ugbo2.html#motivation",
    "title": "6  Unconstrained Gradient Based Optimization (UGBO) II - Steepest Descent",
    "section": "8.1 Motivation",
    "text": "8.1 Motivation\nGiven some objective function \\(f\\), recall that the gradient \\(\\nabla f\\) is a vector with each component quantifying the function’s local rate of change with respect to each variable.\nFact: The gradient is a vector that points to the direction that yields the greatest function increase from the current point.\nIdea: From any given point \\(x\\), we can find the direction of steepest descent by taking \\(-\\nabla f(x)\\). So, we can define our search direction at iteration \\(k\\) as: \\(p_k = -\\nabla f(x_k)\\)\nOne problem with the above idea is that the gradient does not give us any information regarding the step size we should take. Hence, the search direction is often normalized as:\n\\[\np_k = -\\frac{\\nabla f(x_k)}{\\|\\nabla f(x_k)\\|_2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Unconstrained Gradient Based Optimization (UGBO) II - Steepest Descent</span>"
    ]
  },
  {
    "objectID": "21_ugbo2.html#algorithm",
    "href": "21_ugbo2.html#algorithm",
    "title": "6  Unconstrained Gradient Based Optimization (UGBO) II - Steepest Descent",
    "section": "8.2 Algorithm",
    "text": "8.2 Algorithm\nNeed to include an algorithm block here.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Unconstrained Gradient Based Optimization (UGBO) II - Steepest Descent</span>"
    ]
  },
  {
    "objectID": "21_ugbo2.html#example",
    "href": "21_ugbo2.html#example",
    "title": "6  Unconstrained Gradient Based Optimization (UGBO) II - Steepest Descent",
    "section": "8.3 Example",
    "text": "8.3 Example\nConsider the quadratic function: \\[\nf(x_1, x_2) = x_1^2 + \\beta x_2^2\n\\]\nFirst, let us consider the case where \\(\\beta = 1\\) and the starting point is \\(x_0 = (10,1)\\).\n\n\n0.0002487577548903103\n\n\nText(0, 0.5, '$x_2$')\n\n\n\n\n\n\n\n\n\nThis is a test citation (Martins and Ning 2021).\n\n\n\n\nMartins, Joaquim RRA, and Andrew Ning. 2021. Engineering Design Optimization. Cambridge University Press.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Unconstrained Gradient Based Optimization (UGBO) II - Steepest Descent</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Cambridge Dictionary. 2024. “Data Science.”\n2024. https://dictionary.cambridge.org/dictionary/english/data-science.\n\n\nMerriam-Webster Dictionary. 2024. “Machine\nLearning.” 2024. https://www.merriam-webster.com/dictionary/machine%20learning.\n\n\nPetersen, Kaare Brandt, and Michael Syskind Pedersen. 2012. “The\nMatrix Cookbook.” https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf.\n\n\nStrang, Gilbert. 2022. Introduction to Linear Algebra.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "00_linalg.html",
    "href": "00_linalg.html",
    "title": "Appendix A — Linear algebra review",
    "section": "",
    "text": "A.1 Scalars, Vectors, and Matrices",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linear algebra review</span>"
    ]
  },
  {
    "objectID": "00_linalg.html#scalars-vectors-and-matrices",
    "href": "00_linalg.html#scalars-vectors-and-matrices",
    "title": "Appendix A — Linear algebra review",
    "section": "",
    "text": "A.1.1 Notation\nLet’s introduce some notation: \\(\\mathbb{R}\\) is the set of real numbers, \\(\\mathbb{C}\\) is the set of complex numbers, and the operator \\(\\in\\) designates that a variable belongs to a set. To define a real scalar (i.e. just a number), \\(c\\), we write:\n\\[ c \\in \\mathbb{R}\\]\nYou can read this as “the variable \\(c\\) belongs to the set of 1-dimensional real numbers.”\nTo define a real \\(n\\) dimensional vector, \\(\\mathbf{x}\\), we write:\n\\[ \\mathbf{x}  = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\in \\mathbb{R}^n \\]\nTo define a \\(m \\times n\\) matrix \\(\\mathbf{A}\\) of real-valued entries, we write:\n\\[ \\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & & a_{2n} \\\\\n\\vdots & & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n }\n\\]\n\n\nA.1.2 The Transpose\nThe “transpose” of a matrix or vector, swaps the rows and columns. In other words, the first row becomes the first column, the second row the second column and so-on:\n\\[ \\mathbf{x}^\\top = \\begin{bmatrix} x_1 & \\dots & x_n \\end{bmatrix} \\in \\mathbb{R}^{1 \\times n} \\]\n\\[ \\mathbf{A}^\\top = \\begin{bmatrix} a_{11} & a_{21} & \\dots & a_{m1} \\\\\na_{12} & a_{22} & & a_{m2} \\\\\n\\vdots & & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\dots & a_{mn}\n\\end{bmatrix} \\in \\mathbb{R}^{n \\times m } \\]\nTransposing the product of matrices reverses their order:\n\\[ \\left( \\mathbf{A} \\mathbf{B} \\mathbf{C} \\mathbf{D} \\right)^\\top = \\mathbf{D}^\\top \\mathbf{C}^\\top \\mathbf{B}^\\top \\mathbf{A}^\\top \\]\n\n\nA.1.3 Addition & subtraction\nTechnically, only objects of the same dimensionality can be added or subtracted with one another. So, scalars can only be added to scalars, vectors can only be added to vectors of the same dimension, and matrices can only be added to other matrices with the same numbers of rows and columns. Some software like MATLAB or NumPy might let you add scalars to vectors and matrices and under the hood they multiply that scalar by a vector/matrix of ones to make the dimensions match.\n\n\n\n\n\n\nExamples of numpy allowing mathematically invalid operations\n\n\n\n\n\nMathematically, none of the following are valid operations. However, numpy allows us to do them by secretly adding in a vector of ones so that the vector/matrix dimensions are valid. This can be confusing for new programmers.\nAdding a scalar to a vector\n\nimport numpy as np \n# Defining a vector \nu = np.array([\n  1, 2, 3\n])\n\n# Adding a scalar to a vector\nprint(u + 5)\n# What numpy does to make the dimensions match\nprint(u + 5*np.ones_like(u)) \n\n[6 7 8]\n[6 7 8]\n\n\nAdding a row-vector to a column-vector\n\n# Reshaping u to a column-vector \nu = u.reshape((3,1))\n\n# Defining a row-vector, v \nv = np.array([\n  4, 5, 6\n]).reshape((1,3)) \n\n# Adding the row-vector v to a column-vector u\nprint(v + u)\n# What numpy does to make the dimensions match\nprint(np.outer(u, np.ones(3)) + np.outer(np.ones(3), v)) \n\n[[5 6 7]\n [6 7 8]\n [7 8 9]]\n[[5. 6. 7.]\n [6. 7. 8.]\n [7. 8. 9.]]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linear algebra review</span>"
    ]
  },
  {
    "objectID": "00_linalg.html#scalar-multiplication",
    "href": "00_linalg.html#scalar-multiplication",
    "title": "Appendix A — Linear algebra review",
    "section": "A.2 Scalar Multiplication",
    "text": "A.2 Scalar Multiplication\nGenerally, scalars can multiply by any structure (scalar, vector, and matrix) and do not change its dimension. They only “scale” its value by a certain amount. Hence, multiplying a scalar by a scalar returns a scalar, multiplying a scalar times a vector returns a vector, and multiplying a scalar by a matrix returns a matrix. Scalar multiplication is also commutative, i.e. \\(c\\mathbf{A} = \\mathbf{A}c\\) for all scalars \\(c\\). Consider our scalar, \\(c\\), from the previous section, another scalar, \\(b\\), the vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\) and the matrix \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\)\n\nA.2.1 Scalar-Scalar Product\n\\[ b \\times c = bc \\in \\mathbb{R} \\]\n\n\nA.2.2 Scalar-Vector Product\n\\[ c \\mathbf{x} = \\mathbf{x} c = \\begin{bmatrix} c x_1 \\\\ \\vdots \\\\ c x_n \\end{bmatrix} \\in \\mathbb{R}^n \\]\n\n\nA.2.3 Scalar-Matrix Product\n\\[ c \\mathbf{A} = \\mathbf{A} c = \\begin{bmatrix} c a_{11} & c a_{12} & \\dots & c a_{1n} \\\\\nc a_{21} & c a_{22} & & c a_{2n} \\\\\n\\vdots & & \\ddots & \\vdots \\\\\nc a_{m1} & c a_{m2} & \\dots & c a_{mn}\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n } \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linear algebra review</span>"
    ]
  },
  {
    "objectID": "00_linalg.html#matrix-vector-multiplication",
    "href": "00_linalg.html#matrix-vector-multiplication",
    "title": "Appendix A — Linear algebra review",
    "section": "A.3 Matrix & Vector Multiplication",
    "text": "A.3 Matrix & Vector Multiplication\n\nA.3.1 Vector-Vector Multiplication\nTo multiply two matrices (or two vectors, or a matrix with a vector), their inner dimensions must always match. Hence, the only valid way to multiply two vectors is by “inner” or “outer” products. Consider two vectors, \\(\\mathbf{u} \\in \\mathbb{R}^n\\) and \\(x \\in \\mathbb{R}^n\\). An “Inner Product” (sometimes called the Dot-Product) is defined as:\n\\[ \\mathbf{u}^\\top \\mathbf{x} = \\begin{bmatrix} u_1 & \\dots & u_n \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = u_1 x_1 + u_2 x_2 + \\dots u_n x_n \\in \\mathbb{R}\\]\nSome important notes about inner products:\n\nIf the dimensions of \\(\\mathbf{u}\\) and \\(\\mathbf{x}\\) do not match, we cannot multiply them this way as we need to multiply each entry of both vectors.\nThis operation returns a scalar value.\nInner Products are commutative, i.e. \\(\\mathbf{u}^\\top \\mathbf{x} = \\mathbf{x}^\\top \\mathbf{u}\\), as they are sums of scalar-scalar multiplications which are also commutative.\n\nRecall \\(\\mathbf{x} \\in \\mathbb{R}^n\\). Now consider a new vector \\(\\mathbf{w} \\in \\mathbb{R}^m\\) where \\(m \\neq n\\). An “Outer Product” is defined as:\n\\[ \\mathbf{w} \\mathbf{x}^\\top = \\begin{bmatrix} w_1 \\\\ \\vdots \\\\ w_m \\end{bmatrix} \\mathbf{x}^\\top = \\begin{bmatrix} w_1 \\mathbf{x}^\\top  \\\\ \\vdots \\\\ w_m \\mathbf{x}^\\top  \\end{bmatrix} = \\begin{bmatrix} w_1 x_1 & w_1 x_2 & \\dots & w_1 x_n \\\\ w_2 x_1 & w_2 x_2 & & w_2 x_n \\\\ \\vdots & & \\ddots & \\vdots \\\\ w_m x_1 & w_m x_2 & \\dots & w_m x_n \\end{bmatrix} \\in \\mathbb{R}^{m \\times n} \\]\nImportant notes on outer products:\n\nOuter products return matrices with dimensions according to the vectors multiplied.\nAs long as \\(\\mathbf{w}\\) and \\(\\mathbf{x}\\) are vectors, outer products share an inner dimension of \\(1\\), which means that any two vectors have an outer product.\nLastly, this operation is non-commutative, meaning \\(\\mathbf{w} \\mathbf{x}^\\top \\neq \\mathbf{x} \\mathbf{w}^\\top\\). Rather, these two are transposes of each other.\n\n\n\nA.3.2 Matrix-Vector Multiplication\nMultiplying a matrix with a vector means the dimension of the vector must equal the number of columns of the matrix. Recall \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) and \\(\\mathbf{x} \\in \\mathbb{R}^n\\). Note that the columns of \\(\\mathbf{A}\\) match the dimension of \\(\\mathbf{x}\\), which are both size \\(n\\). The product \\(\\mathbf{A}\\mathbf{x}\\) can be written as:\n\\[ \\mathbf{A}\\mathbf{x} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & & a_{2n} \\\\\n\\vdots & & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix} \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} a_{11} x_1 + a_{12} x_2 + \\dots + a_{1n} x_n \\\\  a_{21} x_1 + a_{22} x_2 + \\dots + a_{2n} x_n \\\\ \\vdots \\\\a_{m1} x_1 + a_{m2} x_2 + \\dots + a_{mn} x_n  \\end{bmatrix} \\in \\mathbb{R}^m \\]\nPerhaps a more useful way to visualize this is to break \\(\\mathbf{A}\\) down into its constituent columns and show this as a sum of scalar-vector multiplications:\n\\[ \\mathbf{A}\\mathbf{x} = \\begin{bmatrix} | & | &  & | \\\\\n\\mathbf{a}_1 & \\mathbf{a}_2 & \\dots & \\mathbf{a}_n \\\\\n| & | & & | \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} | \\\\ \\mathbf{a}_1 \\\\ | \\end{bmatrix} x_1 + \\begin{bmatrix} | \\\\ \\mathbf{a}_2 \\\\ | \\end{bmatrix} x_2 + \\dots + \\begin{bmatrix} | \\\\ \\mathbf{a}_n \\\\ | \\end{bmatrix} x_n  \\in \\mathbb{R}^m \\]\nwhere \\(\\mathbf{a}_i\\) is the \\(i\\)th column of \\(\\mathbf{A}\\). Note that matrix-vector multiplication forms a linear map from one dimensional vector-space to another. In this case, \\(\\mathbf{A}\\mathbf{x}\\) maps a vector from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\). This linear transformation from one dimensionality to another is a core component of many machine learning algorithms.\n\n\nA.3.3 Matrix-Matrix Multiplication\nConsider two matrices, \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{n \\times k}\\), where \\(m \\neq k\\). Again, the inner dimensions must match to multiply matrices. Because \\(\\mathbf{A}\\) multiplied by \\(\\mathbf{B}\\) have an inner-dimension of \\(n\\), they can be multiplied. Similar to matrix-vector multiplication, we can visualize the product of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) by breaking up the columns and rows of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), respectively:\n\\[ \\mathbf{A}\\mathbf{B} = \\begin{bmatrix} | & | &  & | \\\\\n\\mathbf{a}_1 & \\mathbf{a}_2 & \\dots & \\mathbf{a}_n \\\\\n| & | & & | \\end{bmatrix} \\begin{bmatrix} - & \\mathbf{b}_1 & - \\\\ - & \\mathbf{b}_2 & - \\\\ & \\vdots & \\\\  - & \\mathbf{b}_n & - \\end{bmatrix} \\in \\mathbb{R}^{m \\times k}\\]\nwhere \\(\\mathbf{a}_i\\) is the \\(i\\)th column of \\(\\mathbf{A}\\) and \\(\\mathbf{b}_i\\) is the \\(i\\)th row of \\(\\mathbf{B}\\). We can expand this expression by multiplying each column of \\(\\mathbf{A}\\) with each row of \\(\\mathbf{B}\\):\n\\[ \\mathbf{A}\\mathbf{B} = \\begin{bmatrix} | \\\\ \\mathbf{a}_1 \\\\ | \\end{bmatrix} \\begin{bmatrix} - & \\mathbf{b}_1 & - \\end{bmatrix} +\\begin{bmatrix} | \\\\ \\mathbf{a}_2 \\\\ | \\end{bmatrix} \\begin{bmatrix} - & \\mathbf{b}_2 & - \\end{bmatrix} + \\dots + \\begin{bmatrix} | \\\\ \\mathbf{a}_n \\\\ | \\end{bmatrix} \\begin{bmatrix} - & \\mathbf{b}_n & - \\end{bmatrix}\\]\nAs we can see, this is a sum of outer products (vectors multiplied by transposed vectors). We know \\(\\mathbf{A}\\) has \\(m\\) rows and \\(\\mathbf{B}\\) has \\(k\\) columns, so each outer product will form a matrix of dimension \\(m \\times k\\):\n\\[ \\mathbf{A}\\mathbf{B} = \\sum_{i=1}^n \\mathbf{a}_i \\mathbf{b}_i \\in \\mathbb{R}^{m \\times k}\\]\nNote: matrix multiplication is generally non-commutative; The product \\(\\mathbf{A}\\mathbf{B}\\) is valid, because the number of columns of \\(\\mathbf{A}\\) matches the number of rows of \\(\\mathbf{B}\\). However, the product \\(\\mathbf{B}\\mathbf{A}\\) is not valid, because the number of columns of \\(\\mathbf{B}\\) does not equal the number of rows of \\(\\mathbf{A}\\). The product \\(\\mathbf{B}\\mathbf{A}\\) can be multiplied if \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are both square matrices of the same size. However, even if the dimensions do match, generally speaking, \\(\\mathbf{BA \\neq AB}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linear algebra review</span>"
    ]
  },
  {
    "objectID": "00_linalg.html#vector-norms",
    "href": "00_linalg.html#vector-norms",
    "title": "Appendix A — Linear algebra review",
    "section": "A.4 Vector Norms",
    "text": "A.4 Vector Norms\nOftentimes, it is useful to know how “big” a vector is. And there are many different ways of computing this. The “norm” of an object is a measure of how “large” it is, according to some rule. Consider \\(\\mathbf{x} \\in \\mathbb{R}^n\\). We’ll start with the most common norm:\n\nA.4.1 The 2-Norm\nThe 2-norm of \\(x\\) (which we will use most-often in this class) is defined as the squareroot of the squares of its entries:\n\\[ ||\\mathbf{x}||_2 = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2} \\]\nThe reason the 2-norm is so useful is the fact that the square of the \\(l_2\\) norm is an inner product of a vector with itself:\n\\[ \\mathbf{x}^\\top \\mathbf{x} = x_1^2 + x_2^2 + \\dots + x_3^2 = ||\\mathbf{x}||_2^2 \\]\nThis helps us write a norm in terms of matrix-vector operations, which is extremely useful for optimization. The 2-norm is also quite important because it is the first \\(l_p\\) norm with a continuous derivative, which cannot be said of the 1-norm since it uses absolute-value functions.\n\n\nA.4.2 The \\(l_p\\)-Norm\nWe can take the definition of the 2-Norm and generalize it to describe a class of norms known as \\(l_p\\) norms. The \\(l_p\\) norm of a vector is defined by the following:\n\\[ ||\\mathbf{x}||_p = \\left( \\sum_{i=1}^n |x_i|^p \\right)^{1/p} \\]\n\n\nA.4.3 The 1-Norm\nThe 1-norm of \\(\\mathbf{x}\\) is simply the sum of the absolute-value of its entries:\n\\[ ||\\mathbf{x}||_1 = |x_1| + |x_2| + \\dots + |x_n| \\]\n\n\nA.4.4 The Infinity-Norm\nThe infinity-norm takes the limit as \\(p \\rightarrow \\infty\\) and when this limit is solved, this norm simply becomes the entry of \\(x\\) with the largest absolute value:\n\\[ ||\\mathbf{x}||_\\infty = \\max \\{ |x_1|, |x_2|, \\dots, |x_n| \\} \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linear algebra review</span>"
    ]
  },
  {
    "objectID": "00_linalg.html#rank-rangecolumn-space-and-null-spaces",
    "href": "00_linalg.html#rank-rangecolumn-space-and-null-spaces",
    "title": "Appendix A — Linear algebra review",
    "section": "A.5 Rank, Range/Column-Space and Null-Spaces",
    "text": "A.5 Rank, Range/Column-Space and Null-Spaces\n\nA.5.1 Linear Combinations and Span\nMany of the following ideas hinge upon the idea of “Linear Combinations” of vectors. Consider two vectors \\(\\mathbf{u}, \\mathbf{v}\\in \\mathbb{R}^{d}\\). A linear combination of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is the vector produced by the following:\n\\[ \\mathbf{w}= c_1 \\mathbf{u}+ c_2 \\mathbf{v}\\]\nwhere \\(c_1\\) and \\(c_2\\) are scalars. We can generalize this idea by saying that all possible realizations of \\(\\mathbf{w}\\) live in the vector-space \\(\\mathcal{W}\\) defined by the “span” by \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). This is denoted:\n\\[ \\mathbf{w}\\in \\mathcal{W} = \\text{Span}\\left( \\mathbf{u}, \\mathbf{v}\\right) \\]\nWe can read this in plain english as “the vector \\(\\mathbf{w}\\) belongs to the vector-space \\(\\mathcal{W}\\) which is defined by all possible linear combinations of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\)”.\n\n\nA.5.2 Linear-Independence\nThis is a fundamental idea in Linear Algebra. A set of vectors is said to be “Linearly Independent” if no vector in this set can be reconstructed with any of the others. In other words, no one vector can be reproduced by scaling or combining any of the others. For example, consider the following three vectors:\n\\[ \\{ \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\} \\]\nBecause each vector in this set points in its own unique direction, it is impossible to reconstruct one of these vectors as a linear combination of any others. This may be true with more complex vectors that aren’t unit vectors along the dimensions of the vector-space.\n\n\nA.5.3 Rank\nTechnically, there are two types of rank: Column Rank and Row-Rank. In this class, when we say “rank”, we will be referring to the column rank.\nThe column rank of a matrix is simply its number of linearly independent column-vectors (the row-rank is the number of linearly independent row-vectors). Consider the following matrix:\n\\[ \\mathbf{A} = \\begin{bmatrix} 1 & 2 & 1 & 4 \\\\ 0 & 1 & 1 & 2 \\\\ 0 & 0 & 2 & 0 \\end{bmatrix} \\]\nThe first-three columns of \\(\\mathbf{A}\\) are linearly independent because they cannot be reproduced using linear combinations of the other columns. However, the last column is a multiple of column 2, which means it can be reproduced using a linear combination of columns. Hence, this matrix has a rank of 3.\nTheorem: No matrix \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) where \\(m &lt; n\\) can be full-rank. This is because only \\(m\\) vectors can span \\(\\mathbb{R}^m\\), and since there are more than \\(m\\) columns, we must have some redundancy.\n\n\nA.5.4 Range/Column-Space\nThe Range, Column-Space or Image of a matrix, \\(\\mathbf{A}\\), is the span of all possible combinations of its columns. In human words, it describes the space of \\(\\mathbf{Ax}\\) for all possible \\(\\mathbf{x}\\). Consider the following matrix:\n\\[ \\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\\\ 2 & 4  \\end{bmatrix} \\]\nNote that the second column is a multiple of the first column, hence, the range of \\(\\mathbf{A}\\) is:\n\\[ \\text{Range}(\\mathbf{A}) = \\text{span} \\left( \\begin{bmatrix} 1 \\\\ 3 \\\\ 2 \\end{bmatrix}\\right) \\]\n\n\nA.5.5 Null-Space\nThe Null-Space of a matrix, \\(\\mathbf{A}\\) is the set of all nonzero vectors, \\(\\mathbf{x}\\) such that \\(\\mathbf{Ax=0}\\). Consider a simple example:\n\\[ \\mathbf{A} = \\begin{bmatrix} 1 & 0 & -1\\\\ 0 & 1 & 1   \\end{bmatrix} \\]\nWe can find the set of \\(\\mathbf{x}\\) that produce the zero-vector by solving the following equation:\n\\[ \\begin{bmatrix} 1 & 0 & -1\\\\ 0 & 1 & 1   \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\]\nThis produces the systems of equations:\n\\[ x_1 - x_3 = 0 \\] \\[ x_2 + x_3 = 0 \\] \\[ x_3 = x_3 \\]\nIf we isolate each entry of \\(\\mathbf{x}\\):\n\\[ x_1 = x_3 \\] \\[ x_2 = -x_3 \\] \\[ x_3 = x_3 \\]\nHence, we see that \\(x_3\\) can be any scalar and satisfy this systems of equations:\n\\[ \\mathbf{x} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\end{bmatrix} x_3 = \\text{span}\\left( \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\end{bmatrix} \\right) = \\text{Null}(\\mathbf{A})\\]\n\n\nA.5.6 The Identity Matrix\nThe identity matrix is a square, \\(n\\)-dimensional matrix consisting of ones on its diagonal and zeros elsewhere. It is called the identity matrix because it preserves the identity when multiplied by matrices and vectors.\n\\[ \\mathbf{I} = \\begin{bmatrix} 1 & 0 & \\dots & 0 \\\\ 0 & 1  & & 0 \\\\ \\vdots & & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & 1 \\end{bmatrix} \\]\nConsider \\(\\mathbf{I} \\in \\mathbb{R}^{n \\times n}\\) to be an \\(n \\times n\\) identity matrix. If we recall \\(\\mathbf{A} \\in \\mathbb{m \\times n}\\) and \\(\\mathbf{x} \\in \\mathbb{R}^n\\):\n\\[ \\mathbf{A I = I A = A} \\]\n\\[ \\mathbf{I x = x } \\]\n\\[ \\mathbf{x^\\top I = x^\\top} \\]\n\n\n\n\n\n\nWhy is the identity matrix useful?\n\n\n\nThe identity matrix simplifies expressions involving matrices. If we can transform a term into an identity matrix, then we can reduce the expression to only the remaining terms.\n\n\n\n\nA.5.7 Matrix Inverses\nYou may have noticed that, thus far, we have covered the addition, subtraction, and multiplication of matrices, which conspicuously leaves division. Well, unfortunately, matrix division is complex to say the least. For square matrices, division is accomplished via matrix inversion. However, not all matrices are invertible.\nDefinition: A matrix \\(\\mathbf{B}\\) is called the “inverse” of a square matrix \\(\\mathbf{A}\\) if and only if the following condition holds:\n\\[ \\mathbf{A} \\mathbf{B} = \\mathbf{B} \\mathbf{A} = \\mathbf{I} \\]\nwhere \\(\\mathbf{I}\\) is the identity matrix. Such a \\(\\mathbf{B}\\) is denoted \\(\\mathbf{A}^{-1}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linear algebra review</span>"
    ]
  },
  {
    "objectID": "00_linalg.html#further-reading",
    "href": "00_linalg.html#further-reading",
    "title": "Appendix A — Linear algebra review",
    "section": "A.6 Further Reading",
    "text": "A.6 Further Reading\n\nA great read for a deeper dive into the relationship between Column, Row and Null-spaces of matrices: Gil Strang’s Fundamental Theorem of Linear Algebra",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linear algebra review</span>"
    ]
  },
  {
    "objectID": "00_linalg.html#scalars-vectors-and-matrices-sec-linalg",
    "href": "00_linalg.html#scalars-vectors-and-matrices-sec-linalg",
    "title": "Appendix A — Linear algebra review",
    "section": "",
    "text": "A.1.1 Notation\nLet’s introduce some notation: \\(\\mathbb{R}\\) is the set of real numbers, \\(\\mathbb{C}\\) is the set of complex numbers, and the operator \\(\\in\\) designates that a variable belongs to a set. To define a real scalar (i.e. just a number), \\(c\\), we write:\n\\[ c \\in \\mathbb{R}\\]\nYou can read this as “the variable \\(c\\) belongs to the set of 1-dimensional real numbers.”\nTo define a real \\(n\\) dimensional vector, \\(\\mathbf{x}\\), we write:\n\\[ \\mathbf{x}  = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\in \\mathbb{R}^n \\]\nTo define a \\(m \\times n\\) matrix \\(\\mathbf{A}\\) of real-valued entries, we write:\n\\[ \\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & & a_{2n} \\\\\n\\vdots & & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n }\n\\]\n\n\nA.1.2 The Transpose\nThe “transpose” of a matrix or vector, swaps the rows and columns. In other words, the first row becomes the first column, the second row the second column and so-on:\n\\[ \\mathbf{x}^\\top = \\begin{bmatrix} x_1 & \\dots & x_n \\end{bmatrix} \\in \\mathbb{R}^{1 \\times n} \\]\n\\[ \\mathbf{A}^\\top = \\begin{bmatrix} a_{11} & a_{21} & \\dots & a_{m1} \\\\\na_{12} & a_{22} & & a_{m2} \\\\\n\\vdots & & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\dots & a_{mn}\n\\end{bmatrix} \\in \\mathbb{R}^{n \\times m } \\]\nTransposing the product of matrices reverses their order:\n\\[ \\left( \\mathbf{A} \\mathbf{B} \\mathbf{C} \\mathbf{D} \\right)^\\top = \\mathbf{D}^\\top \\mathbf{C}^\\top \\mathbf{B}^\\top \\mathbf{A}^\\top \\]\n\n\nA.1.3 Addition & subtraction\nTechnically, only objects of the same dimensionality can be added or subtracted with one another. So, scalars can only be added to scalars, vectors can only be added to vectors of the same dimension, and matrices can only be added to other matrices with the same numbers of rows and columns. Some software like MATLAB or NumPy might let you add scalars to vectors and matrices and under the hood they multiply that scalar by a vector/matrix of ones to make the dimensions match.\n\n\n\n\n\n\nExamples of numpy allowing mathematically invalid operations\n\n\n\n\n\nMathematically, none of the following are valid operations. However, numpy allows us to do them by secretly adding in a vector of ones so that the vector/matrix dimensions are valid. This can be confusing for new programmers.\nAdding a scalar to a vector\n\nimport numpy as np \n# Defining a vector \nu = np.array([\n  1, 2, 3\n])\n\n# Adding a scalar to a vector\nprint(u + 5)\n# What numpy does to make the dimensions match\nprint(u + 5*np.ones_like(u)) \n\n[6 7 8]\n[6 7 8]\n\n\nAdding a row-vector to a column-vector\n\n# Reshaping u to a column-vector \nu = u.reshape((3,1))\n\n# Defining a row-vector, v \nv = np.array([\n  4, 5, 6\n]).reshape((1,3)) \n\n# Adding the row-vector v to a column-vector u\nprint(v + u)\n# What numpy does to make the dimensions match\nprint(np.outer(u, np.ones(3)) + np.outer(np.ones(3), v)) \n\n[[5 6 7]\n [6 7 8]\n [7 8 9]]\n[[5. 6. 7.]\n [6. 7. 8.]\n [7. 8. 9.]]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linear algebra review</span>"
    ]
  },
  {
    "objectID": "00_linalg.html#sec-linalg",
    "href": "00_linalg.html#sec-linalg",
    "title": "Appendix A — Linear algebra review",
    "section": "",
    "text": "A.1.1 Notation\nLet’s introduce some notation: \\(\\mathbb{R}\\) is the set of real numbers, \\(\\mathbb{C}\\) is the set of complex numbers, and the operator \\(\\in\\) designates that a variable belongs to a set. To define a real scalar (i.e. just a number), \\(c\\), we write:\n\\[ c \\in \\mathbb{R}\\]\nYou can read this as “the variable \\(c\\) belongs to the set of 1-dimensional real numbers.”\nTo define a real \\(n\\) dimensional vector, \\(\\mathbf{x}\\), we write:\n\\[ \\mathbf{x}  = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\in \\mathbb{R}^n \\]\nTo define a \\(m \\times n\\) matrix \\(\\mathbf{A}\\) of real-valued entries, we write:\n\\[ \\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & & a_{2n} \\\\\n\\vdots & & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n }\n\\]\n\n\nA.1.2 The Transpose\nThe “transpose” of a matrix or vector, swaps the rows and columns. In other words, the first row becomes the first column, the second row the second column and so-on:\n\\[ \\mathbf{x}^\\top = \\begin{bmatrix} x_1 & \\dots & x_n \\end{bmatrix} \\in \\mathbb{R}^{1 \\times n} \\]\n\\[ \\mathbf{A}^\\top = \\begin{bmatrix} a_{11} & a_{21} & \\dots & a_{m1} \\\\\na_{12} & a_{22} & & a_{m2} \\\\\n\\vdots & & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\dots & a_{mn}\n\\end{bmatrix} \\in \\mathbb{R}^{n \\times m } \\]\nTransposing the product of matrices reverses their order:\n\\[ \\left( \\mathbf{A} \\mathbf{B} \\mathbf{C} \\mathbf{D} \\right)^\\top = \\mathbf{D}^\\top \\mathbf{C}^\\top \\mathbf{B}^\\top \\mathbf{A}^\\top \\]\n\n\nA.1.3 Addition & subtraction\nTechnically, only objects of the same dimensionality can be added or subtracted with one another. So, scalars can only be added to scalars, vectors can only be added to vectors of the same dimension, and matrices can only be added to other matrices with the same numbers of rows and columns. Some software like MATLAB or NumPy might let you add scalars to vectors and matrices and under the hood they multiply that scalar by a vector/matrix of ones to make the dimensions match.\n\n\n\n\n\n\nExamples of numpy allowing mathematically invalid operations\n\n\n\n\n\nMathematically, none of the following are valid operations. However, numpy allows us to do them by secretly adding in a vector of ones so that the vector/matrix dimensions are valid. This can be confusing for new programmers.\nAdding a scalar to a vector\n\nimport numpy as np \n# Defining a vector \nu = np.array([\n  1, 2, 3\n])\n\n# Adding a scalar to a vector\nprint(u + 5)\n# What numpy does to make the dimensions match\nprint(u + 5*np.ones_like(u)) \n\n[6 7 8]\n[6 7 8]\n\n\nAdding a row-vector to a column-vector\n\n# Reshaping u to a column-vector \nu = u.reshape((3,1))\n\n# Defining a row-vector, v \nv = np.array([\n  4, 5, 6\n]).reshape((1,3)) \n\n# Adding the row-vector v to a column-vector u\nprint(v + u)\n# What numpy does to make the dimensions match\nprint(np.outer(u, np.ones(3)) + np.outer(np.ones(3), v)) \n\n[[5 6 7]\n [6 7 8]\n [7 8 9]]\n[[5. 6. 7.]\n [6. 7. 8.]\n [7. 8. 9.]]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linear algebra review</span>"
    ]
  },
  {
    "objectID": "12_lslinalg.html#linear-least-squares-and-linear-systems-algorithms",
    "href": "12_lslinalg.html#linear-least-squares-and-linear-systems-algorithms",
    "title": "3  LLS: Linear Algebra Perspective",
    "section": "3.3 Linear least squares and linear systems: Algorithms",
    "text": "3.3 Linear least squares and linear systems: Algorithms",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLS: Linear Algebra Perspective</span>"
    ]
  },
  {
    "objectID": "12_lslinalg.html#algorithms-for-solving-linear-systems-and-least-squares-problems",
    "href": "12_lslinalg.html#algorithms-for-solving-linear-systems-and-least-squares-problems",
    "title": "3  LLS: Linear Algebra Perspective",
    "section": "3.3 Algorithms for solving linear systems and least squares problems",
    "text": "3.3 Algorithms for solving linear systems and least squares problems",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLS: Linear Algebra Perspective</span>"
    ]
  },
  {
    "objectID": "12_lslinalg.html#algorithms-for-solving-linear-systems",
    "href": "12_lslinalg.html#algorithms-for-solving-linear-systems",
    "title": "3  LLS: Linear Algebra Perspective",
    "section": "3.3 Algorithms for solving linear systems",
    "text": "3.3 Algorithms for solving linear systems\nWe now turn our attention to the algorithms for solving equations of the form \\(X\\beta = Y\\) for invertible as well as over- or underdetermined systems. For all of these cases, the best practice is to use functions that are part of standard linear algebra packages to solve these systems. In python, both the numpy and scipy libraries have linalg packages that contain various useful linear algebra functions. Both are based on the LAPACK library, which efficiently implements many linear algebra computations in Fortran. Popular deep learning packages like PyTorch and TensorFlow have their own linear algebra packages that build on other libraries that leverage GPU computing capabilities.\n\n\n3.3.1 Invertible linear systems\nIf \\(X\\) is invertible (and therefore square), the python functions we use to solve the system \\(X\\beta=Y\\) are numpy.linalg.solve and scipy.linalg.solve.\nThese packages solve \\(X\\beta = Y\\) using LU factorization, which is a topic that should have been covered in your linear algebra class. Here’s a quick recap if you’ve forgotten4: any square matrix \\(X\\) can be written (sometimes after appropriate re-ordering of the rows/columns) as the product of a lower triangular matrix \\(L\\) with an upper triangular matrix \\(U\\), that is, \\(X=LU\\). Here’s some pseudocode for LU decomposition (without pivoting, that is, assuming no re-ordering of the rows/columns of \\(X\\) are necessary):\nLU decomposition: pseudocode\n\nInitialize \\(L = I\\) (Identity matrix).\nfor \\(j = 1\\) to \\(n\\):\n\nfor \\(i = 1\\) to \\(j\\):\n\n\\(s_1 = 0\\)\nfor \\(k = 1\\) to \\(i\\):\n\n\\(s_1 = s_1 + U_{kj} L_{ik}\\)\n\n\\(U_{ij} = X_{ij} - s_1\\)\n\nfor \\(i = (j + 1)\\) to \\(n\\):\n\n\\(s_2 = 0\\)\nfor \\(k = 1\\) to \\(j - 1\\):\n\n\\(s_2 = s_2 + U_{kj}  L_{ik}\\)\n\n\\(L_{ij} = \\frac{X_{ij} - s_2}{U_{jj}}\\)\n\n\nreturn \\(L\\), \\(U\\)\n\nOnce the LU factorization of \\(X\\) is found, then we can solve for \\(\\beta\\) in two steps:\n\nSolve \\(L\\alpha = Y\\) for \\(\\alpha\\).\nSolve \\(U\\beta = \\alpha\\) for \\(\\beta\\).\n\nThis may look like we’ve just overcomplicated things by turning our one problem of solving \\(X\\beta = Y\\) into two problems, but the triangular structure of \\(L\\) and \\(U\\) lets us solve these two new systems relatively efficiently. To see this, consider solving \\(L\\alpha = Y\\) for a three-dimensional example:\n\\[\nL = \\begin{pmatrix}1 & & \\\\ 2 & 1 & \\\\\n2 & 1 & 2\n\\end{pmatrix}, \\qquad Y = \\begin{pmatrix}1 \\\\ 2 \\\\ 0\\end{pmatrix}.\n\\]\nRecall that this represents a system of three linear equations:\n\\[\n\\begin{aligned}\n1 \\alpha_1 &             & = 1\\\\\n2 \\alpha_1 &+ 1 \\alpha_2  &= 2 \\\\\n2 \\alpha_1 &+ 1 \\alpha_2 + 2 \\alpha_3 & = 0\n\\end{aligned}\n\\]\nWe can immediately read off the solution of the first equation, \\(\\alpha_1 = 1\\) because there is only one unknown variable in it. From there we substitute that into the second equation to get \\(2 + 1\\alpha_2 = 2\\), so we again one unknown variable, which we can easily solve to get \\(\\alpha_2 = 0\\). From there we now substitute our values for \\(\\alpha_1\\) and \\(\\alpha_2\\) into the third equation, yielding \\(2 + 2\\alpha_3 = 0\\), which again has one unknown variable, so we can easily solve to get \\(\\alpha_3 = -1\\). This yields our overall solution \\(\\alpha = (1, 0, -1)^\\top\\).\nThe process we have just described is called forward substitution and is a standard algorithm for solving lower triangular systems. It generalizes easily to triangular systems of arbitrary dimension \\(n\\times n\\) — we just keep substituting in the results of previous rows into the next row, as follows:\nForward substitution: pseudocode\n\n\\(\\alpha_1 = Y_1 / L_{11}\\).\nfor \\(i\\) = 2 to \\(n\\):\n\n\\(s = 0\\)\nfor \\(j\\) = 1 to \\((i - 1)\\):\n\n\\(s = s + L_{ij}\\alpha_j\\)\n\n\\(\\alpha_i = (Y_i - s) / L_{ii}\\)\n\nreturn \\(\\alpha\\)\n\nNow that we have solved \\(L\\alpha = Y\\) for \\(\\alpha\\) using forward substitution, we need to solve \\(U\\beta = \\alpha\\) for \\(\\beta\\), where \\(U\\) is upper triangular rather than lower triangular. This is done using backward substitution, which is a very similar algorithm to forward substitution. That gives us our final solution \\(\\beta\\).\n\n\n\n\n\n\nExercise\n\n\n\nWithout looking it up, can you write pseudocode for a backward substitution algorithm for solving upper triangular systems?\n\n\n\n\n3.3.2 Least squares problems\nIf \\(X\\) is non-invertible, the python functions we use to solve \\(X\\beta = Y\\) are numpy.linalg.lstsq and scipy.linalg.lstsq. These functions can use a number of different algorithms to solve the least squares problem. A common choice is based on QR decomposition, which should also have been covered in your linear algebra class.\nHere’s a reminder of the essential facts of QR decomposition: if \\(X\\in\\mathbb{R}^{N\\times n}\\) has full column rank, then it can be written as the product of an orthogonal matrix \\(Q\\in\\mathbb{R}^{N\\times n}\\) with an upper triangular matrix \\(R\\in\\mathbb{R}^{n\\times n}\\). Analogous to solving invertible systems, once we have factored \\(X=QR\\) we can then solve the least squares problem in two steps:\n\nSolve \\(Q\\alpha = Y\\) for \\(\\alpha\\).\nSolve \\(R\\beta = \\alpha\\) for \\(\\beta\\).\n\nRecall that orthogonal matrices \\(Q\\) satisfy \\(Q^\\top Q=I\\), so this special structure lets us find \\(\\alpha\\) via a matrix multiplication, which is computationally more efficient than a full LU decomposition. Then, since \\(R\\) is upper triangular, we can solve for \\(\\beta\\) using backward substitution.\nIf \\(X\\) is low-rank, there are a couple of options, but the standard one is based on the the singular value decomposition, which deserves its own set of notes, so we will defer discussion of this until later.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLS: Linear Algebra Perspective</span>"
    ]
  },
  {
    "objectID": "13_lsnla.html#algorithms-for-solving-linear-systems",
    "href": "13_lsnla.html#algorithms-for-solving-linear-systems",
    "title": "4  LLS: Computational Considerations",
    "section": "4.2 Algorithms for solving linear systems",
    "text": "4.2 Algorithms for solving linear systems\nWe now turn our attention to the algorithms for solving equations of the form \\(X\\beta = Y\\) for invertible as well as over- or underdetermined systems. For all of these cases, the best practice is to use functions that are part of standard linear algebra packages to solve these systems. In python, both the numpy and scipy libraries have linalg packages that contain various useful linear algebra functions. Both are based on the LAPACK library, which efficiently implements many linear algebra computations in Fortran. Popular deep learning packages like PyTorch and TensorFlow have their own linear algebra packages that build on other libraries that leverage GPU computing capabilities.\n\n\n4.2.1 Invertible linear systems\nIf \\(X\\) is invertible (and therefore square), the python functions we use to solve the system \\(X\\beta=Y\\) are numpy.linalg.solve and scipy.linalg.solve.\nThese packages solve \\(X\\beta = Y\\) using LU factorization, which is a topic that should have been covered in your linear algebra class. Here’s a quick recap if you’ve forgotten1: any square matrix \\(X\\) can be written (sometimes after appropriate re-ordering of the rows/columns) as the product of a lower triangular matrix \\(L\\) with an upper triangular matrix \\(U\\), that is, \\(X=LU\\). Here’s some pseudocode for LU decomposition (without pivoting, that is, assuming no re-ordering of the rows/columns of \\(X\\) are necessary):\nLU decomposition: pseudocode\n\nInitialize \\(L = I\\) (Identity matrix).\nfor \\(j = 1\\) to \\(n\\):\n\nfor \\(i = 1\\) to \\(j\\):\n\n\\(s_1 = 0\\)\nfor \\(k = 1\\) to \\(i\\):\n\n\\(s_1 = s_1 + U_{kj} L_{ik}\\)\n\n\\(U_{ij} = X_{ij} - s_1\\)\n\nfor \\(i = (j + 1)\\) to \\(n\\):\n\n\\(s_2 = 0\\)\nfor \\(k = 1\\) to \\(j - 1\\):\n\n\\(s_2 = s_2 + U_{kj}  L_{ik}\\)\n\n\\(L_{ij} = \\frac{X_{ij} - s_2}{U_{jj}}\\)\n\n\nreturn \\(L\\), \\(U\\)\n\nOnce the LU factorization of \\(X\\) is found, then we can solve for \\(\\beta\\) in two steps:\n\nSolve \\(L\\alpha = Y\\) for \\(\\alpha\\).\nSolve \\(U\\beta = \\alpha\\) for \\(\\beta\\).\n\nThis may look like we’ve just overcomplicated things by turning our one problem of solving \\(X\\beta = Y\\) into two problems, but the triangular structure of \\(L\\) and \\(U\\) lets us solve these two new systems relatively efficiently. To see this, consider solving \\(L\\alpha = Y\\) for a three-dimensional example:\n\\[\nL = \\begin{pmatrix}1 & & \\\\ 2 & 1 & \\\\\n2 & 1 & 2\n\\end{pmatrix}, \\qquad Y = \\begin{pmatrix}1 \\\\ 2 \\\\ 0\\end{pmatrix}.\n\\]\nRecall that this represents a system of three linear equations:\n\\[\n\\begin{aligned}\n1 \\alpha_1 &             & = 1\\\\\n2 \\alpha_1 &+ 1 \\alpha_2  &= 2 \\\\\n2 \\alpha_1 &+ 1 \\alpha_2 + 2 \\alpha_3 & = 0\n\\end{aligned}\n\\]\nWe can immediately read off the solution of the first equation, \\(\\alpha_1 = 1\\) because there is only one unknown variable in it. From there we substitute that into the second equation to get \\(2 + 1\\alpha_2 = 2\\), so we again one unknown variable, which we can easily solve to get \\(\\alpha_2 = 0\\). From there we now substitute our values for \\(\\alpha_1\\) and \\(\\alpha_2\\) into the third equation, yielding \\(2 + 2\\alpha_3 = 0\\), which again has one unknown variable, so we can easily solve to get \\(\\alpha_3 = -1\\). This yields our overall solution \\(\\alpha = (1, 0, -1)^\\top\\).\nThe process we have just described is called forward substitution and is a standard algorithm for solving lower triangular systems. It generalizes easily to triangular systems of arbitrary dimension \\(n\\times n\\) — we just keep substituting in the results of previous rows into the next row, as follows:\nForward substitution: pseudocode\n\n\\(\\alpha_1 = Y_1 / L_{11}\\).\nfor \\(i\\) = 2 to \\(n\\):\n\n\\(s = 0\\)\nfor \\(j\\) = 1 to \\((i - 1)\\):\n\n\\(s = s + L_{ij}\\alpha_j\\)\n\n\\(\\alpha_i = (Y_i - s) / L_{ii}\\)\n\nreturn \\(\\alpha\\)\n\nNow that we have solved \\(L\\alpha = Y\\) for \\(\\alpha\\) using forward substitution, we need to solve \\(U\\beta = \\alpha\\) for \\(\\beta\\), where \\(U\\) is upper triangular rather than lower triangular. This is done using backward substitution, which is a very similar algorithm to forward substitution. That gives us our final solution \\(\\beta\\).\n\n\n\n\n\n\nExercise\n\n\n\nWithout looking it up, can you write pseudocode for a backward substitution algorithm for solving upper triangular systems?\n\n\n\n\n4.2.2 Least squares problems\nIf \\(X\\) is non-invertible, the python functions we use to solve \\(X\\beta = Y\\) are numpy.linalg.lstsq and scipy.linalg.lstsq. These functions can use a number of different algorithms to solve the least squares problem. A common choice is based on QR decomposition, which should also have been covered in your linear algebra class.\nHere’s a reminder of the essential facts of QR decomposition: if \\(X\\in\\mathbb{R}^{N\\times n}\\) has full column rank, then it can be written as the product of an orthogonal matrix \\(Q\\in\\mathbb{R}^{N\\times n}\\) with an upper triangular matrix \\(R\\in\\mathbb{R}^{n\\times n}\\). Analogous to solving invertible systems, once we have factored \\(X=QR\\) we can then solve the least squares problem in two steps:\n\nSolve \\(Q\\alpha = Y\\) for \\(\\alpha\\).\nSolve \\(R\\beta = \\alpha\\) for \\(\\beta\\).\n\nRecall that orthogonal matrices \\(Q\\) satisfy \\(Q^\\top Q=I\\), so this special structure lets us find \\(\\alpha\\) via a matrix multiplication, which is computationally more efficient than a full LU decomposition. Then, since \\(R\\) is upper triangular, we can solve for \\(\\beta\\) using backward substitution.\nIf \\(X\\) is low-rank, there are a couple of options, but the standard one is based on the the singular value decomposition, which deserves its own set of notes, so we will defer discussion of this until later.\n\n\n\n\nStrang, Gilbert. 2022. Introduction to Linear Algebra.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLS: Computational Considerations</span>"
    ]
  },
  {
    "objectID": "13_lsnla.html#footnotes",
    "href": "13_lsnla.html#footnotes",
    "title": "4  LLS: Computational Cost",
    "section": "",
    "text": "if you want more of a refresher, revisit your favorite linear algebra textbook or notes. My favorite is (Strang 2022). There’s also no substitute for your own notes that reflect how you think about things – if you have your own notes, go back and look at them now!↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLS: Computational Cost</span>"
    ]
  },
  {
    "objectID": "13_lsnla.html#measuring-computational-cost",
    "href": "13_lsnla.html#measuring-computational-cost",
    "title": "4  LLS: Computational Cost",
    "section": "",
    "text": "4.1.1 Measuring time complexity using FLOPs\nWe will focus on compute time first. Since compute time will vary from computer to computer, the way we measure time complexity in computational linear algebra is by counting the number of floating point operations (FLOPs) required by an algorithm. A single FLOP refers to the addition, subtraction, multiplication, or division of two scalar quantities. Compute speed of supercomputers is measured in FLOPs per second (also abbreviated FLOPs), and you’ll the fastest computers in the world measured in gigaFLOPs (\\(10^9\\)), teraFLOPs (\\(10^{12}\\)), or even exaFLOPs (\\(10^{18}\\)). All computational linear algebra algorithms are simply comprised of a bunch of FLOPs strung together. To see this, consider adding two vectors, \\(a,b\\in\\mathbb{R}^n\\):\n\\[\na + b = \\begin{pmatrix}a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n\\end{pmatrix} +\n\\begin{pmatrix}b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n\\end{pmatrix}\n\\]\nSince vector addition means that the corresponding elements each get added to each other, there are as many scalar addition operations as there are elements of the the vector. Thus vector addition has cost \\(n\\).\nLet’s now consider another operation between vectors, the dot product:\n\\[\na\\cdot b = a^\\top b = a_1b_1 + a_2b_2 + \\cdots + a_nb_n\n\\]\nThe dot product involves \\(n\\) element-wise multiplications followed by \\((n-1)\\) additions. The total cost is thus \\(2n-1\\) FLOPs.\nRecall that we are most interested in how the computational cost of a computation scales as the dimension of the problem changes. For both vector addition and the dot product between vectors, the computational cost is said to scale linearly with the size of the vectors \\(n\\). This is because the highest-order polynomial term in the cost is \\(n^1\\), so this linear term dominates the cost as \\(n\\) becomes large. This means that if we increase \\(n\\) by a factor of 10, the cost will also increase by approximately a factor of 10. Note that this is true regardless of what the constant factor in front of the \\(n^1\\) term is. Because of this, it is common to write that the cost is \\(\\mathcal{O}(n)\\) (read this notation as “order n” or “big-O n”, although the technical definition of “big-O” is not one I plan to get into). Other common terminology is to say that the computation has “first-order” cost scaling/complexity.\nAs a final example of FLOP-counting in action before we move on, consider matrix-vector multiplication. That is, let \\(A\\in\\mathbb{R}^{m\\times n}\\) and let \\(b\\in\\mathbb{R}^n\\). Then, note that\n\\[\nAb = \\begin{pmatrix}a_{1,:}\\cdot b \\\\ a_{2,:}\\cdot b \\\\ \\vdots \\\\ a_{m,:} \\cdot b \\end{pmatrix}\n\\]\nwhere \\(a_{i,:}\\in\\mathbb{R}^n\\) denotes the row vector that is the \\(i\\)th row of \\(A\\). Writing it this way shows that a matrix-vector multiplication consists of \\(m\\) dot products between two vectors of length \\(n\\). Thus, the total number of FLOPs is \\(m(2n-1)\\), which we say is \\(\\mathcal{O}(mn)\\). If the matrix \\(A\\) is square so that \\(m=n\\), we would say that matrix-vector multiplication is \\(\\mathcal{O}(n^2)\\), i.e., it has second-order or quadratic cost scaling/complexity.\n\n\n\n\n\n\nNote\n\n\n\nCost scaling really describes the cost of algorithms as \\(n\\) becomes large. This means that as you increase \\(n\\) from 2 to 3, or from 3 to 30, you may not see the expected scaling in run time. This is because small computations happen fast enough the run time might be more influenced by how busy your computer is doing other things than by how many FLOPs it needs to do. However, as you increase \\(n\\) by orders of magnitude, to 1000 and 10000 and so on, you should see the expected cost scaling as \\(n\\) becomes larger.\n\n\n\n\n4.1.2 Measuring memory complexity\nThere are essentially two considerations when measuring memory complexity of scientific computing algorithms: (1) how a computer stores a single number (scalar), and (2) how many scalars must be stored in order to carry out a computation.\n\n4.1.2.1 Floating point numbers: an introduction\nComputers represent numbers using binary, a base-2 numeral system (our usual number system is base-10, or “decimal”). Binary uses only two digits: 0 and 1. In binary, each digit is referred to as a “bit” (short for binary digit), and a group of eight bits is called a “byte.” In base-10, the right-most digit tells us how many ones (\\(10^0\\)) we have, the second-right-most how many tens (\\(10^1\\)), the third-right-most how many hundreds (\\(10^2\\)), etc. Similarly, in binary the right-most bit tells us how many ones (\\(2^0\\)) we have, the second-right-most how many 2s (\\(2^1\\)), the third-right-most how many 4s (\\(2^2\\)) etc. To see this in action, consider the binary number \\(1101_2\\) (the subscript \\(\\cdot_2\\) tells us the number should be intepreted as a binary number as opposed to one-thousand-one-hundred-one). To convert \\(1101_2\\) to decimal, we sum the powers of 2 corresponding to each bit:\n\\[\n1101_2 = (1 \\times 2^3) + (1 \\times 2^2) + (0 \\times 2^1) + (1 \\times 2^0) = 8 + 4 + 0 + 1 = 13_{10}\n\\]\nHow can we use binary to represent decimal numbers, as well as negative numbers? If you have ever learned programming in a language like Java or C, you will have learned that computers can store variables in different data types. In scientific computing, the main data types we care about are single- and double-precision floats (short for floating-point number), which represent numbers with decimal points (as opposed to ints which represent integers). The difference between single-precision float and a double-precision float is the number of binary bits the computer uses to store the number. Single precision uses 32 bits, whereas double precision uses 64 bits. These are standard data types that divide up the available bits into three parts that define a type of binary scientific notation: the sign, the mantissa or significand, and the exponent. We convert these to a value as follows:\n\\[\n\\text{value} = (-1)^\\text{sign} \\cdot \\text{significand} \\cdot 2^\\text{exponent}\n\\]\n\nsingle precision uses 1 bit for the sign, 8 bits for the exponent, and 23 bits for the “mantissa” or “significand” (the significant digits that pre-multiply the exponent)\ndouble precision uses 1 bit for the sign, 11 bits for the exponent, and 52 bits for the mantissa/significand\n\nThe sign bit can be 0 or 1 — if the bit is 0 we get a positive number and otherwise the number is negative. We do something clever with the significand: just like you would usually not write down the decimal number \\(13\\) as \\(0013\\), in binary we also don’t want to “waste” bits with leading 0s. Thus, the significand is understood as having an implicit “1.” in front of all the bits that are actually stored, giving us 24 binary bits of precision in single-precision floats and 53 binary bits of precision in double-precision floats (note that all these bits of precision come after the implicit “1.”, meaning they’re associated with negative powers of 2). This translates to approximately 8 (15) decimal digits of precision for single-precision (double-precision) floats. Finally, for the exponent, the 8 (11) bits can represent the decimal numbers from 0 to 255 (0 to 2047), but we want to allow negative exponents in order to represent numbers that are smaller than 1. Thus, we subtract 127 (1023) from the number stored in the exponent bits to get the actual exponent.\n\n\n\n\n\n\nWorked example: Converting the single-precision float “0 10000001 10100000000000000000000” to decimal\n\n\n\n\n\n\nExtract the parts:\n\nSign bit (S): 0 (positive)\nExponent bits (E): 10000001\nSignificand (fraction) bits (F): 10100000000000000000000\n\nConvert the exponent:\n\nThe exponent is 10000001 in binary, which is 129 in decimal.\nThe actual exponent is: \\(E - 127 = 129 - 127 = 2\\).\n\nConvert the fraction:\n\nThe fraction is 1.F = 1.101 (because the leading 1 is implied).\nConvert 1.101 to decimal: \\(1.101_2 = (1 \\times 2^0) + (1 \\times 2^{-1}) + (0 \\times 2^{-2}) + (1 \\times 2^{-3}) = 1 + 0.5 + 0 + 0.125 = 1.625\\).\n\nCalculate the value: \\[\n(-1)^S \\times 1.F \\times 2^{E - 127} = (-1)^0 \\times 1.625 \\times 2^2 = 1 \\times 1.625 \\times 4 = 6.5\n\\]\n\n\n\n\nThe default for most computers you will encounter is to store decimal numbers as double-precision floats, and often when you hear someone just say “float” they in fact mean a double. However, the default precision in some popular machine learning packages is single precision.\n\n\n4.1.2.2 Storing matrices and vectors\nIf you’re exhausted by the details above the good news is that things get simpler now. When doing computations with matrices and vectors, each element of the matrix/vector is some kind of float, and the cost to store the whole matrix/vector is just the number of elements multiplied by either 32 or 64 bits, depending on what kind of float it is. So a matrix \\(X\\in\\mathbb{R}^{1000\\times 25}\\) stored in double precision will require \\[\n25000\\times 64 \\text{ bits}=1600000 \\text{ bits} = 200000 \\text{ bytes} = 200 \\text{ kilobytes}\n\\]\nIf you’ve ever run a computation on your laptop and gotten an “Out of Memory” error, that’s your computer telling you it doesn’t have enough RAM to store all the numbers you need. This is a common challenge when working with “big data” in many ML applications: there are so many data points that we cannot load them all into memory at one time — there are some clever tricks to deal with that case that we’ll discuss later on in the course.\nNote that the cost to store a vector scales linearly with its length, and the cost to store an \\(m\\)-by-\\(n\\) matrix scales with \\(mn\\), or if the matrix is square with \\(m=n\\), then quadratically in \\(n\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLS: Computational Cost</span>"
    ]
  },
  {
    "objectID": "13_lsnla.html#algorithms-for-solving-linear-systems-revisited",
    "href": "13_lsnla.html#algorithms-for-solving-linear-systems-revisited",
    "title": "4  LLS: Computational Cost",
    "section": "4.2 Algorithms for solving linear systems (revisited)",
    "text": "4.2 Algorithms for solving linear systems (revisited)\nthis set of notes: LU, QR, pseudocode, time and memory cost. as an exercise, figure out how much RAM your computer has, what’s the largest system you can comfortably solve? try crashing your computer.\nnext ste of notes: machine precision, round off error, SVD and conditioning.\npython examples showing nonzero residual for LU and conditioning error for QR?\nintroduce the SVD in math, singular values vs rank/conditioning, explain the concept of conditioning in terms of being essentially non-invertible from the computer’s point of view, cond/rank commands\nshow that normal equations are terribly conditioned.\npseudoinverse and truncated SVD for regularization\nSVD + PCA??\nWe now turn our attention to the algorithms for solving equations of the form \\(X\\beta = Y\\) for invertible as well as over- or underdetermined systems. For all of these cases, the best practice is to use functions that are part of standard linear algebra packages to solve these systems. In python, both the numpy and scipy libraries have linalg packages that contain various useful linear algebra functions. Both are based on the LAPACK library, which efficiently implements many linear algebra computations in Fortran. Popular deep learning packages like PyTorch and TensorFlow have their own linear algebra packages that build on other libraries that leverage GPU computing capabilities.\n\n\n4.2.1 Invertible linear systems\nIf \\(X\\) is invertible (and therefore square), the python functions we use to solve the system \\(X\\beta=Y\\) are numpy.linalg.solve and scipy.linalg.solve.\nThese packages solve \\(X\\beta = Y\\) using LU factorization, which is a topic that should have been covered in your linear algebra class. Here’s a quick recap if you’ve forgotten1: any square matrix \\(X\\) can be written (sometimes after appropriate re-ordering of the rows/columns) as the product of a lower triangular matrix \\(L\\) with an upper triangular matrix \\(U\\), that is, \\(X=LU\\). Here’s some pseudocode for LU decomposition (without pivoting, that is, assuming no re-ordering of the rows/columns of \\(X\\) are necessary):\nLU decomposition: pseudocode\n\nInitialize \\(L = I\\) (Identity matrix).\nfor \\(j = 1\\) to \\(n\\):\n\nfor \\(i = 1\\) to \\(j\\):\n\n\\(s_1 = 0\\)\nfor \\(k = 1\\) to \\(i\\):\n\n\\(s_1 = s_1 + U_{kj} L_{ik}\\)\n\n\\(U_{ij} = X_{ij} - s_1\\)\n\nfor \\(i = (j + 1)\\) to \\(n\\):\n\n\\(s_2 = 0\\)\nfor \\(k = 1\\) to \\(j - 1\\):\n\n\\(s_2 = s_2 + U_{kj}  L_{ik}\\)\n\n\\(L_{ij} = \\frac{X_{ij} - s_2}{U_{jj}}\\)\n\n\nreturn \\(L\\), \\(U\\)\n\nOnce the LU factorization of \\(X\\) is found, then we can solve for \\(\\beta\\) in two steps:\n\nSolve \\(L\\alpha = Y\\) for \\(\\alpha\\).\nSolve \\(U\\beta = \\alpha\\) for \\(\\beta\\).\n\nThis may look like we’ve just overcomplicated things by turning our one problem of solving \\(X\\beta = Y\\) into two problems, but the triangular structure of \\(L\\) and \\(U\\) lets us solve these two new systems relatively efficiently. To see this, consider solving \\(L\\alpha = Y\\) for a three-dimensional example:\n\\[\nL = \\begin{pmatrix}1 & & \\\\ 2 & 1 & \\\\\n2 & 1 & 2\n\\end{pmatrix}, \\qquad Y = \\begin{pmatrix}1 \\\\ 2 \\\\ 0\\end{pmatrix}.\n\\]\nRecall that this represents a system of three linear equations:\n\\[\n\\begin{aligned}\n1 \\alpha_1 &             & = 1\\\\\n2 \\alpha_1 &+ 1 \\alpha_2  &= 2 \\\\\n2 \\alpha_1 &+ 1 \\alpha_2 + 2 \\alpha_3 & = 0\n\\end{aligned}\n\\]\nWe can immediately read off the solution of the first equation, \\(\\alpha_1 = 1\\) because there is only one unknown variable in it. From there we substitute that into the second equation to get \\(2 + 1\\alpha_2 = 2\\), so we again one unknown variable, which we can easily solve to get \\(\\alpha_2 = 0\\). From there we now substitute our values for \\(\\alpha_1\\) and \\(\\alpha_2\\) into the third equation, yielding \\(2 + 2\\alpha_3 = 0\\), which again has one unknown variable, so we can easily solve to get \\(\\alpha_3 = -1\\). This yields our overall solution \\(\\alpha = (1, 0, -1)^\\top\\).\nThe process we have just described is called forward substitution and is a standard algorithm for solving lower triangular systems. It generalizes easily to triangular systems of arbitrary dimension \\(n\\times n\\) — we just keep substituting in the results of previous rows into the next row, as follows:\nForward substitution: pseudocode\n\n\\(\\alpha_1 = Y_1 / L_{11}\\).\nfor \\(i\\) = 2 to \\(n\\):\n\n\\(s = 0\\)\nfor \\(j\\) = 1 to \\((i - 1)\\):\n\n\\(s = s + L_{ij}\\alpha_j\\)\n\n\\(\\alpha_i = (Y_i - s) / L_{ii}\\)\n\nreturn \\(\\alpha\\)\n\nNow that we have solved \\(L\\alpha = Y\\) for \\(\\alpha\\) using forward substitution, we need to solve \\(U\\beta = \\alpha\\) for \\(\\beta\\), where \\(U\\) is upper triangular rather than lower triangular. This is done using backward substitution, which is a very similar algorithm to forward substitution. That gives us our final solution \\(\\beta\\).\n\n\n\n\n\n\nExercise\n\n\n\nWithout looking it up, can you write pseudocode for a backward substitution algorithm for solving upper triangular systems?\n\n\n\n\n4.2.2 Least squares problems\nIf \\(X\\) is non-invertible, the python functions we use to solve \\(X\\beta = Y\\) are numpy.linalg.lstsq and scipy.linalg.lstsq. These functions can use a number of different algorithms to solve the least squares problem. A common choice is based on QR decomposition, which should also have been covered in your linear algebra class.\nHere’s a reminder of the essential facts of QR decomposition: if \\(X\\in\\mathbb{R}^{N\\times n}\\) has full column rank, then it can be written as the product of an orthogonal matrix \\(Q\\in\\mathbb{R}^{N\\times n}\\) with an upper triangular matrix \\(R\\in\\mathbb{R}^{n\\times n}\\). Analogous to solving invertible systems, once we have factored \\(X=QR\\) we can then solve the least squares problem in two steps:\n\nSolve \\(Q\\alpha = Y\\) for \\(\\alpha\\).\nSolve \\(R\\beta = \\alpha\\) for \\(\\beta\\).\n\nRecall that orthogonal matrices \\(Q\\) satisfy \\(Q^\\top Q=I\\), so this special structure lets us find \\(\\alpha\\) via a matrix multiplication, which is computationally more efficient than a full LU decomposition. Then, since \\(R\\) is upper triangular, we can solve for \\(\\beta\\) using backward substitution.\nIf \\(X\\) is low-rank, there are a couple of options, but the standard one is based on the the singular value decomposition, which deserves its own set of notes, so we will defer discussion of this until later.\n\n\n\n\nStrang, Gilbert. 2022. Introduction to Linear Algebra.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLS: Computational Cost</span>"
    ]
  },
  {
    "objectID": "14_lssvd.html",
    "href": "14_lssvd.html",
    "title": "5  LLS and the singular value decomposition",
    "section": "",
    "text": "5.1 SVD: the basics\nIn this section we will introduce a new matrix decomposition called the Singular Value Decomposition (SVD). The SVD is generally applicable to all matrices of any dimension or rank, and has many applications in machine learning and beyond. We will begin by defining the decomposition and then discuss some of it uses.\nThe intended learning outcomes of these notes are that students should be able to:\nThe SVD decomposes the matrix \\(X\\in\\mathbb{R}^{N\\times n}\\) into the product of three matrices, \\(U\\in\\mathbb{R}^{N\\times N}\\), \\(\\Sigma\\in\\mathbb{R}^{N \\times n}\\), and \\(V^\\top \\in\\mathbb{R}^{n\\times n}\\), so that\n\\[\nX = U\\Sigma V^\\top\n\\qquad(5.1)\\]\nThe matrices \\(U\\) and \\(V\\) are orthogonal (unitary), so that \\(U^\\top U = I_N\\) and \\(V^\\top V = I_n\\), and the matrix \\(\\Sigma\\) is diagonal. The diagonal entries of \\(\\Sigma\\) are denoted \\(\\sigma_1,\\sigma_2,\\ldots,\\) and are called the singular values of \\(X\\), and are always non-negative and in non-increasing order, that is: \\(\\sigma_1\\geq \\sigma_2\\geq \\cdots \\geq 0\\). \\(\\Sigma\\) is called the singular value matrix, and the number of nonzero singular values is equal to the rank of the matrix \\(X\\). The matrix \\(U\\) is called the left singular vector matrix, and its columns are called the left singular vectors. The matrix \\(V\\) is called the right singular vector matrix, and its columns are called the right singular vectors.\nTo compute the SVD in python, you would use numpy.linalg.svd or scipy.linalg.svd. Like other matrix decompositions, the time complexity to compute the SVD is third-order, specifically \\(\\mathcal{O}(\\min(Nn^2, N^2n))\\). (Note that when \\(N&gt;n\\) this is the same time complexity as the QR decomposition for tall matrices.) The storage complexity is \\(\\mathcal{O}(\\max(N,n)^2)\\).\nAbove, we have described the full SVD. Another version of the SVD is the reduced SVD or the thin SVD1. Let \\(k = \\min(N,n)\\). The reduced SVD decomposes \\(X\\) into the product \\(X=U\\Sigma V^\\top\\) where \\(U\\in\\mathbb{R}^{N\\times k}\\), \\(\\Sigma\\in\\mathbb{R}^{k\\times k}\\), and \\(V\\in\\mathbb{R}^{n\\times k}\\). The matrices \\(U\\) and \\(V\\) are still unitary, so that \\(U^\\top U = I_k\\) and \\(V^\\top V = I_k\\) and \\(\\Sigma\\) is square and diagonal. When one of the dimensions of \\(X\\) is much larger than the other, this can lead to signficant cost savings in both time and memory. Note that the reduced SVD is subtly different from the truncated SVD which we describe next.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LLS and the singular value decomposition</span>"
    ]
  },
  {
    "objectID": "13_lsnla.html#computational-cost-of-solving-linear-systems",
    "href": "13_lsnla.html#computational-cost-of-solving-linear-systems",
    "title": "4  LLS: Computational Cost",
    "section": "4.2 Computational cost of solving linear systems",
    "text": "4.2 Computational cost of solving linear systems\n\nWe now assess the computational cost of two common linear algebra algorithms: (1) solving linear systems via LU decomposition, and (2) solving least squares problems via QR decomposition. We will start with a brief review of these decompositions1 before diving into the algorithms that are used to compute them and their computational cost.\n\n4.2.1 LU and QR decomposition: linear algebra review\nThe LU decomposition is applicable to square matrices \\(X\\). The LU decomposition (or “factorization”) writes a square matrix \\(X\\) as a product of a lower triangular matrix \\(L\\) with an upper triangular matrix \\(U\\), that is, \\(X=LU\\). Sometimes we have to first re-order the rows of \\(X\\) in order to be able to give it an LU factorization; we do this with a permutation matrix \\(P\\), which is a square matrix made up of permuted rows of the identity matrix \\(I\\). In this case, the result is called an “LU factorization with partial pivoting”, and is written as \\(PX = LU\\). Such a factorization (with pivoting) always exists for any square matrix (note that if no row permutation is required then we just have \\(P=I\\)).\n\n\n\n\n\n\nNote\n\n\n\nDo you remember what it means for a matrix to be upper/lower triangular?\n\n\nThe QR decomposition is applicable to square or tall rectangular matrices, \\(X\\in\\mathbb{R}^{N\\times n}\\), where \\(N\\geq n\\), and allows the matrix \\(X\\) to be written as a product of an orthogonal (unitary) matrix \\(Q\\) with an upper triangular matrix \\(R\\). There are two versions of the QR decomposition, the full QR decomposition and the reduced QR decomposition:\n\nin the full QR decomposition, \\(X=QR\\) where \\(Q\\in\\mathbb{R}^{N\\times N}\\) is square with orthogonal columns so that \\(Q^\\top Q = I_N\\) (the \\(N\\)-by-\\(N\\) identity matrix), and \\(R\\in\\mathbb{R}^{N\\times n}\\) is an upper-triangular rectangular matrix. Note that in this case the columns of \\(Q\\) are an orthonormal basis for \\(\\mathbb{R}^N\\).\nin the reduced QR decomposition, \\(X=QR\\) where \\(Q\\in\\mathbb{R}^{N\\times n}\\) is rectangular with orthogonal columns so that \\(Q^\\top Q = I_n\\) (the \\(n\\)-by-\\(n\\) identity), and \\(R\\in\\mathbb{R}^{n\\times n}\\) is a square upper triangular matrix. In this case, the columns of \\(Q\\) are an orthonormal basis for \\(\\textsf{Ran}(X)\\) (the range/columnspace of \\(X\\)).\n\nUnfortunately, people often say “QR decomposition” without explicitly specifying whether they mean the full or reduced version, and you have to consider the context carefully to see what they mean. But we will focus on the reduced QR decomposition because it’s most relevant for least squares problems.\nSimilar to the LU decomposition, it will sometimes be helpful to first permute the columns of \\(X\\) before doing a QR decomposition, leading to the expression \\(XP=QR\\). This is referred to as “QR factorization with partial pivoting”, or sometimes “rank-revealing QR” (for reasons that we’ll get into in a later chapter).\n\n\n4.2.2 LU and QR decomposition: algorithms\nWe now turn our attention to algorithms that compute the LU and QR decompositions. To simplify our exposition, we are going to assume that no pivoting is required for either decomposition.\n\n4.2.2.1 LU decomposition\nHere is some pseudocode for the LU decomposition without pivoting:\nLU decomposition of \\(X\\in\\mathbb{R}^{n\\times n}\\) without pivoting: pseudocode\n\nInitialize \\(L = I\\) and \\(U = X\\).\nfor \\(j = 1\\) to \\(n\\):\n\nfor \\(i = 1\\) to \\(j\\):\n\n\\(s_1 = 0\\)\nfor \\(k = 1\\) to \\(i\\):\n\n\\(s_1 = s_1 + u_{kj} \\ell_{ik}\\)\n\n\\(u_{ij} = x_{ij} - s_1\\)\n\nfor \\(i = (j + 1)\\) to \\(n\\):\n\n\\(s_2 = 0\\)\nfor \\(k = 1\\) to \\(j - 1\\):\n\n\\(s_2 = s_2 + u_{kj}  \\ell_{ik}\\)\n\n\\(\\ell_{ij} = \\frac{x_{ij} - s_2}{u_{jj}}\\)\n\n\nreturn \\(L\\), \\(U\\)\n\nTime complexity analysis\nLet’s understand the time complexity of LU decomposition by counting the number of FLOPs involved. Step 1 initializing two square matrices \\(L,U\\in\\mathbb{R}^{n\\times n}\\). This involves initializing two array with \\(n^2\\) values and therefore is treated as having complexity \\(2n^2 = \\mathcal{O}(n^2)\\), although no FLOPs are technically involved.\nStep 2 is where things get hairy: note that there is an outer loop (for \\(j=1,\\ldots,n\\)) around two inner loops, each of which has an inner-most loop. We have to count the operations very carefully here using arithmetic series.\nConsider the first inner loop (for \\(i=1,\\ldots,j\\)): for each \\(i\\), there is one initialization operation (let’s count this as 1 FLOP) for \\(s_1\\), then \\(i\\) addition operations and \\(i\\) multiplication operations due to the inner-most for loop, and finally 1 more subtraction after the inner-most for loop, for a total of \\(2i+2\\) operations. Thus, the total number of operations in the first inner loop is \\(\\sum_{i=1}^j (2i+2) = j(j+3)\\). Finally we sum over the outer loop to get \\(\\sum_{j=1}^n j(j+3) = \\frac13 n(n+1)(n+5) = \\frac13n^3 + 2n^2 + \\frac53 n\\).\nWe do a similar calculation for the second inner loop (for \\(i = (j+1),\\ldots,n\\)): for each \\(i\\), there is one initialization (1 FLOP) of \\(s_2\\), then there are \\((j-1)\\) additions and \\((j-1)\\) multiplications, followed by 1 subtraction and 1 division, for a total of \\(2(j-1)+3=2j+1\\) FLOPs. Summing over this inner loop then gives us \\(\\sum_{i=(j+1)}^n (2j+1) = (2j+1)(n-j)\\) FLOPs, and then we sum over the outer loop to get \\(\\sum_{j=1}^n(2j+1)(n-j) = \\frac16 n(2n^2 +3n-5) = \\frac13n^3 + \\frac12 n^2 - \\frac56 n\\) FLOPs.\nAdding the total FLOPs over both loops gives us \\(\\frac23 n^3 + \\frac52 n^2 + 5/6 n\\) FLOPs, where the leading term is the cubic term so we just say that LU decomposition is \\(\\mathcal{O}(n^3)\\) or “cubic” or “third-order” in cost. Again, we emphasize that this is large-\\(n\\) property – at small \\(n\\) the other terms of the polynomial expression may have more of an influence, but as \\(n\\to\\infty\\) you will see the leading term dominate the cost.\nMemory complexity analysis\nMemory complexity is a little bit simpler — we just want to look at what quantities need to be stored in RAM to do our computation: for this we can just look at the variables that appear in the algorithm, which are \\(L, U, X\\in\\mathbb{R}^{n\\times n}\\) along with \\(s_1,s_2\\in\\mathbb{R}\\). The latter are just two scalars, so the cost is dominated by the cost of storing the three matrices \\(L,U,X\\), and thus the memory complexity is \\(\\mathcal{O}(n^2)\\).\n\n\n4.2.2.2 QR decomposition\nHere’s some pseudocode for the QR decomposition using a method called the “modified Gram-Schmidt” procedure:\nQR decomposition of \\(X\\in\\mathbb{R}^{N\\times n}\\) without pivoting: pseudocode\n\nInitialize \\(V = X\\), \\(R=0_{n\\times n}\\), \\(Q = 0_{N\\times n}\\).\nfor \\(i = 1,\\ldots, n\\):\n\n\\(r_{ii} = \\|v_{:,i}\\|\\)\n\\(q_{:,i} = v_{:,i}/r_{ii}\\)\nfor \\(j=i+1,\\ldots,n\\)\n\n\\(r_{ij} = q_{:,i}^\\top v_{:,j}\\)\n\\(v_{:,j} = v_{:,j} - r_{ij}q_{:,i}\\)\n\n\n\nwhere \\(v_{:,i}\\) and \\(q_{:,i}\\) denote the \\(i\\)th column vectors of \\(V\\) and \\(Q\\), respectively.\nThe QR decomposition has time complexity \\(\\mathcal{O}(N\\cdot n^2)\\) and storage complexity \\(\\mathcal{O}(Nn)\\). For square matrices where \\(N=n\\), the complexity is \\(\\mathcal{O}(N^3)\\). In general, computing matrix decompositions will have third-order complexity: we will see another example when we get to the SVD in the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLS: Computational Cost</span>"
    ]
  },
  {
    "objectID": "13_lsnla.html#exercises",
    "href": "13_lsnla.html#exercises",
    "title": "4  LLS: Computational Cost",
    "section": "4.4 Exercises",
    "text": "4.4 Exercises\n\nShow that the QR decomposition has time complexity \\(\\mathcal{O}(N n^2)\\).\nProvide pseudocode for a backward substitution algorithm analogous to the forward substitution algorithm provided (without looking it up, which you could easily do — but if you can derive it yourself then you know you understand it).\nShow that forward (or backward) substitution has complexity \\(\\mathcal{O}(n^2)\\).\nShow that the overall cost of solving \\(X\\beta=Y\\) via QR decomposition is \\(\\mathcal{O}(Nn^2)\\).\nDetermine how much RAM you have on your personal computer. How many double precision floats can you store in that space? What does this mean for the largest square linear system you can solve using your computer? What are the dimensions of the largest least squares problem you can solve using the QR decomposition on your computer?\nWrite code to set up and solve least squares problems of arbitrary dimension and time the solve. Plot the solve times as the dimensions increase. Do you observe the expected trends? At what point does your computer run out of memory and is it approximately what you expected?\n\n\n\n\n\nStrang, Gilbert. 2022. Introduction to Linear Algebra.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLS: Computational Cost</span>"
    ]
  },
  {
    "objectID": "13_lsnla.html#solving-linear-systems-using-matrix-decompositions",
    "href": "13_lsnla.html#solving-linear-systems-using-matrix-decompositions",
    "title": "4  LLS: Computational Cost",
    "section": "4.3 Solving linear systems using matrix decompositions",
    "text": "4.3 Solving linear systems using matrix decompositions\nFinally we consider the overall cost of solving linear systems of equations using matrix decompositions. In general, the best practice when you are coding something is to use functions that are part of standard linear algebra packages to solve these systems. In python, both the numpy and scipy libraries have linalg packages that contain various useful linear algebra functions. Both are based on the LAPACK library, which efficiently implements many linear algebra computations in Fortran. Popular deep learning packages like PyTorch and TensorFlow have their own linear algebra packages that build on other libraries that leverage GPU computing capabilities. The details in this section explain what happens under the hood in these linear algebra packages: LU decomposition is used to solve invertible linear systems and QR decomposition is used to solve least squares problems where the data matrix has full column rank.\n\n\n\n\n\n\nWarning\n\n\n\nIn this section, we are assuming that \\(X\\) has full column rank (which guarantees invertibility if \\(X\\) is square), whereas previously we did not make this assumption.\n\n\n\n4.3.1 Invertible linear systems\nIf \\(X\\) is invertible (and therefore square), the python functions we use to solve the system \\(X\\beta=Y\\) are numpy.linalg.solve and scipy.linalg.solve. Both of these functions use LU decomposition to solve the system. This means they first compute the factors \\(L\\) and \\(U\\) and described above. Then, once the LU factorization of \\(X\\) is found, then we can solve for \\(\\beta\\) in two steps:\n\nSolve \\(L\\alpha = Y\\) for \\(\\alpha\\).\nSolve \\(U\\beta = \\alpha\\) for \\(\\beta\\).\n\nThis may look like we’ve just overcomplicated things by turning our one problem of solving \\(X\\beta = Y\\) into two problems, but the triangular structure of \\(L\\) and \\(U\\) lets us solve these two new systems relatively efficiently. To see this, consider solving \\(L\\alpha = Y\\) for a three-dimensional example:\n\\[\nL = \\begin{pmatrix}1 & & \\\\ 2 & 1 & \\\\\n2 & 1 & 2\n\\end{pmatrix}, \\qquad Y = \\begin{pmatrix}1 \\\\ 2 \\\\ 0\\end{pmatrix}.\n\\]\nRecall that this represents a system of three linear equations:\n\\[\n\\begin{aligned}\n1 \\alpha_1 &             & = 1\\\\\n2 \\alpha_1 &+ 1 \\alpha_2  &= 2 \\\\\n2 \\alpha_1 &+ 1 \\alpha_2 + 2 \\alpha_3 & = 0\n\\end{aligned}\n\\]\nWe can immediately read off the solution of the first equation, \\(\\alpha_1 = 1\\) because there is only one unknown variable in it. From there we substitute that into the second equation to get \\(2 + 1\\alpha_2 = 2\\), so we again one unknown variable, which we can easily solve to get \\(\\alpha_2 = 0\\). From there we now substitute our values for \\(\\alpha_1\\) and \\(\\alpha_2\\) into the third equation, yielding \\(2 + 2\\alpha_3 = 0\\), which again has one unknown variable, so we can easily solve to get \\(\\alpha_3 = -1\\). This yields our overall solution \\(\\alpha = (1, 0, -1)^\\top\\).\nThe process we have just described is called forward substitution and is a standard algorithm for solving lower triangular systems. It generalizes easily to triangular systems of arbitrary dimension \\(n\\times n\\) — we just keep substituting in the results of previous rows into the next row, as follows:\nForward substitution: pseudocode\n\n\\(\\alpha_1 = Y_1 / L_{11}\\).\nfor \\(i\\) = 2 to \\(n\\):\n\n\\(s = 0\\)\nfor \\(j\\) = 1 to \\((i - 1)\\):\n\n\\(s = s + L_{ij}\\alpha_j\\)\n\n\\(\\alpha_i = (Y_i - s) / L_{ii}\\)\n\nreturn \\(\\alpha\\)\n\nForward substitution has complexity \\(\\mathcal{O}(n^2)\\).\nNow that we have solved \\(L\\alpha = Y\\) for \\(\\alpha\\) using forward substitution, we need to solve \\(U\\beta = \\alpha\\) for \\(\\beta\\), where \\(U\\) is upper triangular rather than lower triangular. This is done using backward substitution, which is a very similar algorithm to forward substitution and also has complexity \\(\\mathcal{O}(n^2)\\). That gives us our final solution \\(\\beta\\).\nThe overall computational cost of solving an invertible linear system is dominated by the cubic cost of the LU decomposition (for large \\(n\\), the quadratic costs of forward and backward substitution are much cheaper than the cubic cost of LU). Thus, we say linear solves have cubic complexity.\n\n\n4.3.2 Least squares problems\nIf \\(X\\) is non-invertible, the python functions we use to solve \\(X\\beta = Y\\) are numpy.linalg.lstsq and scipy.linalg.lstsq. These functions can use a number of different algorithms to solve the least squares problem, but a common choice for overdetermined least squares problems is to use QR decomposition. That is, we first compute the factors \\(Q\\in\\mathbb{R}^{N\\times n}\\) and \\(R\\in\\mathbb{R}^{n\\times n}\\) as described above. Then, we can solve for \\(\\beta\\) is two steps:\n\n\nSolve \\(Q\\alpha = Y\\) for \\(\\alpha\\).\nSolve \\(R\\beta = \\alpha\\) for \\(\\beta\\).\n\nRecall that orthogonal matrices \\(Q\\) satisfy \\(Q^\\top Q=I\\), so this special structure lets us find \\(\\alpha\\) via a matrix multiplication, which is computationally more efficient than a full LU decomposition. Then, since \\(R\\) is upper triangular, we can solve for \\(\\beta\\) using backward substitution.\nIf \\(X\\) is does not have full column rank, there are a couple of options for solving the least squares problem, but the standard one is based on the the singular value decomposition, which deserves its own set of notes, which come next.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLS: Computational Cost</span>"
    ]
  },
  {
    "objectID": "14_lssvd.html#truncated-svd-and-eckart-young",
    "href": "14_lssvd.html#truncated-svd-and-eckart-young",
    "title": "5  LLS and the singular value decomposition",
    "section": "5.2 Truncated SVD and Eckart Young",
    "text": "5.2 Truncated SVD and Eckart Young\nPCA features, data compression",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LLS and the singular value decomposition</span>"
    ]
  },
  {
    "objectID": "14_lssvd.html#the-pseudo-inverse-and-solving-least-squares-problems",
    "href": "14_lssvd.html#the-pseudo-inverse-and-solving-least-squares-problems",
    "title": "5  LLS and the singular value decomposition",
    "section": "5.3 the pseudo inverse and solving least squares problems",
    "text": "5.3 the pseudo inverse and solving least squares problems\nplus truncated SVD regularization?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LLS and the singular value decomposition</span>"
    ]
  },
  {
    "objectID": "14_lssvd.html#utility-in-understanding-numerical-conditioning",
    "href": "14_lssvd.html#utility-in-understanding-numerical-conditioning",
    "title": "5  LLS and the singular value decomposition",
    "section": "5.4 utility in understanding numerical conditioning",
    "text": "5.4 utility in understanding numerical conditioning\nnext ste of notes: machine precision, round off error, SVD and conditioning.\npython examples showing nonzero residual for LU and conditioning error for QR?\nintroduce the SVD in math, singular values vs rank/conditioning, explain the concept of conditioning in terms of being essentially non-invertible from the computer’s point of view, cond/rank commands\nshow that normal equations are terribly conditioned.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LLS and the singular value decomposition</span>"
    ]
  },
  {
    "objectID": "14_lssvd.html#exercises",
    "href": "14_lssvd.html#exercises",
    "title": "5  LLS and the singular value decomposition",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\n\nShow that Equation 5.1 and Equation 5.2 are equivalent (nothing fancy, just write it out using what you know about the structure of \\(U\\), \\(\\Sigma\\), and \\(V\\)).\nLet \\(X\\in\\mathbb{R}^{N\\times n}\\) have rank \\(r\\) and let \\(\\beta\\in\\mathbb{R}^n\\). Show that the matrix multiplication \\(X\\beta\\) can be computed using the expression Equation 5.3 in \\(\\mathcal{O}(r(N+n))\\) FLOPs.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LLS and the singular value decomposition</span>"
    ]
  },
  {
    "objectID": "14_lssvd.html#the-truncated-svd",
    "href": "14_lssvd.html#the-truncated-svd",
    "title": "5  LLS and the singular value decomposition",
    "section": "5.2 The truncated SVD",
    "text": "5.2 The truncated SVD\nLet \\(k=\\min(N,n)\\) as before. Then, we can rewrite Equation 5.1 as follows:\n\\[\nX = \\sum_{i=1}^k u_i \\sigma_i v_i^\\top = \\sum_{i=1}^k \\sigma_i (u_i v_i^\\top)\n\\qquad(5.2)\\]\nwhere \\(u_1,\\ldots,u_N\\in\\mathbb{R}^N\\) are the left singular vectors (columns of \\(U\\)) and \\(v_1,\\ldots,v_n\\in\\mathbb{R}^n\\) are the right singular vectors (columns of \\(V\\)), and \\(\\sigma_i\\) are again the diagonal entries in \\(\\Sigma\\). The matrices \\(u_1v_1^\\top,\\ldots,u_kv_k^\\top\\in\\mathbb{R}^{N\\times n}\\) are rank-1 matrices formed by taking the outer product of \\(u_i\\) with \\(v_i\\).2 Recall that the number of nonzero singular values is equal to the matrix rank and let \\(r = \\textsf{rank}(X)\\). Thus, we can also write\n\\[\nX = \\sum_{i=1}^{r} \\sigma_i (u_i v_i^\\top)\n\\qquad(5.3)\\]\nWe say a matrix is low rank when its rank is less than its maximum possible rank \\(k\\). A low-rank matrix can be stored more efficiently than a full rank matrix: note that \\(X\\in\\mathbb{R}^{N\\times n}\\) generally has \\(Nn\\) elements that must be stored, but if the the matrix has rank \\(r\\), then from Equation 5.3 you can see that it would be more efficient to store \\(r\\) sets of \\((\\sigma_i, u_i, v_i)\\), for a total of \\(r(N+n+1)\\) values. (In fact if storage is your only goal, you can just store the product \\(\\sigma_iu_i\\) and ditch that last 1 in the parentheses.) Low-rank matrices are also more efficient to compute with: while a standard matrix vector multiplication \\(X\\beta\\) where \\(\\beta\\in\\mathbb{R}^n\\) costs \\(\\mathcal{O}(Nn)\\) FLOPs, we can do a matrix vector multiplication using the expression in Equation 5.3 in just \\(\\mathcal{O}(r(N+n))\\) FLOPs (see Exercises).\nBecause low-rank matrices are more efficient to work with and to store, it is often useful to approximate a full-rank matrix with a low-rank matrix (or approximate a low-rank matrix with an even lower-rank matrix). By far the most common way to do this is using the truncated SVD, which just truncates the sum Equation 5.2 at some \\(\\ell &lt; k\\), as follows:\n\\[\nX_\\ell = \\sum_{i=1}^\\ell \\sigma_i(u_iv_i^\\top) = U_\\ell \\Sigma_\\ell V_\\ell^\\top\n\\]\nwhere \\(U_\\ell\\in\\mathbb{R}^{N\\times \\ell}\\) contains the left-most \\(\\ell\\) columns of \\(U\\), and \\(V_\\ell\\in\\mathbb{R}^{n\\times \\ell}\\) contains the left-most \\(\\ell\\) columns of \\(V\\), and \\(\\Sigma_\\ell\\) contains the upper-left \\(\\ell\\)-by-\\(\\ell\\) block of \\(\\Sigma\\) and thus has \\(\\ell\\) diagonal entries. The truncated SVD \\(X_\\ell\\) is the best rank-\\(\\ell\\) approximation of the matrix \\(X\\) — this is a result called the Eckart-Young Theorem:\nTheorem (Eckart-Young). Let \\(X\\in\\mathbb{R}^{N\\times n}\\) and let \\(\\mathcal{M}_\\ell\\) denote the class of matrices in \\(\\mathbb{R}^{N\\times n}\\) with rank equal to or less than \\(\\ell\\). Then, \n\\[\nX_\\ell = \\arg\\min_{M_\\ell \\in\\mathcal{M}_\\ell} ||X - M_\\ell||_F^2\n\\qquad(5.4)\\]\nThat is, the truncated SVD is the rank-\\(\\ell\\) matrix that is closest to the original matrix \\(X\\). Above, we have stated the theorem in the Frobenius norm3, but it also holds for the matrix \\(2\\)-norm. This optimality result is the reason the SVD is one of the most powerful and broadly used matrix decompositions there is. The minimum of Equation 5.4 gives the error incurred by the approximation, and is given by what’s called the “tail sum” of the square of the singular values, that is,\n\\[\n\\min_{M_\\ell \\in\\mathcal{M}_\\ell} ||X - M_\\ell||_F^2 = \\|X_\\ell - X||_F^2 = \\sum_{i=\\ell+1}^k \\sigma_i^2\n\\]\n\n5.2.1 Applications of the truncated SVD in scientific machine learning\nOne application of the SVD is for data compression. If you have a very large data set \\(Z\\in\\mathbb{R}^{N\\times d}\\) with either/both \\(N,d\\), very large, you can quickly have too much data to store easily on whatever machine is available to you. This is a common challenge in scientific simulation of partial differential equations (e.g., in computational fluid dynamics or finite element analysis of structures) where \\(d\\) generally scales with the size of the computational mesh, which can easily have hundreds of millions of cells. Instead of storing the full data matrix, we can store the singular vectors and values needed to represent the truncated SVD. A similar idea is used to compress images for efficient storage.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LLS and the singular value decomposition</span>"
    ]
  },
  {
    "objectID": "14_lssvd.html#footnotes",
    "href": "14_lssvd.html#footnotes",
    "title": "5  LLS and the singular value decomposition",
    "section": "",
    "text": "As with many of the terms we have encountered and will encounter in this course, the term “SVD” is used in the wild to mean either the full SVD or the reduced SVD – often the distinction is not super important, but when it is you need to be able to discern from context what is meant.↩︎\nRecall that the rank of a matrix describes the number of linearly independent rows/columns of the matrix; to see that \\(ab^\\top\\) has rank 1 when \\(a,b\\) are vectors, note that the \\(j\\)th row of \\(ab^\\top\\) is given by \\(a_jb^\\top\\), where \\(a_j\\in\\mathbb{R}\\) is the \\(j\\)th element of \\(a\\), so that \\(ab^\\top\\) is just the same row multiplied by different constants.↩︎\nthe Frobenius norm of a matrix \\(A\\), denoted \\(\\|A\\|_F\\), is the natural generalization of the Euclidean norm from vectors to matrices – it is given by the square root of the sum of squares of all matrix elements.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LLS and the singular value decomposition</span>"
    ]
  }
]