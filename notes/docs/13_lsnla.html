<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>AE 4803 AIM: Course Notes - 4&nbsp; LLS: Computational Cost</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./12_lslinalg.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./13_lsnla.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">LLS: Computational Cost</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AE 4803 AIM: Course Notes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_ls.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Least Squares Problems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_lslinalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">LLS: A linear algebra perspective</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_lsnla.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">LLS: Computational Cost</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00_linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Linear algebra review</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#measuring-computational-cost" id="toc-measuring-computational-cost" class="nav-link active" data-scroll-target="#measuring-computational-cost"><span class="header-section-number">4.1</span> Measuring computational cost</a>
  <ul class="collapse">
  <li><a href="#measuring-time-complexity-using-flops" id="toc-measuring-time-complexity-using-flops" class="nav-link" data-scroll-target="#measuring-time-complexity-using-flops"><span class="header-section-number">4.1.1</span> Measuring time complexity using FLOPs</a></li>
  <li><a href="#measuring-memory-complexity" id="toc-measuring-memory-complexity" class="nav-link" data-scroll-target="#measuring-memory-complexity"><span class="header-section-number">4.1.2</span> Measuring memory complexity</a></li>
  </ul></li>
  <li><a href="#computational-cost-of-solving-linear-systems" id="toc-computational-cost-of-solving-linear-systems" class="nav-link" data-scroll-target="#computational-cost-of-solving-linear-systems"><span class="header-section-number">4.2</span> Computational cost of solving linear systems</a>
  <ul class="collapse">
  <li><a href="#lu-and-qr-decomposition-linear-algebra-review" id="toc-lu-and-qr-decomposition-linear-algebra-review" class="nav-link" data-scroll-target="#lu-and-qr-decomposition-linear-algebra-review"><span class="header-section-number">4.2.1</span> LU and QR decomposition: linear algebra review</a></li>
  <li><a href="#lu-and-qr-decomposition-algorithms" id="toc-lu-and-qr-decomposition-algorithms" class="nav-link" data-scroll-target="#lu-and-qr-decomposition-algorithms"><span class="header-section-number">4.2.2</span> LU and QR decomposition: algorithms</a></li>
  </ul></li>
  <li><a href="#solving-linear-systems-using-matrix-decompositions" id="toc-solving-linear-systems-using-matrix-decompositions" class="nav-link" data-scroll-target="#solving-linear-systems-using-matrix-decompositions"><span class="header-section-number">4.3</span> Solving linear systems using matrix decompositions</a>
  <ul class="collapse">
  <li><a href="#invertible-linear-systems" id="toc-invertible-linear-systems" class="nav-link" data-scroll-target="#invertible-linear-systems"><span class="header-section-number">4.3.1</span> Invertible linear systems</a></li>
  <li><a href="#least-squares-problems" id="toc-least-squares-problems" class="nav-link" data-scroll-target="#least-squares-problems"><span class="header-section-number">4.3.2</span> Least squares problems</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">4.4</span> Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">LLS: Computational Cost</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="hidden">
<!-- \renewcommand{\P}{\mathrm{P}} -->
<!-- $$
\DeclarePairedDelimiters{\set}{\{}{\}}
\DeclareMathOperator*{\argmax}{argmax}
$$

\definecolor{quarto-callout-note-color}{HTML}{4477AA} -->
</div>
<p>In this section we will introduce fundamental algorithms from computational linear algebra that are used to solve linear systems and linear least squares problems. We will also introduce and discuss some of the key properties and theoretical considerations when using these algorithms.</p>
<p>The intended learning outcomes of these notes are that students should be able to:</p>
<ol type="1">
<li><p>Explain how LU decomposition and QR decompositions are used to solve linear systems and linear least squares problems, respectively.</p></li>
<li><p>Analyze the time complexity of numerical linear algebra algorithms by deriving expressions for the number of FLOPs in terms of matrix size</p></li>
<li><p>Analyze the memory complexity and cost of numerical linear algebra algorithms in terms of both matrix size and bits of storage</p></li>
</ol>
<section id="measuring-computational-cost" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="measuring-computational-cost"><span class="header-section-number">4.1</span> Measuring computational cost</h2>
<p>A key property of any algorithm is its computational cost. The two types of cost we are interested in are <em>time</em> and <em>memory</em>: the former describes how long an algorithm takes to run, and the latter describes how many bytes of RAM we must have available in order to run the computation. For both of these measures, we are interested in understanding how the time and memory costs change as the dimensions of the problem (the number of data <span class="math inline">\(N\)</span> or the number of features <span class="math inline">\(d\)</span>) change. These properties are often referred to as the (time or memory) <em>complexity</em> of the algorithm, or as the (time or memory) cost <em>scaling</em> of the algorithm.</p>
<section id="measuring-time-complexity-using-flops" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="measuring-time-complexity-using-flops"><span class="header-section-number">4.1.1</span> Measuring time complexity using FLOPs</h3>
<p>We will focus on compute time first. Since compute time will vary from computer to computer, the way we measure time complexity in computational linear algebra is by counting the number of <em>floating point operations (FLOPs)</em> required by an algorithm. A single FLOP refers to the addition, subtraction, multiplication, or division of two scalar quantities. Compute speed of supercomputers is measured in FLOPs per second (also abbreviated FLOPs), and you’ll the fastest computers in the world measured in gigaFLOPs (<span class="math inline">\(10^9\)</span>), teraFLOPs (<span class="math inline">\(10^{12}\)</span>), or even exaFLOPs (<span class="math inline">\(10^{18}\)</span>). All computational linear algebra algorithms are simply comprised of a bunch of FLOPs strung together. To see this, consider adding two vectors, <span class="math inline">\(a,b\in\mathbb{R}^n\)</span>:</p>
<p><span class="math display">\[
a + b = \begin{pmatrix}a_1 \\ a_2 \\ \vdots \\ a_n\end{pmatrix} +
\begin{pmatrix}b_1 \\ b_2 \\ \vdots \\ b_n\end{pmatrix}
\]</span></p>
<p>Since vector addition means that the corresponding elements each get added to each other, there are as many scalar addition operations as there are elements of the the vector. Thus vector addition has cost <span class="math inline">\(n\)</span>.</p>
<p>Let’s now consider another operation between vectors, the dot product:</p>
<p><span class="math display">\[
a\cdot b = a^\top b = a_1b_1 + a_2b_2 + \cdots + a_nb_n
\]</span></p>
<p>The dot product involves <span class="math inline">\(n\)</span> element-wise multiplications followed by <span class="math inline">\((n-1)\)</span> additions. The total cost is thus <span class="math inline">\(2n-1\)</span> FLOPs.</p>
<p>Recall that we are most interested in how the computational cost of a computation <em>scales</em> as the dimension of the problem changes. For both vector addition and the dot product between vectors, the computational cost is said to scale <em>linearly</em> with the size of the vectors <span class="math inline">\(n\)</span>. This is because the highest-order polynomial term in the cost is <span class="math inline">\(n^1\)</span>, so this linear term dominates the cost as <span class="math inline">\(n\)</span> becomes large. This means that if we increase <span class="math inline">\(n\)</span> by a factor of 10, the cost will also increase by approximately a factor of 10. Note that this is true regardless of what the constant factor in front of the <span class="math inline">\(n^1\)</span> term is. Because of this, it is common to write that the cost is <span class="math inline">\(\mathcal{O}(n)\)</span> (read this notation as “order n” or “big-O n”, although the technical definition of “big-O” is not one I plan to get into). Other common terminology is to say that the computation has “first-order” cost scaling/complexity.</p>
<p>As a final example of FLOP-counting in action before we move on, consider matrix-vector multiplication. That is, let <span class="math inline">\(A\in\mathbb{R}^{m\times n}\)</span> and let <span class="math inline">\(b\in\mathbb{R}^n\)</span>. Then, note that</p>
<p><span class="math display">\[
Ab = \begin{pmatrix}a_{1,:}\cdot b \\ a_{2,:}\cdot b \\ \vdots \\ a_{m,:} \cdot b \end{pmatrix}
\]</span></p>
<p>where <span class="math inline">\(a_{i,:}\in\mathbb{R}^n\)</span> denotes the row vector that is the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(A\)</span>. Writing it this way shows that a matrix-vector multiplication consists of <span class="math inline">\(m\)</span> dot products between two vectors of length <span class="math inline">\(n\)</span>. Thus, the total number of FLOPs is <span class="math inline">\(m(2n-1)\)</span>, which we say is <span class="math inline">\(\mathcal{O}(mn)\)</span>. If the matrix <span class="math inline">\(A\)</span> is square so that <span class="math inline">\(m=n\)</span>, we would say that matrix-vector multiplication is <span class="math inline">\(\mathcal{O}(n^2)\)</span>, i.e., it has second-order or quadratic cost scaling/complexity.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Cost scaling really describes the cost of algorithms as <span class="math inline">\(n\)</span> becomes <em>large</em>. This means that as you increase <span class="math inline">\(n\)</span> from 2 to 3, or from 3 to 30, you may not see the expected scaling in run time. This is because small computations happen fast enough the run time might be more influenced by how busy your computer is doing other things than by how many FLOPs it needs to do. However, as you increase <span class="math inline">\(n\)</span> by orders of magnitude, to 1000 and 10000 and so on, you should see the expected cost scaling as <span class="math inline">\(n\)</span> becomes larger.</p>
</div>
</div>
</section>
<section id="measuring-memory-complexity" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="measuring-memory-complexity"><span class="header-section-number">4.1.2</span> Measuring memory complexity</h3>
<p>There are essentially two considerations when measuring memory complexity of scientific computing algorithms: (1) how a computer stores a single number (scalar), and (2) how many scalars must be stored in order to carry out a computation.</p>
<section id="floating-point-numbers-an-introduction" class="level4" data-number="4.1.2.1">
<h4 data-number="4.1.2.1" class="anchored" data-anchor-id="floating-point-numbers-an-introduction"><span class="header-section-number">4.1.2.1</span> Floating point numbers: an introduction</h4>
<p>Computers represent numbers using <em>binary</em>, a base-2 numeral system (our usual number system is base-10, or “decimal”). Binary uses only two digits: 0 and 1. In binary, each digit is referred to as a “bit” (short for binary digit), and a group of eight bits is called a “byte.” In base-10, the right-most digit tells us how many ones (<span class="math inline">\(10^0\)</span>) we have, the second-right-most how many tens (<span class="math inline">\(10^1\)</span>), the third-right-most how many hundreds (<span class="math inline">\(10^2\)</span>), etc. Similarly, in binary the right-most bit tells us how many ones (<span class="math inline">\(2^0\)</span>) we have, the second-right-most how many 2s (<span class="math inline">\(2^1\)</span>), the third-right-most how many 4s (<span class="math inline">\(2^2\)</span>) etc. To see this in action, consider the binary number <span class="math inline">\(1101_2\)</span> (the subscript <span class="math inline">\(\cdot_2\)</span> tells us the number should be intepreted as a binary number as opposed to one-thousand-one-hundred-one). To convert <span class="math inline">\(1101_2\)</span> to decimal, we sum the powers of 2 corresponding to each bit:</p>
<p><span class="math display">\[
1101_2 = (1 \times 2^3) + (1 \times 2^2) + (0 \times 2^1) + (1 \times 2^0) = 8 + 4 + 0 + 1 = 13_{10}
\]</span></p>
<p>How can we use binary to represent decimal numbers, as well as negative numbers? If you have ever learned programming in a language like Java or C, you will have learned that computers can store variables in different <em>data types</em>. In scientific computing, the main data types we care about are single- and double-precision <em>floats</em> (short for floating-point number), which represent numbers with decimal points (as opposed to <code>int</code>s which represent integers). The difference between single-precision float and a double-precision float is the number of binary bits the computer uses to store the number. Single precision uses 32 bits, whereas double precision uses 64 bits. These are standard data types that divide up the available bits into three parts that define a type of binary scientific notation: the sign, the mantissa or significand, and the exponent. We convert these to a value as follows:</p>
<p><span class="math display">\[
\text{value} = (-1)^\text{sign} \cdot \text{significand} \cdot 2^\text{exponent}
\]</span></p>
<ul>
<li><p>single precision uses 1 bit for the sign, 8 bits for the exponent, and 23 bits for the “mantissa” or “significand” (the significant digits that pre-multiply the exponent)</p></li>
<li><p>double precision uses 1 bit for the sign, 11 bits for the exponent, and 52 bits for the mantissa/significand</p></li>
</ul>
<p>The sign bit can be 0 or 1 — if the bit is 0 we get a positive number and otherwise the number is negative. We do something clever with the significand: just like you would usually not write down the decimal number <span class="math inline">\(13\)</span> as <span class="math inline">\(0013\)</span>, in binary we also don’t want to “waste” bits with leading 0s. Thus, the significand is understood as having an implicit “1.” in front of all the bits that are actually stored, giving us 24 binary bits of precision in single-precision floats and 53 binary bits of precision in double-precision floats (note that all these bits of precision come after the implicit “1.”, meaning they’re associated with negative powers of 2). This translates to approximately 8 (15) decimal digits of precision for single-precision (double-precision) floats. Finally, for the exponent, the 8 (11) bits can represent the decimal numbers from 0 to 255 (0 to 2047), but we want to allow negative exponents in order to represent numbers that are smaller than 1. Thus, we subtract 127 (1023) from the number stored in the exponent bits to get the actual exponent.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Worked example: Converting the single-precision float “0 10000001 10100000000000000000000” to decimal
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>Extract the parts</strong>:
<ul>
<li>Sign bit (S): <code>0</code> (positive)</li>
<li>Exponent bits (E): <code>10000001</code></li>
<li>Significand (fraction) bits (F): <code>10100000000000000000000</code></li>
</ul></li>
<li><strong>Convert the exponent</strong>:
<ul>
<li>The exponent is <code>10000001</code> in binary, which is <code>129</code> in decimal.</li>
<li>The actual exponent is: <span class="math inline">\(E - 127 = 129 - 127 = 2\)</span>.</li>
</ul></li>
<li><strong>Convert the fraction</strong>:
<ul>
<li>The fraction is <code>1.F = 1.101</code> (because the leading 1 is implied).</li>
<li>Convert <code>1.101</code> to decimal: <span class="math inline">\(1.101_2 = (1 \times 2^0) + (1 \times 2^{-1}) + (0 \times 2^{-2}) + (1 \times 2^{-3}) = 1 + 0.5 + 0 + 0.125 = 1.625\)</span>.</li>
</ul></li>
<li><strong>Calculate the value</strong>: <span class="math display">\[
(-1)^S \times 1.F \times 2^{E - 127} = (-1)^0 \times 1.625 \times 2^2 = 1 \times 1.625 \times 4 = 6.5
\]</span></li>
</ol>
</div>
</div>
</div>
<p>The default for most computers you will encounter is to store decimal numbers as double-precision floats, and often when you hear someone just say “float” they in fact mean a double. However, the default precision in some popular machine learning packages is single precision.</p>
</section>
<section id="storing-matrices-and-vectors" class="level4" data-number="4.1.2.2">
<h4 data-number="4.1.2.2" class="anchored" data-anchor-id="storing-matrices-and-vectors"><span class="header-section-number">4.1.2.2</span> Storing matrices and vectors</h4>
<p>If you’re exhausted by the details above the good news is that things get simpler now. When doing computations with matrices and vectors, each element of the matrix/vector is some kind of float, and the cost to store the whole matrix/vector is just the number of elements multiplied by either 32 or 64 bits, depending on what kind of float it is. So a matrix <span class="math inline">\(X\in\mathbb{R}^{1000\times 25}\)</span> stored in double precision will require <span class="math display">\[
25000\times 64 \text{ bits}=1600000 \text{ bits} = 200000 \text{ bytes} = 200 \text{ kilobytes}
\]</span></p>
<p>If you’ve ever run a computation on your laptop and gotten an “Out of Memory” error, that’s your computer telling you it doesn’t have enough RAM to store all the numbers you need. This is a common challenge when working with “big data” in many ML applications: there are so many data points that we cannot load them all into memory at one time — there are some clever tricks to deal with that case that we’ll discuss later on in the course.</p>
<p>Note that the cost to store a vector scales linearly with its length, and the cost to store an <span class="math inline">\(m\)</span>-by-<span class="math inline">\(n\)</span> matrix scales with <span class="math inline">\(mn\)</span>, or if the matrix is square with <span class="math inline">\(m=n\)</span>, then quadratically in <span class="math inline">\(n\)</span>.</p>
<!-- add something about sparse matrices? -->
</section>
</section>
</section>
<section id="computational-cost-of-solving-linear-systems" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="computational-cost-of-solving-linear-systems"><span class="header-section-number">4.2</span> Computational cost of solving linear systems</h2>
<!-- this set of notes: LU, QR, pseudocode, time and memory cost. as an exercise, figure out how much RAM your computer has, what's the largest system you can comfortably solve? try crashing your computer.  -->
<p>We now assess the computational cost of two common linear algebra algorithms: (1) solving linear systems via LU decomposition, and (2) solving least squares problems via QR decomposition. We will start with a brief review of these decompositions<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> before diving into the algorithms that are used to compute them and their computational cost.</p>
<section id="lu-and-qr-decomposition-linear-algebra-review" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="lu-and-qr-decomposition-linear-algebra-review"><span class="header-section-number">4.2.1</span> LU and QR decomposition: linear algebra review</h3>
<p>The LU decomposition is applicable to square matrices <span class="math inline">\(X\)</span>. The LU decomposition (or “factorization”) writes a square matrix <span class="math inline">\(X\)</span> as a product of a lower triangular matrix <span class="math inline">\(L\)</span> with an upper triangular matrix <span class="math inline">\(U\)</span>, that is, <span class="math inline">\(X=LU\)</span>. Sometimes we have to first re-order the rows of <span class="math inline">\(X\)</span> in order to be able to give it an LU factorization; we do this with a permutation matrix <span class="math inline">\(P\)</span>, which is a square matrix made up of permuted rows of the identity matrix <span class="math inline">\(I\)</span>. In this case, the result is called an “LU factorization with partial pivoting”, and is written as <span class="math inline">\(PX = LU\)</span>. Such a factorization (with pivoting) always exists for any square matrix (note that if no row permutation is required then we just have <span class="math inline">\(P=I\)</span>).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Do you remember what it means for a matrix to be upper/lower triangular?</p>
</div>
</div>
<p>The QR decomposition is applicable to square or tall rectangular matrices, <span class="math inline">\(X\in\mathbb{R}^{N\times n}\)</span>, where <span class="math inline">\(N\geq n\)</span>, and allows the matrix <span class="math inline">\(X\)</span> to be written as a product of an orthogonal (unitary) matrix <span class="math inline">\(Q\)</span> with an upper triangular matrix <span class="math inline">\(R\)</span>. There are two versions of the QR decomposition, the full QR decomposition and the reduced QR decomposition:</p>
<ul>
<li><p>in the full QR decomposition, <span class="math inline">\(X=QR\)</span> where <span class="math inline">\(Q\in\mathbb{R}^{N\times N}\)</span> is square with orthogonal columns so that <span class="math inline">\(Q^\top Q = I_N\)</span> (the <span class="math inline">\(N\)</span>-by-<span class="math inline">\(N\)</span> identity matrix), and <span class="math inline">\(R\in\mathbb{R}^{N\times n}\)</span> is an upper-triangular rectangular matrix. Note that in this case the columns of <span class="math inline">\(Q\)</span> are an orthonormal basis for <span class="math inline">\(\mathbb{R}^N\)</span>.</p></li>
<li><p>in the reduced QR decomposition, <span class="math inline">\(X=QR\)</span> where <span class="math inline">\(Q\in\mathbb{R}^{N\times n}\)</span> is rectangular with orthogonal columns so that <span class="math inline">\(Q^\top Q = I_n\)</span> (the <span class="math inline">\(n\)</span>-by-<span class="math inline">\(n\)</span> identity), and <span class="math inline">\(R\in\mathbb{R}^{n\times n}\)</span> is a square upper triangular matrix. In this case, the columns of <span class="math inline">\(Q\)</span> are an orthonormal basis for <span class="math inline">\(\textsf{Ran}(X)\)</span> (the range/columnspace of <span class="math inline">\(X\)</span>).</p></li>
</ul>
<p>Unfortunately, people often say “QR decomposition” without explicitly specifying whether they mean the full or reduced version, and you have to consider the context carefully to see what they mean. But we will focus on the reduced QR decomposition because it’s most relevant for least squares problems.</p>
<p>Similar to the LU decomposition, it will sometimes be helpful to first permute the columns of <span class="math inline">\(X\)</span> before doing a QR decomposition, leading to the expression <span class="math inline">\(XP=QR\)</span>. This is referred to as “QR factorization with partial pivoting”, or sometimes “rank-revealing QR” (for reasons that we’ll get into in a later chapter).</p>
</section>
<section id="lu-and-qr-decomposition-algorithms" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="lu-and-qr-decomposition-algorithms"><span class="header-section-number">4.2.2</span> LU and QR decomposition: algorithms</h3>
<p>We now turn our attention to algorithms that compute the LU and QR decompositions. To simplify our exposition, we are going to assume that no pivoting is required for either decomposition.</p>
<section id="lu-decomposition" class="level4" data-number="4.2.2.1">
<h4 data-number="4.2.2.1" class="anchored" data-anchor-id="lu-decomposition"><span class="header-section-number">4.2.2.1</span> LU decomposition</h4>
<p>Here is some pseudocode for the LU decomposition without pivoting:</p>
<p><strong>LU decomposition of <span class="math inline">\(X\in\mathbb{R}^{n\times n}\)</span> without pivoting: pseudocode</strong></p>
<ol type="1">
<li>Initialize <span class="math inline">\(L = I\)</span> and <span class="math inline">\(U = X\)</span>.</li>
<li><code>for</code> <span class="math inline">\(j = 1\)</span> to <span class="math inline">\(n\)</span>:
<ul>
<li><code>for</code> <span class="math inline">\(i = 1\)</span> to <span class="math inline">\(j\)</span>:
<ul>
<li><span class="math inline">\(s_1 = 0\)</span></li>
<li><code>for</code> <span class="math inline">\(k = 1\)</span> to <span class="math inline">\(i\)</span>:
<ul>
<li><span class="math inline">\(s_1 = s_1 + u_{kj} \ell_{ik}\)</span></li>
</ul></li>
<li><span class="math inline">\(u_{ij} = x_{ij} - s_1\)</span></li>
</ul></li>
<li><code>for</code> <span class="math inline">\(i = (j + 1)\)</span> to <span class="math inline">\(n\)</span>:
<ul>
<li><span class="math inline">\(s_2 = 0\)</span></li>
<li><code>for</code> <span class="math inline">\(k = 1\)</span> to <span class="math inline">\(j - 1\)</span>:
<ul>
<li><span class="math inline">\(s_2 = s_2 + u_{kj}  \ell_{ik}\)</span></li>
</ul></li>
<li><span class="math inline">\(\ell_{ij} = \frac{x_{ij} - s_2}{u_{jj}}\)</span></li>
</ul></li>
</ul></li>
<li><code>return</code> <span class="math inline">\(L\)</span>, <span class="math inline">\(U\)</span></li>
</ol>
<p><strong>Time complexity analysis</strong></p>
<p>Let’s understand the time complexity of LU decomposition by counting the number of FLOPs involved. Step 1 initializing two square matrices <span class="math inline">\(L,U\in\mathbb{R}^{n\times n}\)</span>. This involves initializing two array with <span class="math inline">\(n^2\)</span> values and therefore is treated as having complexity <span class="math inline">\(2n^2 = \mathcal{O}(n^2)\)</span>, although no FLOPs are technically involved.</p>
<p>Step 2 is where things get hairy: note that there is an outer loop (<code>for</code> <span class="math inline">\(j=1,\ldots,n\)</span>) around two inner loops, each of which has an inner-most loop. We have to count the operations very carefully here using arithmetic series.</p>
<p>Consider the first inner loop (<code>for</code> <span class="math inline">\(i=1,\ldots,j\)</span>): for each <span class="math inline">\(i\)</span>, there is one initialization operation (let’s count this as 1 FLOP) for <span class="math inline">\(s_1\)</span>, then <span class="math inline">\(i\)</span> addition operations and <span class="math inline">\(i\)</span> multiplication operations due to the inner-most for loop, and finally 1 more subtraction after the inner-most for loop, for a total of <span class="math inline">\(2i+2\)</span> operations. Thus, the total number of operations in the first inner loop is <span class="math inline">\(\sum_{i=1}^j (2i+2) = j(j+3)\)</span>. Finally we sum over the outer loop to get <span class="math inline">\(\sum_{j=1}^n j(j+3) = \frac13 n(n+1)(n+5) = \frac13n^3 + 2n^2 + \frac53 n\)</span>.</p>
<p>We do a similar calculation for the second inner loop (<code>for</code> <span class="math inline">\(i = (j+1),\ldots,n\)</span>): for each <span class="math inline">\(i\)</span>, there is one initialization (1 FLOP) of <span class="math inline">\(s_2\)</span>, then there are <span class="math inline">\((j-1)\)</span> additions and <span class="math inline">\((j-1)\)</span> multiplications, followed by 1 subtraction and 1 division, for a total of <span class="math inline">\(2(j-1)+3=2j+1\)</span> FLOPs. Summing over this inner loop then gives us <span class="math inline">\(\sum_{i=(j+1)}^n (2j+1) = (2j+1)(n-j)\)</span> FLOPs, and then we sum over the outer loop to get <span class="math inline">\(\sum_{j=1}^n(2j+1)(n-j) = \frac16 n(2n^2 +3n-5) = \frac13n^3 + \frac12 n^2 - \frac56 n\)</span> FLOPs.</p>
<p>Adding the total FLOPs over both loops gives us <span class="math inline">\(\frac23 n^3 + \frac52 n^2 + 5/6 n\)</span> FLOPs, where the leading term is the cubic term so we just say that LU decomposition is <span class="math inline">\(\mathcal{O}(n^3)\)</span> or “cubic” or “third-order” in cost. Again, we emphasize that this is <em>large-<span class="math inline">\(n\)</span></em> property – at small <span class="math inline">\(n\)</span> the other terms of the polynomial expression may have more of an influence, but as <span class="math inline">\(n\to\infty\)</span> you will see the leading term dominate the cost.</p>
<p><strong>Memory complexity analysis</strong></p>
<p>Memory complexity is a little bit simpler — we just want to look at what quantities need to be stored in RAM to do our computation: for this we can just look at the variables that appear in the algorithm, which are <span class="math inline">\(L, U, X\in\mathbb{R}^{n\times n}\)</span> along with <span class="math inline">\(s_1,s_2\in\mathbb{R}\)</span>. The latter are just two scalars, so the cost is dominated by the cost of storing the three matrices <span class="math inline">\(L,U,X\)</span>, and thus the memory complexity is <span class="math inline">\(\mathcal{O}(n^2)\)</span>.</p>
</section>
<section id="qr-decomposition" class="level4" data-number="4.2.2.2">
<h4 data-number="4.2.2.2" class="anchored" data-anchor-id="qr-decomposition"><span class="header-section-number">4.2.2.2</span> QR decomposition</h4>
<p>Here’s some pseudocode for the QR decomposition using a method called the “modified Gram-Schmidt” procedure:</p>
<p><strong>QR decomposition of <span class="math inline">\(X\in\mathbb{R}^{N\times n}\)</span> without pivoting: pseudocode</strong></p>
<ol type="1">
<li>Initialize <span class="math inline">\(V = X\)</span>, <span class="math inline">\(R=0_{n\times n}\)</span>, <span class="math inline">\(Q = 0_{N\times n}\)</span>.</li>
<li><code>for</code> <span class="math inline">\(i = 1,\ldots, n\)</span>:
<ul>
<li><span class="math inline">\(r_{ii} = \|v_{:,i}\|\)</span></li>
<li><span class="math inline">\(q_{:,i} = v_{:,i}/r_{ii}\)</span></li>
<li><code>for</code> <span class="math inline">\(j=i+1,\ldots,n\)</span>
<ul>
<li><span class="math inline">\(r_{ij} = q_{:,i}^\top v_{:,j}\)</span></li>
<li><span class="math inline">\(v_{:,j} = v_{:,j} - r_{ij}q_{:,i}\)</span></li>
</ul></li>
</ul></li>
</ol>
<p>where <span class="math inline">\(v_{:,i}\)</span> and <span class="math inline">\(q_{:,i}\)</span> denote the <span class="math inline">\(i\)</span>th column vectors of <span class="math inline">\(V\)</span> and <span class="math inline">\(Q\)</span>, respectively.</p>
<p>The QR decomposition has time complexity <span class="math inline">\(\mathcal{O}(N\cdot n^2)\)</span> and storage complexity <span class="math inline">\(\mathcal{O}(Nn)\)</span>. For square matrices where <span class="math inline">\(N=n\)</span>, the complexity is <span class="math inline">\(\mathcal{O}(N^3)\)</span>. In general, computing matrix decompositions will have third-order complexity: we will see another example when we get to the SVD in the next chapter.</p>
</section>
</section>
</section>
<section id="solving-linear-systems-using-matrix-decompositions" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="solving-linear-systems-using-matrix-decompositions"><span class="header-section-number">4.3</span> Solving linear systems using matrix decompositions</h2>
<p>Finally we consider the overall cost of solving linear systems of equations using matrix decompositions. In general, the best practice when you are coding something is to use functions that are part of standard linear algebra packages to solve these systems. In python, both the <tt>numpy</tt> and <tt>scipy</tt> libraries have <tt>linalg</tt> packages that contain various useful linear algebra functions. Both are based on the LAPACK library, which efficiently implements many linear algebra computations in Fortran. Popular deep learning packages like PyTorch and TensorFlow have their own linear algebra packages that build on other libraries that leverage GPU computing capabilities. The details in this section explain what happens under the hood in these linear algebra packages: LU decomposition is used to solve invertible linear systems and QR decomposition is used to solve least squares problems where the data matrix has full column rank.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this section, we are assuming that <span class="math inline">\(X\)</span> has full column rank (which guarantees invertibility if <span class="math inline">\(X\)</span> is square), whereas previously we did not make this assumption.</p>
</div>
</div>
<section id="invertible-linear-systems" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="invertible-linear-systems"><span class="header-section-number">4.3.1</span> Invertible linear systems</h3>
<p>If <span class="math inline">\(X\)</span> is invertible (and therefore square), the python functions we use to solve the system <span class="math inline">\(X\beta=Y\)</span> are <tt>numpy.linalg.solve</tt> and <tt>scipy.linalg.solve</tt>. Both of these functions use LU decomposition to solve the system. This means they first compute the factors <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span> and described above. Then, once the LU factorization of <span class="math inline">\(X\)</span> is found, then we can solve for <span class="math inline">\(\beta\)</span> in two steps:</p>
<ol type="1">
<li><p>Solve <span class="math inline">\(L\alpha = Y\)</span> for <span class="math inline">\(\alpha\)</span>.</p></li>
<li><p>Solve <span class="math inline">\(U\beta = \alpha\)</span> for <span class="math inline">\(\beta\)</span>.</p></li>
</ol>
<p>This may look like we’ve just overcomplicated things by turning our one problem of solving <span class="math inline">\(X\beta = Y\)</span> into two problems, but the triangular structure of <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span> lets us solve these two new systems relatively efficiently. To see this, consider solving <span class="math inline">\(L\alpha = Y\)</span> for a three-dimensional example:</p>
<p><span class="math display">\[
L = \begin{pmatrix}1 &amp; &amp; \\ 2 &amp; 1 &amp; \\
2 &amp; 1 &amp; 2
\end{pmatrix}, \qquad Y = \begin{pmatrix}1 \\ 2 \\ 0\end{pmatrix}.
\]</span></p>
<p>Recall that this represents a system of three linear equations:</p>
<p><span class="math display">\[
\begin{aligned}
1 \alpha_1 &amp;             &amp; = 1\\
2 \alpha_1 &amp;+ 1 \alpha_2  &amp;= 2 \\
2 \alpha_1 &amp;+ 1 \alpha_2 + 2 \alpha_3 &amp; = 0
\end{aligned}
\]</span></p>
<p>We can immediately read off the solution of the first equation, <span class="math inline">\(\alpha_1 = 1\)</span> because there is only one unknown variable in it. From there we substitute that into the second equation to get <span class="math inline">\(2 + 1\alpha_2 = 2\)</span>, so we again one unknown variable, which we can easily solve to get <span class="math inline">\(\alpha_2 = 0\)</span>. From there we now substitute our values for <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\alpha_2\)</span> into the third equation, yielding <span class="math inline">\(2 + 2\alpha_3 = 0\)</span>, which again has one unknown variable, so we can easily solve to get <span class="math inline">\(\alpha_3 = -1\)</span>. This yields our overall solution <span class="math inline">\(\alpha = (1, 0, -1)^\top\)</span>.</p>
<p>The process we have just described is called <em>forward substitution</em> and is a standard algorithm for solving lower triangular systems. It generalizes easily to triangular systems of arbitrary dimension <span class="math inline">\(n\times n\)</span> — we just keep substituting in the results of previous rows into the next row, as follows:</p>
<p><strong>Forward substitution: pseudocode</strong></p>
<ol type="1">
<li><span class="math inline">\(\alpha_1 = Y_1 / L_{11}\)</span>.</li>
<li><code>for</code> <span class="math inline">\(i\)</span> = 2 to <span class="math inline">\(n\)</span>:
<ul>
<li><span class="math inline">\(s = 0\)</span></li>
<li><code>for</code> <span class="math inline">\(j\)</span> = 1 to <span class="math inline">\((i - 1)\)</span>:
<ul>
<li><span class="math inline">\(s = s + L_{ij}\alpha_j\)</span></li>
</ul></li>
<li><span class="math inline">\(\alpha_i = (Y_i - s) / L_{ii}\)</span></li>
</ul></li>
<li><code>return</code> <span class="math inline">\(\alpha\)</span></li>
</ol>
<p>Forward substitution has complexity <span class="math inline">\(\mathcal{O}(n^2)\)</span>.</p>
<p>Now that we have solved <span class="math inline">\(L\alpha = Y\)</span> for <span class="math inline">\(\alpha\)</span> using forward substitution, we need to solve <span class="math inline">\(U\beta = \alpha\)</span> for <span class="math inline">\(\beta\)</span>, where <span class="math inline">\(U\)</span> is upper triangular rather than lower triangular. This is done using <em>backward substitution</em>, which is a very similar algorithm to forward substitution and also has complexity <span class="math inline">\(\mathcal{O}(n^2)\)</span>. That gives us our final solution <span class="math inline">\(\beta\)</span>.</p>
<p>The overall computational cost of solving an invertible linear system is dominated by the cubic cost of the LU decomposition (for large <span class="math inline">\(n\)</span>, the quadratic costs of forward and backward substitution are much cheaper than the cubic cost of LU). Thus, we say linear solves have cubic complexity.</p>
</section>
<section id="least-squares-problems" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="least-squares-problems"><span class="header-section-number">4.3.2</span> Least squares problems</h3>
<p>If <span class="math inline">\(X\)</span> is non-invertible, the python functions we use to solve <span class="math inline">\(X\beta = Y\)</span> are <tt>numpy.linalg.lstsq</tt> and <tt>scipy.linalg.lstsq</tt>. These functions can use a number of different algorithms to solve the least squares problem, but a common choice for overdetermined least squares problems is to use QR decomposition. That is, we first compute the factors <span class="math inline">\(Q\in\mathbb{R}^{N\times n}\)</span> and <span class="math inline">\(R\in\mathbb{R}^{n\times n}\)</span> as described above. Then, we can solve for <span class="math inline">\(\beta\)</span> is two steps:</p>
<!-- Here's a reminder of the essential facts of QR decomposition: if $X\in\R^{N\times n}$ has full column rank, then it can be written as the product of an orthogonal matrix $Q\in\R^{N\times n}$ with an upper triangular matrix $R\in\R^{n\times n}$. Analogous to solving invertible systems, once we have factored $X=QR$ we can then solve the least squares problem in two steps: -->
<ol type="1">
<li><p>Solve <span class="math inline">\(Q\alpha = Y\)</span> for <span class="math inline">\(\alpha\)</span>.</p></li>
<li><p>Solve <span class="math inline">\(R\beta = \alpha\)</span> for <span class="math inline">\(\beta\)</span>.</p></li>
</ol>
<p>Recall that orthogonal matrices <span class="math inline">\(Q\)</span> satisfy <span class="math inline">\(Q^\top Q=I\)</span>, so this special structure lets us find <span class="math inline">\(\alpha\)</span> via a matrix multiplication, which is computationally more efficient than a full LU decomposition. Then, since <span class="math inline">\(R\)</span> is upper triangular, we can solve for <span class="math inline">\(\beta\)</span> using backward substitution.</p>
<p>If <span class="math inline">\(X\)</span> is does not have full column rank, there are a couple of options for solving the least squares problem, but the standard one is based on the the singular value decomposition, which deserves its own set of notes, which come next.</p>
</section>
</section>
<section id="exercises" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="exercises"><span class="header-section-number">4.4</span> Exercises</h2>
<ol type="1">
<li>Show that the QR decomposition has time complexity <span class="math inline">\(\mathcal{O}(N n^2)\)</span>.</li>
<li>Provide pseudocode for a backward substitution algorithm analogous to the forward substitution algorithm provided (without looking it up, which you could easily do — but if you can derive it yourself then you know you understand it).</li>
<li>Show that forward (or backward) substitution has complexity <span class="math inline">\(\mathcal{O}(n^2)\)</span>.</li>
<li>Show that the overall cost of solving <span class="math inline">\(X\beta=Y\)</span> via QR decomposition is <span class="math inline">\(\mathcal{O}(Nn^2)\)</span>.</li>
<li>Determine how much RAM you have on your personal computer. How many double precision floats can you store in that space? What does this mean for the largest square linear system you can solve using your computer? What are the dimensions of the largest least squares problem you can solve using the QR decomposition on your computer?</li>
<li>Write code to set up and solve least squares problems of arbitrary dimension and time the solve. Plot the solve times as the dimensions increase. Do you observe the expected trends? At what point does your computer run out of memory and is it approximately what you expected?</li>
</ol>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-strang2022introduction" class="csl-entry" role="listitem">
Strang, Gilbert. 2022. <em>Introduction to Linear Algebra</em>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>if you want more of a refresher, revisit your favorite linear algebra textbook or notes. My favorite is <span class="citation" data-cites="strang2022introduction">(<a href="references.html#ref-strang2022introduction" role="doc-biblioref">Strang 2022</a>)</span>. There’s also no substitute for your own notes that reflect how you think about things – if you have your own notes, go back and look at them now!<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./12_lslinalg.html" class="pagination-link" aria-label="LLS: A linear algebra perspective">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">LLS: A linear algebra perspective</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>