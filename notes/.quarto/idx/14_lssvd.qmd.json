{"title":"LLS and the singular value decomposition","markdown":{"yaml":{"title":"LLS and the singular value decomposition","html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"}},"headingText":"SVD: the basics","containsRefs":false,"markdown":"\n\n{{< include _macros.qmd >}}\n\nIn this section we will introduce a new matrix decomposition called the Singular Value Decomposition (SVD). The SVD is generally applicable to all matrices of any dimension or rank, and has many applications in machine learning and beyond. We will begin by defining the decomposition and then discuss some of it uses.\n\nThe intended learning outcomes of these notes are that students should be able to:\n\n\n\nThe SVD decomposes the matrix $X\\in\\R^{N\\times n}$ into the product of three matrices, $U\\in\\R^{N\\times N}$, $\\Sigma\\in\\R^{N \\times n}$, and $V^\\top \\in\\R^{n\\times n}$, so that \n\n$$\nX = U\\Sigma V^\\top\n$$ {#eq-svd-product}\n\nThe matrices $U$ and $V$ are orthogonal (unitary), so that $U^\\top U = I_N$ and $V^\\top V = I_n$, and the matrix $\\Sigma$ is diagonal. The diagonal entries of $\\Sigma$ are denoted $\\sigma_1,\\sigma_2,\\ldots,$ and are called the *singular values* of $X$, and are always non-negative and in non-increasing order, that is: $\\sigma_1\\geq \\sigma_2\\geq \\cdots \\geq 0$.  $\\Sigma$ is called the singular value matrix, and the number of nonzero singular values is equal to the rank of the matrix $X$. The matrix $U$ is called the left singular vector matrix, and its columns are called the *left singular vectors*. The matrix $V$ is called the right singular vector matrix, and its columns are called the *right singular vectors*. \n\n<!-- pseudocode in the future? -->\n\nTo compute the SVD in python, you would use <tt>numpy.linalg.svd</tt> or <tt>scipy.linalg.svd</tt>. Like other matrix decompositions, the time complexity to compute the SVD is third-order, specifically $\\mathcal{O}(\\min(Nn^2, N^2n))$. (Note that when $N>n$ this is the same time complexity as the QR decomposition for tall matrices.) The storage complexity is $\\mathcal{O}(\\max(N,n)^2)$.\n\nAbove, we have described the *full SVD*. Another version of the SVD is the *reduced SVD* or the *thin SVD*[^1]. \nLet $k = \\min(N,n)$. The reduced SVD decomposes $X$ into the product $X=U\\Sigma V^\\top$ where $U\\in\\R^{N\\times k}$, $\\Sigma\\in\\R^{k\\times k}$, and $V\\in\\R^{n\\times k}$. The matrices $U$ and $V$ are still unitary, so that $U^\\top U = I_k$ and $V^\\top V = I_k$ and $\\Sigma$ is square and diagonal. When one of the dimensions of $X$ is much larger than the other, this can lead to signficant cost savings in both time and memory. Note that the reduced SVD is subtly different from the truncated SVD which we describe next. \n\n[^1]: As with many of the terms we have encountered and will encounter in this course, the term \"SVD\" is used in the wild to mean either the full SVD or the reduced SVD -- often the distinction is not super important, but when it is you need to be able to discern from context what is meant.\n\n<!-- add eigenvalue decomposition of square product? -->\n\n<!-- add the operator interpretation? rotation/stretching/scaling -->\n\n## The truncated SVD\nLet $k=\\min(N,n)$ as before. Then, we can rewrite @eq-svd-product as follows:\n\n$$\nX = \\sum_{i=1}^k u_i \\sigma_i v_i^\\top = \\sum_{i=1}^k \\sigma_i (u_i v_i^\\top)\n$$ {#eq-svd-sum}\n\nwhere $u_1,\\ldots,u_N\\in\\R^N$ are the left singular vectors (columns of $U$) and $v_1,\\ldots,v_n\\in\\R^n$ are the right singular vectors (columns of $V$), and $\\sigma_i$ are again the diagonal entries in $\\Sigma$. The matrices $u_1v_1^\\top,\\ldots,u_kv_k^\\top\\in\\R^{N\\times n}$ are rank-1 matrices formed by taking the *outer product* of $u_i$ with $v_i$. This notation shows that $X$ can be expressed as the sum of $k$ rank-1 matrices, so we know that $X$ has rank at most $k$. In fact, the number of nonzero singular values is equal to the rank of $X$. \n\n\nPCA features, data compression\n\n## the pseudo inverse and solving least squares problems\nplus truncated SVD regularization?\n\n## utility in understanding numerical conditioning\n\nnext ste of notes: machine precision, round off error, SVD and conditioning.\n\n\npython examples showing nonzero residual for LU and conditioning error for QR?\n\n\nintroduce the SVD in math, singular values vs rank/conditioning, explain the concept of conditioning in terms of being essentially non-invertible from the computer's point of view, cond/rank commands\n\nshow that normal equations are terribly conditioned.\n\n## Exercises \n\n1. ","srcMarkdownNoYaml":"\n\n{{< include _macros.qmd >}}\n\nIn this section we will introduce a new matrix decomposition called the Singular Value Decomposition (SVD). The SVD is generally applicable to all matrices of any dimension or rank, and has many applications in machine learning and beyond. We will begin by defining the decomposition and then discuss some of it uses.\n\nThe intended learning outcomes of these notes are that students should be able to:\n\n\n## SVD: the basics\n\nThe SVD decomposes the matrix $X\\in\\R^{N\\times n}$ into the product of three matrices, $U\\in\\R^{N\\times N}$, $\\Sigma\\in\\R^{N \\times n}$, and $V^\\top \\in\\R^{n\\times n}$, so that \n\n$$\nX = U\\Sigma V^\\top\n$$ {#eq-svd-product}\n\nThe matrices $U$ and $V$ are orthogonal (unitary), so that $U^\\top U = I_N$ and $V^\\top V = I_n$, and the matrix $\\Sigma$ is diagonal. The diagonal entries of $\\Sigma$ are denoted $\\sigma_1,\\sigma_2,\\ldots,$ and are called the *singular values* of $X$, and are always non-negative and in non-increasing order, that is: $\\sigma_1\\geq \\sigma_2\\geq \\cdots \\geq 0$.  $\\Sigma$ is called the singular value matrix, and the number of nonzero singular values is equal to the rank of the matrix $X$. The matrix $U$ is called the left singular vector matrix, and its columns are called the *left singular vectors*. The matrix $V$ is called the right singular vector matrix, and its columns are called the *right singular vectors*. \n\n<!-- pseudocode in the future? -->\n\nTo compute the SVD in python, you would use <tt>numpy.linalg.svd</tt> or <tt>scipy.linalg.svd</tt>. Like other matrix decompositions, the time complexity to compute the SVD is third-order, specifically $\\mathcal{O}(\\min(Nn^2, N^2n))$. (Note that when $N>n$ this is the same time complexity as the QR decomposition for tall matrices.) The storage complexity is $\\mathcal{O}(\\max(N,n)^2)$.\n\nAbove, we have described the *full SVD*. Another version of the SVD is the *reduced SVD* or the *thin SVD*[^1]. \nLet $k = \\min(N,n)$. The reduced SVD decomposes $X$ into the product $X=U\\Sigma V^\\top$ where $U\\in\\R^{N\\times k}$, $\\Sigma\\in\\R^{k\\times k}$, and $V\\in\\R^{n\\times k}$. The matrices $U$ and $V$ are still unitary, so that $U^\\top U = I_k$ and $V^\\top V = I_k$ and $\\Sigma$ is square and diagonal. When one of the dimensions of $X$ is much larger than the other, this can lead to signficant cost savings in both time and memory. Note that the reduced SVD is subtly different from the truncated SVD which we describe next. \n\n[^1]: As with many of the terms we have encountered and will encounter in this course, the term \"SVD\" is used in the wild to mean either the full SVD or the reduced SVD -- often the distinction is not super important, but when it is you need to be able to discern from context what is meant.\n\n<!-- add eigenvalue decomposition of square product? -->\n\n<!-- add the operator interpretation? rotation/stretching/scaling -->\n\n## The truncated SVD\nLet $k=\\min(N,n)$ as before. Then, we can rewrite @eq-svd-product as follows:\n\n$$\nX = \\sum_{i=1}^k u_i \\sigma_i v_i^\\top = \\sum_{i=1}^k \\sigma_i (u_i v_i^\\top)\n$$ {#eq-svd-sum}\n\nwhere $u_1,\\ldots,u_N\\in\\R^N$ are the left singular vectors (columns of $U$) and $v_1,\\ldots,v_n\\in\\R^n$ are the right singular vectors (columns of $V$), and $\\sigma_i$ are again the diagonal entries in $\\Sigma$. The matrices $u_1v_1^\\top,\\ldots,u_kv_k^\\top\\in\\R^{N\\times n}$ are rank-1 matrices formed by taking the *outer product* of $u_i$ with $v_i$. This notation shows that $X$ can be expressed as the sum of $k$ rank-1 matrices, so we know that $X$ has rank at most $k$. In fact, the number of nonzero singular values is equal to the rank of $X$. \n\n\nPCA features, data compression\n\n## the pseudo inverse and solving least squares problems\nplus truncated SVD regularization?\n\n## utility in understanding numerical conditioning\n\nnext ste of notes: machine precision, round off error, SVD and conditioning.\n\n\npython examples showing nonzero residual for LU and conditioning error for QR?\n\n\nintroduce the SVD in math, singular values vs rank/conditioning, explain the concept of conditioning in terms of being essentially non-invertible from the computer's point of view, cond/rank commands\n\nshow that normal equations are terribly conditioned.\n\n## Exercises \n\n1. "},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","number-sections":true,"html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"},"output-file":"14_lssvd.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.554","bibliography":["references.bib"],"theme":"cosmo","title":"LLS and the singular value decomposition"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}