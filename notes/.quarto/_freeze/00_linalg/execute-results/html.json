{
  "hash": "44223c5c1f2818306258086af59fdd3b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Linear algebra review\"\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nnumber-sections: true\nformat: \n    html: default\nexecute:\n    echo: true\n    freeze: auto\n---\n\n::: {.hidden}\n\n\\newcommand*{\\R}{\\mathbb{R}}\n\\newcommand*{\\N}{\\mathbb{N}}\n\\newcommand*{\\cF}{\\mathcal{F}}\n\n\\newcommand{\\E}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\Var}[1]{\\text{Var}\\left[#1\\right]}\n\\newcommand{\\Corr}[2]{\\text{Corr}\\left[#1, #2\\right]}\n\\newcommand{\\Cov}[2]{\\text{Cov}\\left[#1, #2\\right]}\n\\newcommand{\\Tr}{\\text{Tr}}\n\n\\newcommand{\\bA}{\\mathbf{A}}\n\\newcommand{\\bB}{\\mathbf{B}}\n\\newcommand{\\bC}{\\mathbf{C}}\n\\newcommand{\\bD}{\\mathbf{D}}\n\\newcommand{\\bE}{\\mathbf{E}}\n\\newcommand{\\bF}{\\mathbf{F}}\n\\newcommand{\\bG}{\\mathbf{G}}\n\\newcommand{\\bH}{\\mathbf{H}}\n\\newcommand{\\bI}{\\mathbf{I}}\n\\newcommand{\\bJ}{\\mathbf{J}}\n\\newcommand{\\bK}{\\mathbf{K}}\n\\newcommand{\\bL}{\\mathbf{L}}\n\\newcommand{\\bM}{\\mathbf{M}}\n\\newcommand{\\bN}{\\mathbf{N}}\n\\newcommand{\\bO}{\\mathbf{O}}\n\\newcommand{\\bP}{\\mathbf{P}}\n\\newcommand{\\bQ}{\\mathbf{Q}}\n\\newcommand{\\bR}{\\mathbf{R}}\n\\newcommand{\\bS}{\\mathbf{S}}\n\\newcommand{\\bT}{\\mathbf{T}}\n\\newcommand{\\bU}{\\mathbf{U}}\n\\newcommand{\\bV}{\\mathbf{V}}\n\\newcommand{\\bW}{\\mathbf{W}}\n\\newcommand{\\bX}{\\mathbf{X}}\n\\newcommand{\\bY}{\\mathbf{Y}}\n\\newcommand{\\bZ}{\\mathbf{Z}}\n\n\\newcommand{\\ba}{\\mathbf{a}}\n\\newcommand{\\bb}{\\mathbf{b}}\n\\newcommand{\\bc}{\\mathbf{c}}\n\\newcommand{\\bd}{\\mathbf{d}}\n\\newcommand{\\be}{\\mathbf{e}}\n\\newcommand{\\bef}{\\mathbf{f}}\n\\newcommand{\\bg}{\\mathbf{g}}\n\\newcommand{\\bh}{\\mathbf{h}}\n\\newcommand{\\bi}{\\mathbf{i}}\n\\newcommand{\\bj}{\\mathbf{j}}\n\\newcommand{\\bk}{\\mathbf{k}}\n\\newcommand{\\bl}{\\mathbf{l}}\n\\newcommand{\\bem}{\\mathbf{m}}\n\\newcommand{\\bn}{\\mathbf{n}}\n\\newcommand{\\bo}{\\mathbf{o}}\n\\newcommand{\\bp}{\\mathbf{p}}\n\\newcommand{\\bq}{\\mathbf{q}}\n\\newcommand{\\br}{\\mathbf{r}}\n\\newcommand{\\bs}{\\mathbf{s}}\n\\newcommand{\\bt}{\\mathbf{t}}\n\\newcommand{\\bu}{\\mathbf{u}}\n\\newcommand{\\bv}{\\mathbf{v}}\n\\newcommand{\\bw}{\\mathbf{w}}\n\\newcommand{\\bx}{\\mathbf{x}}\n\\newcommand{\\by}{\\mathbf{y}}\n\\newcommand{\\bz}{\\mathbf{z}}\n\n<!-- \\renewcommand{\\P}{\\mathrm{P}} -->\n\n<!-- $$\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n$$\n\n\\definecolor{quarto-callout-note-color}{HTML}{4477AA} -->\n\n:::\n\n\n\nBefore we dive into the world of machine learning and optimization, it's important that we get comfortable with the fundamental building blocks of mathematics in higher dimensions: Linear Algebra. This course assumes you have seen basic concepts in Linear Algebra, but we'll provide a refresher to get everyone on the same page. \n\n## Scalars, Vectors, and Matrices {#sec-linalg}\n\n### Notation \n\nLet's introduce some notation: $\\mathbb{R}$ is the set of real numbers, $\\mathbb{C}$ is the set of complex numbers, and the operator $\\in$ designates that a variable *belongs* to a set. To define a real scalar (i.e. just a number), $c$, we write: \n\n$$ c \\in \\mathbb{R}$$ \n\nYou can read this as \"the variable $c$ belongs to the set of 1-dimensional real numbers.\" \n\nTo define a real $n$ dimensional vector, $\\mathbf{x}$, we write: \n\n$$ \\mathbf{x}  = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\in \\mathbb{R}^n $$ \n\n\nTo define a $m \\times n$ matrix $\\mathbf{A}$ of real-valued entries, we write: \n\n$$ \\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ \na_{21} & a_{22} & & a_{2n} \\\\ \n\\vdots & & \\ddots & \\vdots \\\\ \na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n }\n$$ \n\n### The Transpose\n\nThe \"transpose\" of a matrix or vector, swaps the rows and columns. In other words, the first row becomes the first column, the second row the second column and so-on: \n\n$$ \\mathbf{x}^\\top = \\begin{bmatrix} x_1 & \\dots & x_n \\end{bmatrix} \\in \\mathbb{R}^{1 \\times n} $$ \n\n \n\n$$ \\mathbf{A}^\\top = \\begin{bmatrix} a_{11} & a_{21} & \\dots & a_{m1} \\\\ \na_{12} & a_{22} & & a_{m2} \\\\ \n\\vdots & & \\ddots & \\vdots \\\\ \na_{1n} & a_{2n} & \\dots & a_{mn}\n\\end{bmatrix} \\in \\mathbb{R}^{n \\times m } $$\n\nTransposing the product of matrices reverses their order: \n\n$$ \\left( \\mathbf{A} \\mathbf{B} \\mathbf{C} \\mathbf{D} \\right)^\\top = \\mathbf{D}^\\top \\mathbf{C}^\\top \\mathbf{B}^\\top \\mathbf{A}^\\top $$ \n\n### Addition & subtraction \n\nTechnically, only objects of the same dimensionality can be added or subtracted with one another. So, scalars can only be added to scalars, vectors can only be added to vectors of the same dimension, and matrices can only be added to other matrices with the same numbers of rows and columns. Some software like MATLAB or NumPy might let you add scalars to vectors and matrices and under the hood they multiply that scalar by a vector/matrix of ones to make the dimensions match. \n\n::: {.callout-caution collapse=\"true\"}\n## Examples of `numpy` allowing mathematically invalid operations \nMathematically, none of the following are valid operations. However, `numpy` allows us to do them by secretly adding in a vector of ones so that the vector/matrix dimensions are valid. This can be confusing for new programmers. \n\n*Adding a scalar to a vector* \n\n::: {#de8dafbb .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np \n# Defining a vector \nu = np.array([\n  1, 2, 3\n])\n\n# Adding a scalar to a vector\nprint(u + 5)\n# What numpy does to make the dimensions match\nprint(u + 5*np.ones_like(u)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[6 7 8]\n[6 7 8]\n```\n:::\n:::\n\n\n*Adding a row-vector to a column-vector*\n\n::: {#d32be7ee .cell execution_count=2}\n``` {.python .cell-code}\n# Reshaping u to a column-vector \nu = u.reshape((3,1))\n\n# Defining a row-vector, v \nv = np.array([\n  4, 5, 6\n]).reshape((1,3)) \n\n# Adding the row-vector v to a column-vector u\nprint(v + u)\n# What numpy does to make the dimensions match\nprint(np.outer(u, np.ones(3)) + np.outer(np.ones(3), v)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[5 6 7]\n [6 7 8]\n [7 8 9]]\n[[5. 6. 7.]\n [6. 7. 8.]\n [7. 8. 9.]]\n```\n:::\n:::\n\n\n:::\n\n## Scalar Multiplication \n\nGenerally, scalars can multiply by any structure (scalar, vector, and matrix) and do not change its dimension. They only \"scale\" its value by a certain amount. Hence, multiplying a scalar by a scalar returns a scalar, multiplying a scalar times a vector returns a vector, and multiplying a scalar by a matrix returns a matrix. Scalar multiplication is also commutative, i.e. $c\\mathbf{A} = \\mathbf{A}c$ for all scalars $c$. Consider our scalar, $c$, from the previous section, another scalar, $b$, the vector $\\mathbf{x} \\in \\mathbb{R}^n$ and the matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ \n\n### Scalar-Scalar Product \n\n$$ b \\times c = bc \\in \\mathbb{R} $$\n\n### Scalar-Vector Product \n\n$$ c \\mathbf{x} = \\mathbf{x} c = \\begin{bmatrix} c x_1 \\\\ \\vdots \\\\ c x_n \\end{bmatrix} \\in \\mathbb{R}^n $$\n\n### Scalar-Matrix Product \n\n$$ c \\mathbf{A} = \\mathbf{A} c = \\begin{bmatrix} c a_{11} & c a_{12} & \\dots & c a_{1n} \\\\ \nc a_{21} & c a_{22} & & c a_{2n} \\\\ \n\\vdots & & \\ddots & \\vdots \\\\ \nc a_{m1} & c a_{m2} & \\dots & c a_{mn}\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n } $$ \n\n## Matrix & Vector Multiplication \n\n### Vector-Vector Multiplication\n\nTo multiply two matrices (or two vectors, or a matrix with a vector), their inner dimensions must always match. Hence, the only valid way to multiply two vectors is by \"inner\" or \"outer\" products. Consider two vectors, $\\mathbf{u} \\in \\mathbb{R}^n$ and $x \\in \\mathbb{R}^n$. An \"**Inner Product**\" (sometimes called the Dot-Product) is defined as: \n\n$$ \\mathbf{u}^\\top \\mathbf{x} = \\begin{bmatrix} u_1 & \\dots & u_n \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = u_1 x_1 + u_2 x_2 + \\dots u_n x_n \\in \\mathbb{R}$$ \n\nSome important notes about inner products: \n\n* If the dimensions of $\\mathbf{u}$ and $\\mathbf{x}$ do not match, we cannot multiply them this way as we need to multiply each entry of both vectors. \n* This operation returns a scalar value. \n* Inner Products are commutative, i.e. $\\mathbf{u}^\\top \\mathbf{x} = \\mathbf{x}^\\top \\mathbf{u}$, as they are sums of scalar-scalar multiplications which are also commutative.\n\nRecall $\\mathbf{x} \\in \\mathbb{R}^n$. Now consider a new vector $\\mathbf{w} \\in \\mathbb{R}^m$ where $m \\neq n$. An \"**Outer Product**\" is defined as: \n\n$$ \\mathbf{w} \\mathbf{x}^\\top = \\begin{bmatrix} w_1 \\\\ \\vdots \\\\ w_m \\end{bmatrix} \\mathbf{x}^\\top = \\begin{bmatrix} w_1 \\mathbf{x}^\\top  \\\\ \\vdots \\\\ w_m \\mathbf{x}^\\top  \\end{bmatrix} = \\begin{bmatrix} w_1 x_1 & w_1 x_2 & \\dots & w_1 x_n \\\\ w_2 x_1 & w_2 x_2 & & w_2 x_n \\\\ \\vdots & & \\ddots & \\vdots \\\\ w_m x_1 & w_m x_2 & \\dots & w_m x_n \\end{bmatrix} \\in \\mathbb{R}^{m \\times n} $$ \n\nImportant notes on outer products: \n\n* Outer products return *matrices* with dimensions according to the vectors multiplied. \n* As long as $\\mathbf{w}$ and $\\mathbf{x}$ are vectors, outer products share an inner dimension of $1$, which means that any two vectors have an outer product. \n* Lastly, this operation is *non-commutative*, meaning $\\mathbf{w} \\mathbf{x}^\\top \\neq \\mathbf{x} \\mathbf{w}^\\top$. Rather, these two are transposes of each other. \n\n### Matrix-Vector Multiplication \n\nMultiplying a matrix with a vector means the dimension of the vector must equal the number of columns of the matrix. Recall $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{x} \\in \\mathbb{R}^n$. Note that the columns of $\\mathbf{A}$ match the dimension of $\\mathbf{x}$, which are both size $n$. The product $\\mathbf{A}\\mathbf{x}$ can be written as: \n\n$$ \\mathbf{A}\\mathbf{x} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ \na_{21} & a_{22} & & a_{2n} \\\\ \n\\vdots & & \\ddots & \\vdots \\\\ \na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix} \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} a_{11} x_1 + a_{12} x_2 + \\dots + a_{1n} x_n \\\\  a_{21} x_1 + a_{22} x_2 + \\dots + a_{2n} x_n \\\\ \\vdots \\\\a_{m1} x_1 + a_{m2} x_2 + \\dots + a_{mn} x_n  \\end{bmatrix} \\in \\mathbb{R}^m $$ \n\nPerhaps a more useful way to visualize this is to break $\\mathbf{A}$ down into its constituent columns and show this as a sum of scalar-vector multiplications: \n\n$$ \\mathbf{A}\\mathbf{x} = \\begin{bmatrix} | & | &  & | \\\\ \n\\mathbf{a}_1 & \\mathbf{a}_2 & \\dots & \\mathbf{a}_n \\\\ \n| & | & & | \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} | \\\\ \\mathbf{a}_1 \\\\ | \\end{bmatrix} x_1 + \\begin{bmatrix} | \\\\ \\mathbf{a}_2 \\\\ | \\end{bmatrix} x_2 + \\dots + \\begin{bmatrix} | \\\\ \\mathbf{a}_n \\\\ | \\end{bmatrix} x_n  \\in \\mathbb{R}^m $$ \n\nwhere $\\mathbf{a}_i$ is the $i$th column of $\\mathbf{A}$. Note that matrix-vector multiplication forms a linear map from one dimensional vector-space to another. In this case, $\\mathbf{A}\\mathbf{x}$ maps a vector from $\\mathbb{R}^n$ to $\\mathbb{R}^m$. This linear transformation from one dimensionality to another is a core component of many machine learning algorithms. \n\n### Matrix-Matrix Multiplication \n\nConsider two matrices, $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{B} \\in \\mathbb{R}^{n \\times k}$, where $m \\neq k$. Again, the inner dimensions must match to multiply matrices. Because $\\mathbf{A}$ multiplied by $\\mathbf{B}$ have an inner-dimension of $n$, they can be multiplied. Similar to matrix-vector multiplication, we can visualize the product of $\\mathbf{A}$ and $\\mathbf{B}$ by breaking up the columns and rows of $\\mathbf{A}$ and $\\mathbf{B}$, respectively: \n\n$$ \\mathbf{A}\\mathbf{B} = \\begin{bmatrix} | & | &  & | \\\\ \n\\mathbf{a}_1 & \\mathbf{a}_2 & \\dots & \\mathbf{a}_n \\\\ \n| & | & & | \\end{bmatrix} \\begin{bmatrix} - & \\mathbf{b}_1 & - \\\\ - & \\mathbf{b}_2 & - \\\\ & \\vdots & \\\\  - & \\mathbf{b}_n & - \\end{bmatrix} \\in \\mathbb{R}^{m \\times k}$$ \n\nwhere $\\mathbf{a}_i$ is the $i$th column of $\\mathbf{A}$ and $\\mathbf{b}_i$ is the $i$th row of $\\mathbf{B}$. We can expand this expression by multiplying each column of $\\mathbf{A}$ with each row of $\\mathbf{B}$: \n\n$$ \\mathbf{A}\\mathbf{B} = \\begin{bmatrix} | \\\\ \\mathbf{a}_1 \\\\ | \\end{bmatrix} \\begin{bmatrix} - & \\mathbf{b}_1 & - \\end{bmatrix} +\\begin{bmatrix} | \\\\ \\mathbf{a}_2 \\\\ | \\end{bmatrix} \\begin{bmatrix} - & \\mathbf{b}_2 & - \\end{bmatrix} + \\dots + \\begin{bmatrix} | \\\\ \\mathbf{a}_n \\\\ | \\end{bmatrix} \\begin{bmatrix} - & \\mathbf{b}_n & - \\end{bmatrix}$$ \n\nAs we can see, this is a sum of outer products (vectors multiplied by transposed vectors). We know $\\mathbf{A}$ has $m$ rows and $\\mathbf{B}$ has $k$ columns, so each outer product will form a *matrix* of dimension $m \\times k$: \n\n$$ \\mathbf{A}\\mathbf{B} = \\sum_{i=1}^n \\mathbf{a}_i \\mathbf{b}_i \\in \\mathbb{R}^{m \\times k}$$ \n\n*Note:* matrix multiplication is generally non-commutative; The product $\\mathbf{A}\\mathbf{B}$ is valid, because the number of columns of $\\mathbf{A}$ matches the number of rows of $\\mathbf{B}$. However, the product $\\mathbf{B}\\mathbf{A}$ is not valid, because the number of columns of $\\mathbf{B}$ does not equal the number of rows of $\\mathbf{A}$. The product $\\mathbf{B}\\mathbf{A}$ can be multiplied if $\\mathbf{A}$ and $\\mathbf{B}$ are both square matrices of the same size. However, even if the dimensions do match, generally speaking, $\\mathbf{BA \\neq AB}$. \n\n## Vector Norms \n\nOftentimes, it is useful to know how \"big\" a vector is. And there are many different ways of computing this. The \"norm\" of an object is a measure of how \"large\" it is, according to some rule. Consider $\\mathbf{x} \\in \\mathbb{R}^n$. We'll start with the most common norm:  \n\n### The 2-Norm\n\nThe 2-norm of $x$ (which we will use most-often in this class) is defined as the squareroot of the squares of its entries: \n\n$$ ||\\mathbf{x}||_2 = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2} $$ \n\nThe reason the 2-norm is so useful is the fact that the square of the $l_2$ norm is an inner product of a vector with itself: \n\n$$ \\mathbf{x}^\\top \\mathbf{x} = x_1^2 + x_2^2 + \\dots + x_3^2 = ||\\mathbf{x}||_2^2 $$ \n\nThis helps us write a norm in terms of matrix-vector operations, which is extremely useful for optimization. The 2-norm is also quite important because it is the first $l_p$ norm with a continuous derivative, which cannot be said of the 1-norm since it uses absolute-value functions. \n\n### The $l_p$-Norm \n\nWe can take the definition of the 2-Norm and generalize it to describe a class of norms known as $l_p$ norms. The $l_p$ norm of a vector is defined by the following: \n\n$$ ||\\mathbf{x}||_p = \\left( \\sum_{i=1}^n |x_i|^p \\right)^{1/p} $$\n\n### The 1-Norm\n\nThe 1-norm of $\\mathbf{x}$ is simply the sum of the absolute-value of its entries: \n\n$$ ||\\mathbf{x}||_1 = |x_1| + |x_2| + \\dots + |x_n| $$ \n\n### The Infinity-Norm\n\nThe infinity-norm takes the limit as $p \\rightarrow \\infty$ and when this limit is solved, this norm simply becomes the entry of $x$ with the largest absolute value: \n\n$$ ||\\mathbf{x}||_\\infty = \\max \\{ |x_1|, |x_2|, \\dots, |x_n| \\} $$ \n\n## Rank, Range/Column-Space and Null-Spaces\n\n### Linear Combinations and Span \n\nMany of the following ideas hinge upon the idea of \"Linear Combinations\" of vectors. Consider two vectors $\\bu, \\bv \\in \\R^{d}$. A linear combination of $\\bu$ and $\\bv$ is the vector produced by the following: \n\n$$ \\bw = c_1 \\bu + c_2 \\bv $$ \n\nwhere $c_1$ and $c_2$ are scalars. We can generalize this idea by saying that all possible realizations of $\\bw$ live in the vector-space $\\mathcal{W}$ defined by the \"span\" by $\\bu$ and $\\bv$. This is denoted: \n\n$$ \\bw \\in \\mathcal{W} = \\text{Span}\\left( \\bu, \\bv \\right) $$ \n\nWe can read this in plain english as \"the vector $\\bw$ belongs to the vector-space $\\mathcal{W}$ which is defined by all possible linear combinations of $\\bu$ and $\\bv$\". \n\n\n\n### Linear-Independence \n\nThis is a fundamental idea in Linear Algebra. A set of vectors is said to be \"Linearly Independent\" if no vector in this set can be reconstructed with any of the others. In other words, no one vector can be reproduced by scaling or combining any of the others. For example, consider the following three vectors: \n\n$$ \\{ \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\} $$\n\nBecause each vector in this set points in its own unique direction, it is impossible to reconstruct one of these vectors as a linear combination of any others. This may be true with more complex vectors that aren't unit vectors along the dimensions of the vector-space. \n\n### Rank \n\nTechnically, there are two types of rank: Column Rank and Row-Rank. In this class, when we say \"rank\", we will be referring to the column rank. \n\nThe column rank of a matrix is simply its number of linearly independent column-vectors (the row-rank is the number of linearly independent row-vectors). Consider the following matrix: \n\n$$ \\mathbf{A} = \\begin{bmatrix} 1 & 2 & 1 & 4 \\\\ 0 & 1 & 1 & 2 \\\\ 0 & 0 & 2 & 0 \\end{bmatrix} $$ \n\nThe first-three columns of $\\mathbf{A}$ are linearly independent because they cannot be reproduced using linear combinations of the other columns. However, the last column is a multiple of column 2, which means it *can* be reproduced using a linear combination of columns. Hence, this matrix has a rank of 3. \n\n*Theorem:* No matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ where $m < n$ can be full-rank.  This is because only $m$ vectors can span $\\mathbb{R}^m$, and since there are more than $m$ columns, we must have some redundancy. \n\n### Range/Column-Space  \n\nThe Range, Column-Space or Image of a matrix, $\\mathbf{A}$, is the span of all possible combinations of its columns. In human words, it describes the space of $\\mathbf{Ax}$ for all possible $\\mathbf{x}$. Consider the following matrix: \n\n$$ \\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\\\ 2 & 4  \\end{bmatrix} $$ \n\nNote that the second column is a multiple of the first column, hence, the range of $\\mathbf{A}$ is: \n\n$$ \\text{Range}(\\mathbf{A}) = \\text{span} \\left( \\begin{bmatrix} 1 \\\\ 3 \\\\ 2 \\end{bmatrix}\\right) $$ \n\n### Null-Space  \n\nThe Null-Space of a matrix, $\\mathbf{A}$ is the set of all nonzero vectors, $\\mathbf{x}$ such that $\\mathbf{Ax=0}$. Consider a simple example: \n\n$$ \\mathbf{A} = \\begin{bmatrix} 1 & 0 & -1\\\\ 0 & 1 & 1   \\end{bmatrix} $$ \n\nWe can find the set of $\\mathbf{x}$ that produce the zero-vector by solving the following equation: \n\n$$ \\begin{bmatrix} 1 & 0 & -1\\\\ 0 & 1 & 1   \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} $$ \n\nThis produces the systems of equations: \n\n$$ x_1 - x_3 = 0 $$ \n$$ x_2 + x_3 = 0 $$ \n$$ x_3 = x_3 $$ \n\nIf we isolate each entry of $\\mathbf{x}$: \n\n$$ x_1 = x_3 $$ \n$$ x_2 = -x_3 $$ \n$$ x_3 = x_3 $$ \n\nHence, we see that $x_3$ can be any scalar and satisfy this systems of equations: \n\n$$ \\mathbf{x} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\end{bmatrix} x_3 = \\text{span}\\left( \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\end{bmatrix} \\right) = \\text{Null}(\\mathbf{A})$$ \n\n\n### The Identity Matrix \n\nThe identity matrix is a square, $n$-dimensional matrix consisting of ones on its diagonal and zeros elsewhere. It is called the identity matrix because it preserves the identity when multiplied by matrices and vectors. \n\n$$ \\mathbf{I} = \\begin{bmatrix} 1 & 0 & \\dots & 0 \\\\ 0 & 1  & & 0 \\\\ \\vdots & & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & 1 \\end{bmatrix} $$ \n\nConsider $\\mathbf{I} \\in \\mathbb{R}^{n \\times n}$ to be an $n \\times n$ identity matrix. If we recall $\\mathbf{A} \\in \\mathbb{m \\times n}$ and $\\mathbf{x} \\in \\mathbb{R}^n$: \n\n$$ \\mathbf{A I = I A = A} $$ \n\n$$ \\mathbf{I x = x } $$\n\n$$ \\mathbf{x^\\top I = x^\\top} $$ \n\n:::{.callout-note}\n## Why is the identity matrix useful?\n\nThe identity matrix simplifies expressions involving matrices. If we can transform a term into an identity matrix, then we can reduce the expression to only the remaining terms. \n::: \n\n### Matrix Inverses \n\nYou may have noticed that, thus far, we have covered the addition, subtraction, and multiplication of matrices, which conspicuously leaves division. Well, unfortunately, matrix division is complex to say the least. For square matrices, division is accomplished via matrix inversion. However, not all matrices are invertible. \n\n*Definition:* A matrix $\\mathbf{B}$ is called the \"inverse\" of a square matrix $\\mathbf{A}$ if and only if the following condition holds:\n\n$$ \\mathbf{A} \\mathbf{B} = \\mathbf{B} \\mathbf{A} = \\mathbf{I} $$ \n\nwhere $\\mathbf{I}$ is the identity matrix. Such a $\\mathbf{B}$ is denoted $\\mathbf{A}^{-1}$. \n\n## Further Reading \n\n* A great read for a deeper dive into the relationship between Column, Row and Null-spaces of matrices: [Gil Strang's Fundamental Theorem of Linear Algebra](https://home.engineering.iastate.edu/~julied/classes/CE570/Notes/strangpaper.pdf){.external target=\"_blank\"}\n\n",
    "supporting": [
      "00_linalg_files"
    ],
    "filters": [],
    "includes": {}
  }
}