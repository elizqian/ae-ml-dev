{
  "hash": "7f30e7cde2d679372aa7c687883afb55",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Unconstrained Gradient Based Optimization (UGBO)\"\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nnumber-sections: true\nformat: \n    html: default\nexecute:\n    echo: true\n    freeze: auto\n---\n\n# Problem Motivation \nIn this lecture, we will be focusing on the following optimization problem,\n\n$$\\min_x f(x)$$\n\nwhere $f: \\mathbb{R}^n \\to \\mathbb{R}$ is our *objective function* (or loss function), and $x\\in\\mathbb{R}^n$ is a vector of parameters (variables) which we can change. Minimization problems arise in numerous physical and real-world circumstances, including:\n\n* adjusting parameters to optimize performance in engeneering design\n* finding the minimum state of energy in a system\n* minimizing a loss function when training a neural network\n\nFor this lecture, our approach for solving the above minimization problem will be a *gradient-based* one. This means that we will make use of information from the gradient of $f$ (first-order information), and the curvature of $f$ (second-order information).\n\n# Solutions, Gradient, and Curvature\n## What are we looking for?\n\nTo start, we first need to motivate some fundamental definitions regarding the nature of the solution we are looking for.\n\n**Definition:** We say that a point $x^\\star$ is a *global minimizer* of $f$ if:\n$$f(x^\\star)\\leq f(x) \\text{ for all }x\\in \\mathcal{D}\\subseteq\\mathbb{R}^n.$$ \n\nAlthough a global minimizer is the most desireable outcome from our minimization problem, finding a global minimizer is often quite challenging. This is because a global minimizer may not always exist, and because we often only possess *local* information about the function $f$. This motivates the following defintion:\n\n**Definition:** We say that a point $x^\\star$ is a *local minimizer* of $f$ if there exists some open set $\\mathcal{B}$, that contains $x^\\star$, such that:\n$$\nf(x^\\star)\\leq f(x) \\text{ for all }x\\in\\mathcal{B}.\n$$\nIf the inequality becomes strict, then $x^\\star$ is referred to as a *strict* local minimizer. To illustrate the above definitions, let us look at the function $f(x) = 2 + \\cos(x) + \\frac{1}{2} \\cos\\left(5x - \\frac{1}{2}\\right)$ over the domain $\\mathcal{D} = [0, 3.7]$. In this domain, the function has two local minimizers and one global minimizer, as illustrated in the plot below.\n\n::: {#338c6c23 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](20_ugbo_files/figure-html/cell-2-output-1.png){width=757 height=442}\n:::\n:::\n\n\nWe now have a way to understand and characterize our solutions. However, the above definitions suggest that we need to examine all points that are next to $x^\\star$ in order to properly characterize it. However, if our function is *smooth*, i.e., its higher order derivatives exist and are continuous, then we can classify $x^\\star$ by examining the derivatives of $f$.\n\n## Gradient\nRecall that the *gradient* of a function $f(x)$, denoted $\\nabla f(x)$, is the column vector of partial derivaties with respect to each parameter:\n$$\n\\nabla f(x) = \n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_1}\\\\\n\\vdots\\\\\n\\frac{\\partial f}{\\partial x_i}\\\\\n\\vdots\\\\\n\\frac{\\partial f}{\\partial x_n}\n\\end{bmatrix}\n$$\nEach element of $\\nabla f(x)$ captures the rate of change of the function $f$ with respect to the parameter $x_i$. The gradient of a function helps us identify *local* minimizers and allows us to state (without proof) the first fundamental result of gradient-based optimization.\n\n**Theorem (first-order necessary conditions):** If $x^\\star$ is a local minimizer and $f$ is continuously differentiable in an open nieghborhood of $x^\\star$, then $\\nabla f(x^\\star) = 0$.\n\nA point $x^\\star$ is called a *stationary point* if $\\nabla f(x^\\star) = 0$. The above theorem guarantees that all local minimizers must be stationary points. However, the theorem cannot guarantee that all stationary points are local minimizers. This is because the conditions are necessary but not sufficient. As a counterexample, a stationary point may be a local maximizer instead of a local minimizer. In order to obtain necessary and suffient conditions, we need to move one derivative higher.\n\n## Curvature\nThe *curvature* of a function $f$ is the rate of change of the gradient, and tells us whether the slope is stationary, increasing, or decreasing. To compute the curvature of $f$, we need to take a partial derivate of every component of the gradient with respect to each coordinate direction. This leads to an $(n\\times n)$ matrix of second partial derivatives called the *Hessian*:\n\n$$\nH(x) = \\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}\n$$\n\nIf our function $f$ has continuous second partial derivatives, then the order of differentiation does not matter and the matrix $H$ becomes symmetric. The Hessian allows us to establish two more (stronger) fundamental results of gradient-based optimization. Before stating them, let us recall that a matrix $A$ is *positive definite* if $x^\\top A x > 0$ for all $x\\neq 0$ and *positive semidefinite* if $x^\\top A x \\geq 0$ for all $x$.\n\n**Theorem (second-order necessary conditions):** If $x^\\star$ is a local minimizer of $f$ and the Hessian $H$ exists and is continuous in an open neighborhood of $x^\\star$, then $\\nabla f(x^\\star) = 0$ and $H$ is positive semidefinite.\n\n**Theorem (second-order sufficient conditions):** Suppose that $H$ is is continuous in an open neighborhood of $x^\\star$ and that $\\nabla f(x^\\star) =0$ and $H$ is positive definite. Then, $x^\\star$ is a *strict* local minimizer of $f$.\n\nIt is important to remark that the second-order sufficient condition gurantee a stronger result when compared to the second-order necessary condtions. Namely, the second-order sufficient conditions lead to a *strict* local minimizer. Furthermore, we should also note that the sufficient condtions are not necessary; it is possible for a point $x^\\star$ to be a strict local minimizer but fail to satisfy the sufficient conditions. As an example, consider $f(x) = x^6$, where $x^\\star = 0$ is a strict local minimizer, yet the Hessian evaluates to zero at $x^\\star = 0$ and is therefore not positive definite. \n\n## A complete example\nHere, we work our way through an example problem where we can see how the concepts from above can be applied to help us identify and characterize critical points. Consider the function:\n\n$$\nf(x_1, x_2) = 0.5x_1^4 + 2x_1^3 + 1.5x_1^2 + x_2^2 - 2x_1x_2\n$$\n\nLet's plot the contours of $f$.\n\n::: {#dd4407c1 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function\ndef f(x1, x2):\n    return 0.5 * x1**4 + 2 * x1**3 + (3/2) * x1**2 + x2**2 - 2 * x1 * x2\n\n# Create the contour plot\nx = np.linspace(-4, 2, 1000)\ny = np.linspace(-4, 2, 1000)\nX, Y = np.meshgrid(x, y)\nZ = f(X, Y)\n\nplt.figure(figsize=(9.5, 5))\ncontour = plt.contour(X, Y, Z, levels=75)\nplt.xlabel(r'$x_1$')\nplt.ylabel(r'$x_2$')\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nText(0, 0.5, '$x_2$')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](20_ugbo_files/figure-html/cell-3-output-2.png){width=778 height=427}\n:::\n:::\n\n\nTo start, we need to identify all critical points of $f$. We can do that by solving for all points such that $\\nabla f(x) = 0$, i.e.,\n\n$$\n\\nabla f(x_1, x_2) = \n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2}\n\\end{bmatrix} = \n\\begin{bmatrix}\n2x_1^3 + 6x_1^2 + 3x_1 - 2x_2 \\\\ 2x_2 - 2x_1\n\\end{bmatrix} = \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}\n$$\n\nWe can do that by using <code>fsolve()</code> from <code>scipy.optimize</code>.\n\n::: {#a8ff274d .cell execution_count=3}\n``` {.python .cell-code}\nfrom scipy.optimize import fsolve\n\n# Define the first partial derivatives\ndef df_dx1(x1, x2):\n    return 2 * x1**3 + 6 * x1**2 + 3 * x1 - 2 * x2\n\ndef df_dx2(x1, x2):\n    return 2 * x2 - 2 * x1\n\n# Define the system of equations\ndef equations(vars):\n    x1, x2 = vars\n    return [df_dx1(x1, x2), df_dx2(x1, x2)]\n\n# Solve for critical points\ninitial_guesses = [(0, 0), (-1, -1), (-10, -10)]\ncritical_points = [fsolve(equations, guess) for guess in initial_guesses]\n\nprint(critical_points)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[array([0., 0.]), array([-0.17712434, -0.17712434]), array([-2.82287566, -2.82287566])]\n```\n:::\n:::\n\n\nNow, we need to classify each critical point. To do so, let's compute the Hessian:\n\n$$\nH(x_1, x_2) = \n\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n6x_1^2 + 12x_1 + 3 & -2 \\\\\n-2 & 2\n\\end{bmatrix}\n$$\n\nWe now need to evaluate the Hessian at each one of the critical points and check whether is it positive semidefinite. There are a number of ways we can check this, but we will be examining the eigenvalues of the Hessian and checking whether they are all positive.\n\n::: {#592201aa .cell execution_count=4}\n``` {.python .cell-code}\n# Define the Hessian\ndef Hessian(x1, x2):\n    return np.array([[6 * x1**2 + 12 * x1 + 3, -2], [-2, 2]])\n\neigs = [np.linalg.eigvals(Hessian(pt[0], pt[1])) for pt in critical_points]\n\nprint(eigs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[array([4.56155281, 0.43844719]), array([-0.5227962 ,  3.58554227]), array([17.20040482,  1.73684911])]\n```\n:::\n:::\n\n\nBased on these results, we know that the first and third critical points are local minimizers. The second critical point is classifies as a *saddle point* because the Hessian is not positive semidefinite. To find out which of the local minimizers is the global minimzer, we evaluate the function at each of these points. Let's go ahead and label the points accordingly.\n\n::: {#cf05a3a7 .cell execution_count=5}\n``` {.python .cell-code}\n# Create the contour plot\nx = np.linspace(-4, 2, 1000)\ny = np.linspace(-4, 2, 1000)\nX, Y = np.meshgrid(x, y)\nZ = f(X, Y)\n\nplt.figure(figsize=(9.5, 5))\ncontour = plt.contour(X, Y, Z, levels=75)\nplt.xlabel(r'$x_1$')\nplt.ylabel(r'$x_2$')\n\n# Plot and label the critical points\nlabels = [\"local minimum\", \"saddle point\", \"global minimum\"]\n\nfor (x, y), label in zip(critical_points, labels):\n    plt.scatter(x, y, label=f'{label} ({x:.2f}, {y:.2f})', s=100)\n    if label == \"saddle point\":\n        plt.text(x, y, f' {label}', fontsize=12, ha='right', weight='bold')\n    else:\n        plt.text(x, y, f' {label}', fontsize=12, ha='left', weight='bold')\n\nplt.grid(False)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](20_ugbo_files/figure-html/cell-6-output-1.png){width=778 height=427}\n:::\n:::\n\n\n",
    "supporting": [
      "20_ugbo_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}