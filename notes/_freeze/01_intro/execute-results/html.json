{
  "hash": "73e42b148620fbd6de8fe0225b918f00",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Introduction\"\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nnumber-sections: true\nexecute:\n    freeze: auto\n---\n\nWelcome to AE 4803 AIM: Foundations of Scientific Machine Learning. This is a new course offered in the Spring 2025 term at Georgia Tech and is intended to be a \"foundations\" level course in the new College of Engineering AI for Engineering minor. \n\n## What's in a name? Artificial intelligence, machine learning, and data science\n\nI googled \"artificial intelligence\" and the first result comes from Google's new \"AI overview\" feature[^1]:\n![Image Source: Google.com](images/01_intro_AIgoogle.png){fig-align=\"center\" width=100% fig-alt=\"Artificial intelligence (AI) is a field of study that focuses on developing machines that can learn, reason, and act in ways that are usually associated with human intelligence.\"}\n\n[^1]: It seems apt to use AI to define AI, and I'm quoting it for illustrative purposes here, but I do want to point out that Google's AI Overview is not at all transparent about how these responses are generated and thus does not meet the standards for being cited as a source in most publishing venues. \n\nBecause I closed the tab with my first search and wanted to go back to the webpage to copy the text for the image alt-text above, I repeated my search, and got a somewhat different answer: \n![Image Source: Google.com](images/01_intro_AIgoogle2.png){fig-align=\"center\" width=100% fig-alt=\"Artificial intelligence (AI) is the ability of a machine to perform tasks that are usually associated with human intelligence.\"}\n\nThese are probably both reasonable definitions for casual conversation, but that's not what we're here for. Instead, we're here to really learn deeply about what AI is, and for that we're going to need **precision** -- that is, of our definitions. \n\n::: {.callout-note}\n## Exercise\nConsider the two different definitions of \"AI\" above carefully. In what (if any) senses are they (a) exactly the same, (b) similar, (c) somewhat different, (d) very different? What consequences might these differences have (a) developing AI algorithms, (b) evaluating AI impacts, (c) creating AI policies?\n:::\n\nI'm not going to provide a definition of AI for now and instead I'm going to throw two more terms into the mix that you may have heard. The first is \"machine learning\". Rather than get an AI definition for this too, I decided to go to the dictionary Merriam-Webster, which provides the following primary definition [@merriamML2024]: \n\n> a computational method that is a subfield of artificial intelligence and that enables a computer to learn to perform tasks by analyzing a large dataset without being explicitly programmed\n\nAnd finally I'll add the term \"data science.\" I wanted to give you a Merriam-Webster definition here, but the term isn't in their dictionary as of writing this on October 28, 2024. So instead I'm going to use Cambridge Dictionary's definition [@cambridgeDS2024]:\n\n> the use of scientific methods to obtain useful information from computer data, especially large amounts of data\n\n\n<!-- That's certainly one view. Another view comes from Yann LeCun, an NYU professor who is widely viewed as one of the \"godfathers\" of AI research, who negatively compares today's so-called AI capability with the intelligence of domestic cats: \"Felines, after all, have a mental model of the physical world, persistent memory, some reasoning ability and a capacity for planning, he says. None of these qualities are present in todayâ€™s 'frontier' AIs\" [@wsjLeCun2024].  -->\n\nThat's probably a reasonable definition for casual conversation, although\n\nsome remarks about what these things mean to different people, some links to further reading, narrow focus to supervised learning \n\n### Regression\n\nproblem of predicting outputs from inputs \n\nformulation as selection from within a parametrized model class \n\nchoosing parameters via optimization \n\n### Things that are not regression\n\nGive one example, list other examples, I cannot possibly hope to cover all these things in this course, so another goal of the course is to teach the language of ML so you are prepared to learn other things in future courses and on your own. \n\n## The three pillars of AI: mathematics, computation, and human intelligence \n\nmathematics is the language that describes what we want to do\n\ncomputation/programming is the execution (translation)\n\nhuman intelligence is woven throughout, specifies problem, evaluates results, re-specifies problem. \n\n### Course structure and intended learning outcomes\nfill in nearer to term\n\n<!-- ## A Motivating Example -->\n\n<!-- ![Image Source: Embry-Riddle Aeronautical University](images/AirfoilDrag.png){fig-align=\"center\" width=80%} -->\n<!-- \nSuppose we wish to efficiently approximate the force of drag ($F_d$), measured in $N$, on a specific airplane wing if we only have access to the following information about the wing and its operating conditions: \n\n* The angle of attack ($\\alpha$), measured in degrees\n* The density of the fluid ($\\rho$), measured in $kg/m^3$ \n* The velocity of the fluid ($v$), measured in $m/s$  -->\n\n<!-- ## Linear Approximation of an Unknown Function\n\nLet's bundle our inputs into a single vector, defined by: \n\n$$ \\mathbf{x} = \\begin{bmatrix} \\alpha \\\\ \\rho \\\\ v\\end{bmatrix} $$ \n\n\nWe assume there is some unknown function, $f(\\cdot):\\mathbb{R}^3 \\rightarrow \\mathbb{R}$, that gives us an optimal estimate for drag-force based on these \"features\": \n\n$$ F_d = f (\\mathbf{x}) + \\varepsilon $$ \n\nwhere $\\varepsilon$ is some external noise, disturbances or information entirely independent of the input variables.  -->\n\n<!-- Now consider a function $h(\\mathbf{x}; \\beta):\\mathbb{R}^3 \\rightarrow \\mathbb{R}$ which is a simple linear combination of the inputs: \n\n$$ h(\\mathbf{x}; \\beta) = \\begin{bmatrix} 1 & \\mathbf{x}^\\top \\end{bmatrix} \\begin{bmatrix}  \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3  \\end{bmatrix} = \\beta_0 + \\beta_1 \\alpha + \\beta_2 \\rho + \\beta_3 v $$\n\n**The Million Dollar Question:**\n\nGiven we know the structure of $h(\\mathbf{x}; \\beta)$, how can we efficiently optimize the parameters of $h(\\mathbf{x}; \\beta)$ to best approximate $f(\\mathbf{x})$? \n\n## Representing Observed Input-Output Data\n\nSuppose we have taken $N$ real-life samples of the input variables and their corresponding drag-force, $F_d$. Let each sample of the system under various operating conditions form row of a matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times 4}$ for the inputs and a matrix $\\mathbf{Y} \\in \\mathbb{R}^N$ for their corresponding outputs: \n\n$$ \\mathbf{X} = \\begin{bmatrix} 1 & \\alpha_1 & \\rho_1 & v_1 \\\\ 1 & \\alpha_2 & \\rho_2 & v_2 \\\\ & & \\vdots & \\\\ 1 & \\alpha_N & \\rho_N & v_N \\end{bmatrix} , \\mathbf{Y} = \\begin{bmatrix} F_{d1} \\\\ F_{d2} \\\\ \\vdots \\\\ F_{d3} \\end{bmatrix} $$ \n\n \nWe can efficiently compute the predictions of $h(\\mathbf{X}; \\beta)$ with a simple matrix-vector multiplication: \n\n$$ \\hat{\\mathbf{Y}} = \\mathbf{X} \\beta = \\begin{bmatrix} 1 & \\alpha_1 & \\rho_1 & v_1 \\\\ 1 & \\alpha_2 & \\rho_2 & v_2 \\\\ & & \\vdots & \\\\ 1 & \\alpha_N & \\rho_N & v_N \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{bmatrix} = \\begin{bmatrix}  \\beta_0 + \\beta_1 \\alpha_1 + \\beta_2 \\rho_1 + \\beta_3 v_1 \\\\ \\beta_0 + \\beta_1 \\alpha_2 + \\beta_2 \\rho_2 + \\beta_3 v_2  \\\\ \\vdots \\\\ \\beta_0 + \\beta_1 \\alpha_N + \\beta_2 \\rho_N + \\beta_3 v_N \\end{bmatrix}  \\in \\mathbb{R}^N $$ \n\n## The Least-Squares Optimization Problem\n\nOur goal is to adjust the parameters $\\beta$ so that our predictions, $\\hat{\\mathbf{Y}}$, are as close to the true outputs, $\\mathbf{Y}$ as possible. One way to measure prediction error is with Mean Squared Error: \n\n$$ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2 = \\frac{1}{N} || \\hat{\\mathbf{Y}} - \\mathbf{Y}||_2^2 = \\frac{1}{N} || \\mathbf{X} \\beta - \\mathbf{Y} ||_2^2 $$ \n\nthe $\\frac{1}{N}$ is only a scalar, we wish to choose the value of $\\beta$ that minimizes the following loss-function: \n\n$$ L(\\beta) = ||\\mathbf{X} \\beta - \\mathbf{Y}||_2^2 $$\n\n## Solving the Least-Squares Problem\n\nBecause the squared 2-Norm can be rewritten as an inner product, we can rewrite this loss-function as: \n\n$$ L(\\beta) = ||\\mathbf{X} \\beta - \\mathbf{Y}||_2^2= (\\mathbf{X} \\beta - \\mathbf{Y})^\\top (\\mathbf{X} \\beta - \\mathbf{Y}) $$\n$$ = (\\beta^\\top \\mathbf{X}^\\top - \\mathbf{Y}^\\top) (\\mathbf{X} \\beta - \\mathbf{Y}) $$\n\n$$ = \\beta^\\top \\mathbf{X^\\top X} \\beta - 2 \\beta^\\top \\mathbf{X^\\top \\mathbf{Y}} + \\mathbf{Y^\\top Y}$$\n\nAs we learned in multivariable calculus, to find a the extrema of a continuous function, we need to identify the critical points of the function. This means setting the gradient of the loss-function equal to the zero-vector:\n\n$$ \\nabla L_\\beta = 2 \\mathbf{X^\\top X} \\beta - 2 \\mathbf{X^\\top Y} = \\mathbf{0}$$ \n\nSolving this equation for $\\beta$ yields only one critical point: \n\n$$ \\hat{\\beta} = (\\mathbf{X^\\top X})^{-1} \\mathbf{X^\\top Y} $$ \n\nTo check whether this point is a local minimum, maximum, or neither, we examine the nature of the Hessian: \n\n$$ \\nabla^2 L_\\beta = 2 \\mathbf{X^\\top X} $$ \n\nBecause the Hessian is symmetric positive semidefinite and does not depend on $\\beta$, this means that $L(\\beta)$ is convex everywhere. Hence, $\\hat{\\beta}$ must be a global local minimum. \n\n## Experimental Example\n\nNow suppose we have 100 experimental observations of various angles of attack, air-densities and velocities with the corresponding drag-force in a pandas DataFrame called `df`: \n\n\n\n::: {#c9e581ff .cell execution_count=2}\n``` {.python .cell-code}\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alpha</th>\n      <th>rho</th>\n      <th>velocity</th>\n      <th>f_drag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>16.854305</td>\n      <td>1.319114</td>\n      <td>79.481130</td>\n      <td>9425.204935</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>42.782144</td>\n      <td>1.203298</td>\n      <td>50.263709</td>\n      <td>13722.379248</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>32.939727</td>\n      <td>1.320528</td>\n      <td>57.690388</td>\n      <td>12863.441979</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26.939632</td>\n      <td>0.696729</td>\n      <td>49.251769</td>\n      <td>3644.076532</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.020839</td>\n      <td>1.227098</td>\n      <td>19.524299</td>\n      <td>186.612122</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe can create $\\mathbf{X}$ and $\\mathbf{Y}$ matrices by extracting the input features and outputs from the dataframe: \n\n::: {#a211e74f .cell execution_count=3}\n``` {.python .cell-code}\nX = np.hstack((np.ones((N, 1)), df[['alpha', 'rho', 'velocity']].values))\nY = df['f_drag'].values\n```\n:::\n\n\nWe can solve for $\\hat{\\beta}$ by computing the normal equations: \n\n::: {#ad457603 .cell execution_count=4}\n``` {.python .cell-code}\nbeta_hat = np.linalg.inv(X.T @ X) @ (X.T @ Y)\n```\n:::\n\n\nAnd then compute the model's predictions at the training inputs: \n\n::: {#0e1ec900 .cell execution_count=5}\n``` {.python .cell-code}\nY_hat = X @ beta_hat\n```\n:::\n\n\nNow let's plot the $\\hat{\\mathbf{Y}}$ and $\\mathbf{Y}$ to examine how closely correlated the two are. A straight line with slope-1 would mean the model exactly fitted all of the outputs: \n\n::: {#40f45498 .cell execution_count=6}\n``` {.python .cell-code}\nplt.figure(figsize=(8,4))\nplt.scatter(Y, Y_hat)\nplt.grid()\nplt.xlabel(\"True Drag Force\")\nplt.ylabel(\"Model Predicted Drag Force\")\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\nText(0, 0.5, 'Model Predicted Drag Force')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](01_intro_files/figure-html/cell-7-output-2.png){width=697 height=350}\n:::\n:::\n\n\nThere is clearly a strong, but nonlinear correlation between the true and model-predicted drag-force. This indicates that there is a nonlinear relationship between the features and the output values. Let us quantify the MSE: \n\n::: {#e9736d06 .cell execution_count=7}\n``` {.python .cell-code}\nlinear_mse = np.linalg.norm(Y_hat - Y, 2) / N\nprint(\"Linear Features Mean-Squared Error: %.2f\" % (linear_mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Features Mean-Squared Error: 645.47\n```\n:::\n:::\n\n\n## Using Nonlinear Features to Improve Model Performance\n\nWe know the underlying formula for drag-force is: \n\n$$ F_d = \\frac{1}{2} \\rho v^2 C_d A  $$ \n\nWhile this function doesn't depend on the angle of attack, we do see how the features are polynomially related to one another. Naturally, it is going to be difficult for a linear combination of features to replicate this. However, if we take the log of both sides, something interesting happens: \n\n$$ \\ln(F_d) = \\ln(\\frac{1}{2} \\rho v^2 C_d A) = \\ln(\\frac{1}{2}) + \\ln(\\rho) + 2\\ln(v) + \\ln(C_d) + \\ln(A) $$\n\nWhen our features and outputs are log-scaled, we see a friendly linear-combination of log-scaled versions features emerge! Let's alter our inputs and outputs accordingly, so our problem becomes: \n\n$$ \\mathbf{X} = \\begin{bmatrix} 1 & \\ln \\alpha_1 & \\ln \\rho_1 & \\ln v_1 \\\\ 1 & \\ln \\alpha_2 & \\ln \\rho_2 & \\ln v_2 \\\\ & & \\vdots & \\\\ 1 & \\ln \\alpha_N & \\ln \\rho_N & \\ln v_N \\end{bmatrix} , \\mathbf{Y} = \\begin{bmatrix} \\ln F_{d1} \\\\ \\ln F_{d2} \\\\ \\vdots \\\\ \\ln F_{d3} \\end{bmatrix} $$ \n\nLet's see how this works programmatically: \n\n::: {#87b10c43 .cell execution_count=8}\n``` {.python .cell-code}\nX_log = np.hstack((np.ones((N, 1)), np.log(df[['alpha', 'rho', 'velocity']].values)))\nY_log = np.log(df['f_drag'].values - df['f_drag'].min() + 1)\n\nbeta_log = np.linalg.inv(X_log.T @ X_log) @ (X_log.T @ Y_log)\n\nY_hat_log = np.exp(X_log @ beta_log)+ df['f_drag'].min() - 1\n\nplt.figure(figsize=(8,4))\nplt.scatter(Y, Y_hat_log)\nplt.grid()\nplt.xlabel(\"True Drag Force\")\nplt.ylabel(\"Model Predicted Drag Force\")\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\nText(0, 0.5, 'Model Predicted Drag Force')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](01_intro_files/figure-html/cell-9-output-2.png){width=686 height=350}\n:::\n:::\n\n\nNow let's see if our MSE has changed: \n\n::: {#ab35ca21 .cell execution_count=9}\n``` {.python .cell-code}\nlog_mse = np.linalg.norm(Y_hat_log - Y, 2)/N\nprint(\"Logarithmically Scaled Features Mean-Squared Error: %.2f\" % (log_mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogarithmically Scaled Features Mean-Squared Error: 144.70\n```\n:::\n:::\n\n\nWe have nearly a 5x reduction in MSE just by log-scaling our features! If we choose the right features, linear regression can be an extremely powerful method for approximating complex functions.  -->\n\n---\n\n",
    "supporting": [
      "01_intro_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}