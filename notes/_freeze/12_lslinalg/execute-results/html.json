{
  "hash": "a99e034fa3e6732d8edafddd6914e10f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"LLS: Linear Algebra Perspective\"\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n\n---\n\n::: {.hidden}\n\n\\newcommand*{\\R}{\\mathbb{R}}\n\\newcommand*{\\N}{\\mathbb{N}}\n\\newcommand*{\\cF}{\\mathcal{F}}\n<!-- \\renewcommand{\\P}{\\mathrm{P}} -->\n\n<!-- $$\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n$$\n\n\\definecolor{quarto-callout-note-color}{HTML}{4477AA} -->\n\n:::\n\n\n\nThe intended learning outcomes of these notes are that students should be able to:\n\n\n## range, kernel and expressivity, well-posedness\nstart with \"if invertible\" and go from there...\n\n\nfeature selection \n\nmultiple solutions \n\nlow-rank or near-low-rank-ness\n\n## QR decomposition\n\nalgorithm, cost \n\n## SVD \n\nfor data reduction, and for regularization\n\n## Experimental Example\n\nNow suppose we have 100 experimental observations of various angles of attack, air-densities and velocities with the corresponding drag-force in a pandas DataFrame called `df`: \n\n\n\n::: {#f80a2905 .cell execution_count=2}\n``` {.python .cell-code}\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alpha</th>\n      <th>rho</th>\n      <th>velocity</th>\n      <th>f_drag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>16.854305</td>\n      <td>1.319114</td>\n      <td>79.481130</td>\n      <td>9425.204935</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>42.782144</td>\n      <td>1.203298</td>\n      <td>50.263709</td>\n      <td>13722.379248</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>32.939727</td>\n      <td>1.320528</td>\n      <td>57.690388</td>\n      <td>12863.441979</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26.939632</td>\n      <td>0.696729</td>\n      <td>49.251769</td>\n      <td>3644.076532</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.020839</td>\n      <td>1.227098</td>\n      <td>19.524299</td>\n      <td>186.612122</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe can create $\\mathbf{X}$ and $\\mathbf{Y}$ matrices by extracting the input features and outputs from the dataframe: \n\n::: {#e6420fd6 .cell execution_count=3}\n``` {.python .cell-code}\nX = np.hstack((np.ones((N, 1)), df[['alpha', 'rho', 'velocity']].values))\nY = df['f_drag'].values\n```\n:::\n\n\nWe can solve for $\\hat{\\beta}$ by computing the normal equations: \n\n::: {#9e316f54 .cell execution_count=4}\n``` {.python .cell-code}\nbeta_hat = np.linalg.inv(X.T @ X) @ (X.T @ Y)\n```\n:::\n\n\nAnd then compute the model's predictions at the training inputs: \n\n::: {#616ec706 .cell execution_count=5}\n``` {.python .cell-code}\nY_hat = X @ beta_hat\n```\n:::\n\n\nNow let's plot the $\\hat{\\mathbf{Y}}$ and $\\mathbf{Y}$ to examine how closely correlated the two are. A straight line with slope-1 would mean the model exactly fitted all of the outputs: \n\n::: {#93680555 .cell execution_count=6}\n``` {.python .cell-code}\nplt.figure(figsize=(8,4))\nplt.scatter(Y, Y_hat)\nplt.grid()\nplt.xlabel(\"True Drag Force\")\nplt.ylabel(\"Model Predicted Drag Force\")\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nText(0, 0.5, 'Model Predicted Drag Force')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](12_lslinalg_files/figure-html/cell-7-output-2.png){width=697 height=350}\n:::\n:::\n\n\nThere is clearly a strong, but nonlinear correlation between the true and model-predicted drag-force. This indicates that there is a nonlinear relationship between the features and the output values. Let us quantify the MSE: \n\n::: {#467dec92 .cell execution_count=7}\n``` {.python .cell-code}\nlinear_mse = np.linalg.norm(Y_hat - Y, 2) / N\nprint(\"Linear Features Mean-Squared Error: %.2f\" % (linear_mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Features Mean-Squared Error: 645.47\n```\n:::\n:::\n\n\n## Using Nonlinear Features to Improve Model Performance\n\nWe know the underlying formula for drag-force is: \n\n$$ F_d = \\frac{1}{2} \\rho v^2 C_d A  $$ \n\nWhile this function doesn't depend on the angle of attack, we do see how the features are polynomially related to one another. Naturally, it is going to be difficult for a linear combination of features to replicate this. However, if we take the log of both sides, something interesting happens: \n\n$$ \\ln(F_d) = \\ln(\\frac{1}{2} \\rho v^2 C_d A) = \\ln(\\frac{1}{2}) + \\ln(\\rho) + 2\\ln(v) + \\ln(C_d) + \\ln(A) $$\n\nWhen our features and outputs are log-scaled, we see a friendly linear-combination of log-scaled versions features emerge! Let's alter our inputs and outputs accordingly, so our problem becomes: \n\n$$ \\mathbf{X} = \\begin{bmatrix} 1 & \\ln \\alpha_1 & \\ln \\rho_1 & \\ln v_1 \\\\ 1 & \\ln \\alpha_2 & \\ln \\rho_2 & \\ln v_2 \\\\ & & \\vdots & \\\\ 1 & \\ln \\alpha_N & \\ln \\rho_N & \\ln v_N \\end{bmatrix} , \\mathbf{Y} = \\begin{bmatrix} \\ln F_{d1} \\\\ \\ln F_{d2} \\\\ \\vdots \\\\ \\ln F_{d3} \\end{bmatrix} $$ \n\nLet's see how this works programmatically: \n\n::: {#739738e7 .cell execution_count=8}\n``` {.python .cell-code}\nX_log = np.hstack((np.ones((N, 1)), np.log(df[['alpha', 'rho', 'velocity']].values)))\nY_log = np.log(df['f_drag'].values - df['f_drag'].min() + 1)\n\nbeta_log = np.linalg.inv(X_log.T @ X_log) @ (X_log.T @ Y_log)\n\nY_hat_log = np.exp(X_log @ beta_log)+ df['f_drag'].min() - 1\n\nplt.figure(figsize=(8,4))\nplt.scatter(Y, Y_hat_log)\nplt.grid()\nplt.xlabel(\"True Drag Force\")\nplt.ylabel(\"Model Predicted Drag Force\")\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nText(0, 0.5, 'Model Predicted Drag Force')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](12_lslinalg_files/figure-html/cell-9-output-2.png){width=686 height=350}\n:::\n:::\n\n\nNow let's see if our MSE has changed: \n\n::: {#c279ed41 .cell execution_count=9}\n``` {.python .cell-code}\nlog_mse = np.linalg.norm(Y_hat_log - Y, 2)/N\nprint(\"Logarithmically Scaled Features Mean-Squared Error: %.2f\" % (log_mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogarithmically Scaled Features Mean-Squared Error: 144.70\n```\n:::\n:::\n\n\nWe have nearly a 5x reduction in MSE just by log-scaling our features! If we choose the right features, linear regression can be an extremely powerful method for approximating complex functions. \n\n---\n\n",
    "supporting": [
      "12_lslinalg_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}