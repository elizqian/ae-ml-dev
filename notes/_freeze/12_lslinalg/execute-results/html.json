{
  "hash": "bff011a7a6bf8bede9df94969251e354",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"LLS: Linear Algebra Perspective\"\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n\n---\n\n::: {.hidden}\n\n\\newcommand*{\\R}{\\mathbb{R}}\n\\newcommand*{\\N}{\\mathbb{N}}\n\\newcommand*{\\cF}{\\mathcal{F}}\n\n\\newcommand{\\E}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\Var}[1]{\\text{Var}\\left[#1\\right]}\n\\newcommand{\\Corr}[2]{\\text{Corr}\\left[#1, #2\\right]}\n\\newcommand{\\Cov}[2]{\\text{Cov}\\left[#1, #2\\right]}\n\\newcommand{\\Tr}{\\text{Tr}}\n\n\\newcommand{\\bA}{\\mathbf{A}}\n\\newcommand{\\bB}{\\mathbf{B}}\n\\newcommand{\\bC}{\\mathbf{C}}\n\\newcommand{\\bD}{\\mathbf{D}}\n\\newcommand{\\bE}{\\mathbf{E}}\n\\newcommand{\\bF}{\\mathbf{F}}\n\\newcommand{\\bG}{\\mathbf{G}}\n\\newcommand{\\bH}{\\mathbf{H}}\n\\newcommand{\\bI}{\\mathbf{I}}\n\\newcommand{\\bJ}{\\mathbf{J}}\n\\newcommand{\\bK}{\\mathbf{K}}\n\\newcommand{\\bL}{\\mathbf{L}}\n\\newcommand{\\bM}{\\mathbf{M}}\n\\newcommand{\\bN}{\\mathbf{N}}\n\\newcommand{\\bO}{\\mathbf{O}}\n\\newcommand{\\bP}{\\mathbf{P}}\n\\newcommand{\\bQ}{\\mathbf{Q}}\n\\newcommand{\\bR}{\\mathbf{R}}\n\\newcommand{\\bS}{\\mathbf{S}}\n\\newcommand{\\bT}{\\mathbf{T}}\n\\newcommand{\\bU}{\\mathbf{U}}\n\\newcommand{\\bV}{\\mathbf{V}}\n\\newcommand{\\bW}{\\mathbf{W}}\n\\newcommand{\\bX}{\\mathbf{X}}\n\\newcommand{\\bY}{\\mathbf{Y}}\n\\newcommand{\\bZ}{\\mathbf{Z}}\n\n\\newcommand{\\ba}{\\mathbf{a}}\n\\newcommand{\\bb}{\\mathbf{b}}\n\\newcommand{\\bc}{\\mathbf{c}}\n\\newcommand{\\bd}{\\mathbf{d}}\n\\newcommand{\\be}{\\mathbf{e}}\n\\newcommand{\\bef}{\\mathbf{f}}\n\\newcommand{\\bg}{\\mathbf{g}}\n\\newcommand{\\bh}{\\mathbf{h}}\n\\newcommand{\\bi}{\\mathbf{i}}\n\\newcommand{\\bj}{\\mathbf{j}}\n\\newcommand{\\bk}{\\mathbf{k}}\n\\newcommand{\\bl}{\\mathbf{l}}\n\\newcommand{\\bem}{\\mathbf{m}}\n\\newcommand{\\bn}{\\mathbf{n}}\n\\newcommand{\\bo}{\\mathbf{o}}\n\\newcommand{\\bp}{\\mathbf{p}}\n\\newcommand{\\bq}{\\mathbf{q}}\n\\newcommand{\\br}{\\mathbf{r}}\n\\newcommand{\\bs}{\\mathbf{s}}\n\\newcommand{\\bt}{\\mathbf{t}}\n\\newcommand{\\bu}{\\mathbf{u}}\n\\newcommand{\\bv}{\\mathbf{v}}\n\\newcommand{\\bw}{\\mathbf{w}}\n\\newcommand{\\bx}{\\mathbf{x}}\n\\newcommand{\\by}{\\mathbf{y}}\n\\newcommand{\\bz}{\\mathbf{z}}\n\n<!-- \\renewcommand{\\P}{\\mathrm{P}} -->\n\n<!-- $$\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n$$\n\n\\definecolor{quarto-callout-note-color}{HTML}{4477AA} -->\n\n:::\n\n\n\nWe now take a deeper dive into the structure of linear least squares problems. We will see how theoretical concepts from linear algebra can be used to describe and explain properties of learned linear regression models, and we will introduce fundamental algorithms from computational linear algebra that are used to solve linear least squares problems.\n\nThe intended learning outcomes of these notes are that students should be able to:\n\n\n## range, kernel and expressivity, well-posedness\nstart with \"if invertible\" and go from there...\n\n\nfeature selection \n\nmultiple solutions \n\nlow-rank or near-low-rank-ness\n\n## QR decomposition\n\nalgorithm, cost \n\n## SVD \n\nfor data reduction, and for regularization\n\n\n## Using Nonlinear Features to Improve Model Performance\n\n<!-- We know the underlying formula for drag-force is: \n\n$$ F_d = \\frac{1}{2} \\rho v^2 C_d A  $$  -->\n\n<!-- While this function doesn't depend on the angle of attack, we do see how the features are polynomially related to one another. Naturally, it is going to be difficult for a linear combination of features to replicate this. However, if we take the log of both sides, something interesting happens: \n\n$$ \\ln(F_d) = \\ln(\\frac{1}{2} \\rho v^2 C_d A) = \\ln(\\frac{1}{2}) + \\ln(\\rho) + 2\\ln(v) + \\ln(C_d) + \\ln(A) $$\n\nWhen our features and outputs are log-scaled, we see a friendly linear-combination of log-scaled versions features emerge! Let's alter our inputs and outputs accordingly, so our problem becomes: \n\n$$ \\mathbf{X} = \\begin{bmatrix} 1 & \\ln \\alpha_1 & \\ln \\rho_1 & \\ln v_1 \\\\ 1 & \\ln \\alpha_2 & \\ln \\rho_2 & \\ln v_2 \\\\ & & \\vdots & \\\\ 1 & \\ln \\alpha_N & \\ln \\rho_N & \\ln v_N \\end{bmatrix} , \\mathbf{Y} = \\begin{bmatrix} \\ln F_{d1} \\\\ \\ln F_{d2} \\\\ \\vdots \\\\ \\ln F_{d3} \\end{bmatrix} $$  -->\n\n<!-- Let's see how this works programmatically:  -->\n<!-- \n\nX_log = np.hstack((np.ones((N, 1)), np.log(df[['alpha', 'rho', 'velocity']].values)))\nY_log = np.log(df['f_drag'].values - df['f_drag'].min() + 1)\n\nbeta_log = np.linalg.inv(X_log.T @ X_log) @ (X_log.T @ Y_log)\n\nY_hat_log = np.exp(X_log @ beta_log)+ df['f_drag'].min() - 1\n\nplt.figure(figsize=(8,4))\nplt.scatter(Y, Y_hat_log)\nplt.grid()\nplt.xlabel(\"True Drag Force\")\nplt.ylabel(\"Model Predicted Drag Force\")\n``` -->\n\n<!-- Now let's see if our MSE has changed:  -->\n<!-- \n\nlog_mse = np.linalg.norm(Y_hat_log - Y, 2)/N\nprint(\"Logarithmically Scaled Features Mean-Squared Error: %.2f\" % (log_mse))\n``` -->\n\n<!-- We have nearly a 5x reduction in MSE just by log-scaling our features! If we choose the right features, linear regression can be an extremely powerful method for approximating complex functions.  -->\n\n---\n\n",
    "supporting": [
      "12_lslinalg_files"
    ],
    "filters": [],
    "includes": {}
  }
}