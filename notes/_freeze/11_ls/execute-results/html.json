{
  "hash": "775af6d796a149d995b57c09d2af6b7c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Linear Least Squares Problems\"\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n\n---\n\n::: {.hidden}\n\n\\newcommand*{\\R}{\\mathbb{R}}\n\\newcommand*{\\N}{\\mathbb{N}}\n\\newcommand*{\\cF}{\\mathcal{F}}\n<!-- \\renewcommand{\\P}{\\mathrm{P}} -->\n\n<!-- $$\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n$$\n\n\\definecolor{quarto-callout-note-color}{HTML}{4477AA} -->\n\n:::\n\n\n\nWe begin our exploration of scientific machine learning methods in the fundamental setting of *linear regression problems*. The intended learning outcomes of these notes are that students should be able to:\n\n1. Classify a regression problem as linear or nonlinear\n\n2. Understand the abstract form of linear least squares problems\n\n    a. Define the abstract form of linear least squares problems and be able to explain what its key ingredients are\n\n    b. Translate a description of a model learning problem in words into a specific instance of the abstract form\n\n    c. Construct examples of linear least squares problems, cast them in the abstract form, and explain their context\n\n3. Solve linear least-squares problems by deriving and solving the normal equations. \n\n4. Evaluate linear regression models by computing and interpreting mean error, mean relative error, and R^2 values on both training and test data sets.\n\n## Classifying regression problems as linear vs nonlinear\n\nWe have previously introduced regression problems in a general way: recall that the three ingredients of (parametrized) regression problems are (1) paired input and output data, (2) the choice of a parametrized model class, and (3) a method for choosing a model from within that class. The classification of a regression problem as *linear* or *nonlinear* depends solely on ingredient (2), the parametrized model class: if the models in that class depend linearly on the model parameters, the regression problem is a linear regression problem.\n\n::: {.callout-caution collapse=\"true\"}\n## Check your knowledge: Do you remember what it means for a function to depend linearly on a variable?\n\nThe function $f(z;\\theta): \\R^d\\times\\Theta\\to\\R$ is said to be *linear* in the parameters $\\theta$ if, for all $a,b\\in\\R$ and all $\\theta_1,\\theta_2\\in\\Theta$, the following holds:  $f(z; a\\theta_1 + b\\theta_2) = af(z;\\theta_1) + bf(z;\\theta_2)$. \n\nNote that when we say a function is \"linear\", we have to specify in *what*. That is, we can also say $f(z;\\theta)$ is linear in the *inputs* if, for all $a,b\\in\\R$ and all $z_1,z_2\\in\\R^d$, the following holds: $f(az_1 + bz_2;\\theta) = af(z_1;\\theta)+bf(z_2;\\theta)$.\n:::\n\nThe classification of regression problems as linear or nonlinear depends **solely** on the dependence of the functions in the parametrized model class on the **parameters**. That is, we can define functions that are *linear* in $\\theta$ while being *nonlinear* in $z$. \n\n::: {.callout-note}\n## Exercise\nConsider the model classes (@eq-quad-models)-(@eq-perceptron) introduced previously. Are these model classes linear or nonlinear in the parameters? In the inputs?\n:::\n\n## Mathematical problem formulation\nWe are now going to introduce an *abstract* mathematical problem formulation that can be used to describe many *specific instances* of linear regression problems. This is a theme of the course and throughout computational mathematics and engineering: abstraction using the language of mathematics lets us isolate the core essence of the problem we're solving and develop powerful algorithms that can solve specific applications of those problems across a wide range of disciplines. I'll introduce the abstract formulation first, and follow it up with some specific examples. \n\nIngredient 1 (the data set): let $\\{(z_i,y_i)\\}_{i=1}^N$ be a given data set of paired inputs $z_i\\in\\R^d$ and outputs $y_i\\in\\R$. \n\nIngredient 2 (the parametrized model class): let $x:\\R^d\\to\\R^n$ be a function that maps the $d$-dimensional input to an $n$-dimensional *feature vector*. For a fixed $x$, we will consider the following parametrized model class:\n\n$$\n\\mathcal{F}_\\beta := \\{ x(z)^\\top \\beta : \\beta\\in\\R^n\\}\n$$ {#eq-linreg-model-class}\n\nRecall that @eq-linreg-model-class is read as \"$\\mathcal{F}_\\beta$ is defined to be the set of all functions $f(z;\\beta) = x(z)^\\top\\beta$ for all $\\beta\\in\\R^n$.\" This is an abstract way to define the model class for *any* linear regression problem, as we will describe in more detail shortly. \n\nIngredient 3 (the method of choosing the parameters): let $\\beta^*$ be given by \n\n$$\n\\begin{aligned}\n\\beta^* &= \\arg\\min_{\\beta\\in\\R^n} \\frac1N \\sum_{i=1}^N (f(z_i;\\beta) - y_i)^2 \\\\\n&= \\arg\\min_{\\beta\\in\\R^N} \\frac1N \\sum_{i=1}^N (x(z_i)^\\top\\beta - y_i)^2.\n\\end{aligned}\n$$ {#eq-linreg-minimization}\nThen, we define the *learned model* to be $f(z;\\beta^*) = x(z)^\\top\\beta^*$. We call $\\beta^*$ defined this way the \"optimal regression parameters\" or just the \"optimal parameters\", because they are the result of solving an optimization problem. Note that the objective of this optimization function is *defined by the data*, and represents the average squared error of the model over the data set. \n\nTaken together, the three ingredients I have defined above define a *linear least squares problem*, which is a subclass of linear regression problems. We'll now give several examples of specific instances of linear least squares problems to illustrate how broadly applicable this abstract framework is.\n\n## Examples \n\n### Example 1: Aerodynamic drag prediction\n\n<!-- ![Image Source: [@erauAirfoil2024]](images/AirfoilDrag.png){fig-align=\"center\" width=80%} -->\n\nAn important task in aerodynamic design is predicting lift and drag forces on a body moving through air. Let's consider a simplified problem where we are given an fixed airfoil design and our goal is to predict the drag force $F_d$ on the airfoil as a function of three parameters which describe its flight conditions:\n\n* The angle of attack $\\alpha$ \n* The density of the fluid $\\rho$\n* The freestream velocity of the air $v$\n\nTo put this problem in our abstract framework, we define $z = (\\alpha,\\rho,v)^\\top$ to be a three-dimensional input, and take the output to be the drag force $y= F_d$. In order to define a regression problem, we require the existence of a data set $\\{(z_i,y_i)\\}_{i=1}^N$. Note that in this case $z_i = (\\alpha_i,\\rho_i,v_i)$. We assume that this data set is given.\n\nIn linear regression problems, defining the model class amounts to choosing a set of regression *features* by defining $x$. A simple choice takes the inputs themselves to be features: $x^{(1)}(z) = z = (\\alpha,\\rho,v)^\\top$. This leads to a class of parametrized models with a three-dimensional unknown parameter vector $\\beta\\in\\R^3$. The models in this class have the following form:\n\n$$\nf^{(1)}(z;\\beta) = x^{(1)}(z)^\\top\\beta = (\\alpha,\\rho,v) \\beta = \\beta_1\\alpha + \\beta_2\\rho + \\beta_3 v.\n$$\n\nThere are many other possible choices. For example, consider $x^{(2)}(z) = (\\alpha,\\rho, v, \\alpha^2, \\rho^2, v^2, \\alpha\\rho, \\alpha v, \\rho v)^\\top$. This leads to a parametrized model class with a *nine*-dimensional unknown parameter vector $\\beta\\in\\R^9$:\n\n$$\n\\begin{aligned}\nf^{(2)}(z;\\beta) &= x^{(2)}(z)^\\top\\beta = (\\alpha,\\rho, v, \\alpha^2, \\rho^2, v^2, \\alpha\\rho, \\alpha v, \\rho v)\\beta \\\\\n&= \\beta_1\\alpha + \\beta_2\\rho + \\beta_3 v + \\beta_4\\alpha^2 + \\beta_5\\rho^2 + \\beta_6 v^2 + \\beta_7\\alpha\\rho + \\beta_8\\alpha v + \\beta_9 \\rho v\n\\end{aligned}\n$$\n\nFor either the above choices of features $x^{(1)}(z)$ or $x^{(2)}(z)$, we could then define and solve the minimization @eq-linreg-minimization to find the optimal regression parameters and define our learned model $f^{(1)}(z;\\beta^*)$ or $f^{(2)}(z;\\beta^*)$. \n\n::: {.callout-note}\n## Understanding notation\nNote that we use $\\beta$ to denote the unknown parameters in both $f^{(1)}$ and $f^{(2)}$ above despite $\\beta$ referring to different quantities in the definition of the different functions. This is a common notational shortcut --- while we could use the notation $\\beta^{(1)}\\in\\R^3$ and $\\beta^{(2)}\\in\\R^9$ to specify the different $\\beta$ for the different functions, this can be cumbersome if we are considering many different options for the choice of features $x(z)$, and it's standard to just use $\\beta$, where the definition of $\\beta$ is implied by the context. One of the challenges in learning about machine learning and computational mathematics more generally is getting used to similar notation meaning different things in different contexts. That's one of the things that we'll practice in this course.\n:::\n\n\n### Example 2: something else\n\n### Example 3: something else\n\n## Solving linear least squares problems\n\nLinear least squares problems are special because they have closed form solutions: that is, we can write an analytical expression for the optimum parameters $\\beta^*$ in terms of the data. To do so, we are going to define a feature data matrix $X\\in\\R^{N\\times n}$ and an output data vector $Y\\in\\R^N$ as follows:\n\n$$\nX = \\begin{pmatrix}\n- &x(z_1) & - \\\\\n & \\vdots & \\\\\n- & x(z_N) & -\n\\end{pmatrix}, \n\\qquad \nY = \\begin{pmatrix}\ny_1 \\\\ \\vdots \\\\ y_N\n\\end{pmatrix},\n$$\nwhere the *rows* of $X$ and the *elements* of $Y$ correspond to input-output pairs in the data set. Note that each *column* of $X$ corresponds to a different *feature* defined by an element of the vector-valued function \n$x$.\n\nUsing this notation, @eq-linreg-minimization can be rewritten as\n\n$$\n\\beta^* = \\arg\\min_{\\beta\\in\\R^n} \\frac1N\\|X \\beta - Y\\|^2 \n$$ {#eq-linreg-matrix-min}\n\nThis is the minimization of a multivariate function (because $\\beta$ is a vector). Recall from calculus that to find minimizers of multivariate functions we first seek *critical points* that satisfy the *first-order necessary conditions* for optimality: that is, we look for points where the derivative of the objective function is 0. Let $\\mathcal{L}(\\beta) = \\frac1N\\|X\\beta-Y\\|^2 = \\frac1N(X\\beta - Y)^\\top (X\\beta-Y)$. Then,\n\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial \\beta} = \\frac2N(X^\\top X\\beta - X^\\top Y)\n$$\n\nSetting the derivative equal to 0 yields the standard *normal equations*[^1]:\n\n[^1]: note that the constant factor $\\frac2N$ drops out. It's common to play fast and loose with multiplicative constants in minimizations -- other sources define the minimization objective with a $\\frac12$ multiplier, which leads to the derivative not having the factor of 2 in front and leads to the same critical point. Or you may see the objective function defined without the $1/N$ in front, which still leads to the same critical point.\n\n$$\nX^\\top X\\beta^* = X^\\top Y\n$$\n\nIf $(X^\\top X)$ is invertible, then the unique solution to the normal equations is given by\n\n$$ \n\\beta^* = (X^\\top X)^{-1} X^\\top Y\n$$\n\nand this choice of $\\beta$ defines our final learned model: $f(z;\\beta^*) = x(z)^\\top\\beta^*$.\n<!-- \nNow suppose we have 100 experimental observations of various angles of attack, air-densities and velocities with the corresponding drag-force in a pandas DataFrame called `df`: \n\n\n\n::: {#3d0d2d03 .cell execution_count=2}\n``` {.python .cell-code}\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alpha</th>\n      <th>rho</th>\n      <th>velocity</th>\n      <th>f_drag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>16.854305</td>\n      <td>1.319114</td>\n      <td>79.481130</td>\n      <td>9425.204935</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>42.782144</td>\n      <td>1.203298</td>\n      <td>50.263709</td>\n      <td>13722.379248</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>32.939727</td>\n      <td>1.320528</td>\n      <td>57.690388</td>\n      <td>12863.441979</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26.939632</td>\n      <td>0.696729</td>\n      <td>49.251769</td>\n      <td>3644.076532</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.020839</td>\n      <td>1.227098</td>\n      <td>19.524299</td>\n      <td>186.612122</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe can create $\\mathbf{X}$ and $\\mathbf{Y}$ matrices by extracting the input features and outputs from the dataframe: \n\n::: {#56a5bc8a .cell execution_count=3}\n``` {.python .cell-code}\nX = np.hstack((np.ones((N, 1)), df[['alpha', 'rho', 'velocity']].values))\nY = df['f_drag'].values\n```\n:::\n\n\nWe can solve for $\\hat{\\beta}$ by computing the normal equations: \n\n::: {#a4b1e1d2 .cell execution_count=4}\n``` {.python .cell-code}\nbeta_hat = np.linalg.inv(X.T @ X) @ (X.T @ Y)\n```\n:::\n\n\nAnd then compute the model's predictions at the training inputs: \n\n::: {#ab75736d .cell execution_count=5}\n``` {.python .cell-code}\nY_hat = X @ beta_hat\n```\n:::\n\n\nNow let's plot the $\\hat{\\mathbf{Y}}$ and $\\mathbf{Y}$ to examine how closely correlated the two are. A straight line with slope-1 would mean the model exactly fitted all of the outputs: \n\n::: {#b28ba05d .cell execution_count=6}\n``` {.python .cell-code}\nplt.figure(figsize=(8,4))\nplt.scatter(Y, Y_hat)\nplt.grid()\nplt.xlabel(\"True Drag Force\")\nplt.ylabel(\"Model Predicted Drag Force\")\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nText(0, 0.5, 'Model Predicted Drag Force')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](11_ls_files/figure-html/cell-7-output-2.png){width=697 height=350}\n:::\n:::\n\n\nThere is clearly a strong, but nonlinear correlation between the true and model-predicted drag-force. This indicates that there is a nonlinear relationship between the features and the output values. Let us quantify the MSE: \n\n\nlinear_mse = np.linalg.norm(Y_hat - Y, 2) / N\nprint(\"Linear Features Mean-Squared Error: %.2f\" % (linear_mse))\n``` -->\n\n\n## Assessing the learned models\n\nvarious metrics, validation? definitely test data sets.\n\n## Exercises\n\na few relatively short questions that test the learning outcomes. \n\n- math training: derive the derivative of the loss function by expanding, and then applying identities from [@petersen2012matrix]. Hard mode: prove the identities by calculation.\n\n- math training/review: what does it mean for a matrix $A$ to be invertible (inverse exists, square and full rank, linearly independent rows/columns)\n\n- math training: prove that $X^\\top X$ is invertible iff $X$ has full column rank.\n\n- math training: explain why the minimizer is unique iff $X^\\top X$ is invertible... what happens if it's not invertible?\n\n## Further reading\n\nany appropriate links\n---\n\n",
    "supporting": [
      "11_ls_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}