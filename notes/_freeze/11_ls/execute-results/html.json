{
  "hash": "10fd0c6a5888319c5665879904c01465",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"ML for AE: Least Squares Regression\"\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nnumber-sections: false\n# format: pptx\nexecute:\n    freeze: auto\n---\n\n## A Motivating Example\n\n![Image Source: Embry-Riddle Aeronautical University](images/AirfoilDrag.png){fig-align=\"center\" width=80%}\n\nSuppose we wish to efficiently approximate the force of drag ($F_d$), measured in $N$, on a specific airplane wing if we only have access to the following information about the wing and its operating conditions: \n\n* The angle of attack ($\\alpha$), measured in degrees\n* The density of the fluid ($\\rho$), measured in $kg/m^3$ \n* The velocity of the fluid ($v$), measured in $m/s$ \n\n## Linear Approximation of an Unknown Function\n\nLet's bundle our inputs into a single vector, defined by: \n\n$$ \\mathbf{x} = \\begin{bmatrix} \\alpha \\\\ \\rho \\\\ v\\end{bmatrix} $$ \n\n\nWe assume there is some unknown function, $f(\\cdot):\\mathbb{R}^3 \\rightarrow \\mathbb{R}$, that gives us an optimal estimate for drag-force based on these \"features\": \n\n$$ F_d = f (\\mathbf{x}) + \\varepsilon $$ \n\nwhere $\\varepsilon$ is some external noise, disturbances or information entirely independent of the input variables. \n\nNow consider a function $h(\\mathbf{x}; \\beta):\\mathbb{R}^3 \\rightarrow \\mathbb{R}$ which is a simple linear combination of the inputs: \n\n$$ h(\\mathbf{x}; \\beta) = \\begin{bmatrix} 1 & \\mathbf{x}^\\top \\end{bmatrix} \\begin{bmatrix}  \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3  \\end{bmatrix} = \\beta_0 + \\beta_1 \\alpha + \\beta_2 \\rho + \\beta_3 v $$\n\n**The Million Dollar Question:**\n\nGiven we know the structure of $h(\\mathbf{x}; \\beta)$, how can we efficiently optimize the parameters of $h(\\mathbf{x}; \\beta)$ to best approximate $f(\\mathbf{x})$? \n\n## Representing Observed Input-Output Data\n\nSuppose we have taken $N$ real-life samples of the input variables and their corresponding drag-force, $F_d$. Let each sample of the system under various operating conditions form row of a matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times 4}$ for the inputs and a matrix $\\mathbf{Y} \\in \\mathbb{R}^N$ for their corresponding outputs: \n\n$$ \\mathbf{X} = \\begin{bmatrix} 1 & \\alpha_1 & \\rho_1 & v_1 \\\\ 1 & \\alpha_2 & \\rho_2 & v_2 \\\\ & & \\vdots & \\\\ 1 & \\alpha_N & \\rho_N & v_N \\end{bmatrix} , \\mathbf{Y} = \\begin{bmatrix} F_{d1} \\\\ F_{d2} \\\\ \\vdots \\\\ F_{d3} \\end{bmatrix} $$ \n\n \nWe can efficiently compute the predictions of $h(\\mathbf{X}; \\beta)$ with a simple matrix-vector multiplication: \n\n$$ \\hat{\\mathbf{Y}} = \\mathbf{X} \\beta = \\begin{bmatrix} 1 & \\alpha_1 & \\rho_1 & v_1 \\\\ 1 & \\alpha_2 & \\rho_2 & v_2 \\\\ & & \\vdots & \\\\ 1 & \\alpha_N & \\rho_N & v_N \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{bmatrix} = \\begin{bmatrix}  \\beta_0 + \\beta_1 \\alpha_1 + \\beta_2 \\rho_1 + \\beta_3 v_1 \\\\ \\beta_0 + \\beta_1 \\alpha_2 + \\beta_2 \\rho_2 + \\beta_3 v_2  \\\\ \\vdots \\\\ \\beta_0 + \\beta_1 \\alpha_N + \\beta_2 \\rho_N + \\beta_3 v_N \\end{bmatrix}  \\in \\mathbb{R}^N $$ \n\n## The Least-Squares Optimization Problem\n\nOur goal is to adjust the parameters $\\beta$ so that our predictions, $\\hat{\\mathbf{Y}}$, are as close to the true outputs, $\\mathbf{Y}$ as possible. One way to measure prediction error is with Mean Squared Error: \n\n$$ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2 = \\frac{1}{N} || \\hat{\\mathbf{Y}} - \\mathbf{Y}||_2^2 = \\frac{1}{N} || \\mathbf{X} \\beta - \\mathbf{Y} ||_2^2 $$ \n\nthe $\\frac{1}{N}$ is only a scalar, we wish to choose the value of $\\beta$ that minimizes the following loss-function: \n\n$$ L(\\beta) = ||\\mathbf{X} \\beta - \\mathbf{Y}||_2^2 $$\n\n## Solving the Least-Squares Problem\n\nBecause the squared 2-Norm can be rewritten as an inner product, we can rewrite this loss-function as: \n\n$$ L(\\beta) = ||\\mathbf{X} \\beta - \\mathbf{Y}||_2^2= (\\mathbf{X} \\beta - \\mathbf{Y})^\\top (\\mathbf{X} \\beta - \\mathbf{Y}) $$\n$$ = (\\beta^\\top \\mathbf{X}^\\top - \\mathbf{Y}^\\top) (\\mathbf{X} \\beta - \\mathbf{Y}) $$\n\n$$ = \\beta^\\top \\mathbf{X^\\top X} \\beta - 2 \\beta^\\top \\mathbf{X^\\top \\mathbf{Y}} + \\mathbf{Y^\\top Y}$$\n\nAs we learned in multivariable calculus, to find a the extrema of a continuous function, we need to identify the critical points of the function. This means setting the gradient of the loss-function equal to the zero-vector:\n\n$$ \\nabla L_\\beta = 2 \\mathbf{X^\\top X} \\beta - 2 \\mathbf{X^\\top Y} = \\mathbf{0}$$ \n\nSolving this equation for $\\beta$ yields only one critical point: \n\n$$ \\hat{\\beta} = (\\mathbf{X^\\top X})^{-1} \\mathbf{X^\\top Y} $$ \n\nTo check whether this point is a local minimum, maximum, or neither, we examine the nature of the Hessian: \n\n$$ \\nabla^2 L_\\beta = 2 \\mathbf{X^\\top X} $$ \n\nBecause the Hessian is symmetric positive semidefinite and does not depend on $\\beta$, this means that $L(\\beta)$ is convex everywhere. Hence, $\\hat{\\beta}$ must be a global local minimum. \n\n## Experimental Example\n\nNow suppose we have 100 experimental observations of various angles of attack, air-densities and velocities with the corresponding drag-force in a pandas DataFrame called `df`: \n\n\n\n::: {#2311b25e .cell execution_count=2}\n``` {.python .cell-code}\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alpha</th>\n      <th>rho</th>\n      <th>velocity</th>\n      <th>f_drag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>16.854305</td>\n      <td>1.319114</td>\n      <td>79.481130</td>\n      <td>9425.204935</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>42.782144</td>\n      <td>1.203298</td>\n      <td>50.263709</td>\n      <td>13722.379248</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>32.939727</td>\n      <td>1.320528</td>\n      <td>57.690388</td>\n      <td>12863.441979</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26.939632</td>\n      <td>0.696729</td>\n      <td>49.251769</td>\n      <td>3644.076532</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.020839</td>\n      <td>1.227098</td>\n      <td>19.524299</td>\n      <td>186.612122</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe can create $\\mathbf{X}$ and $\\mathbf{Y}$ matrices by extracting the input features and outputs from the dataframe: \n\n::: {#fe54d5ff .cell execution_count=3}\n``` {.python .cell-code}\nX = np.hstack((np.ones((N, 1)), df[['alpha', 'rho', 'velocity']].values))\nY = df['f_drag'].values\n```\n:::\n\n\nWe can solve for $\\hat{\\beta}$ by computing the normal equations: \n\n::: {#a89aae28 .cell execution_count=4}\n``` {.python .cell-code}\nbeta_hat = np.linalg.inv(X.T @ X) @ (X.T @ Y)\n```\n:::\n\n\nAnd then compute the model's predictions at the training inputs: \n\n::: {#43b75c15 .cell execution_count=5}\n``` {.python .cell-code}\nY_hat = X @ beta_hat\n```\n:::\n\n\nNow let's plot the $\\hat{\\mathbf{Y}}$ and $\\mathbf{Y}$ to examine how closely correlated the two are. A straight line with slope-1 would mean the model exactly fitted all of the outputs: \n\n::: {#9ab75ab0 .cell execution_count=6}\n``` {.python .cell-code}\nplt.figure(figsize=(8,4))\nplt.scatter(Y, Y_hat)\nplt.grid()\nplt.xlabel(\"True Drag Force\")\nplt.ylabel(\"Model Predicted Drag Force\")\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nText(0, 0.5, 'Model Predicted Drag Force')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](11_ls_files/figure-html/cell-7-output-2.png){width=697 height=350}\n:::\n:::\n\n\nThere is clearly a strong, but nonlinear correlation between the true and model-predicted drag-force. This indicates that there is a nonlinear relationship between the features and the output values. Let us quantify the MSE: \n\n::: {#c8a7a32c .cell execution_count=7}\n``` {.python .cell-code}\nlinear_mse = np.linalg.norm(Y_hat - Y, 2) / N\nprint(\"Linear Features Mean-Squared Error: %.2f\" % (linear_mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Features Mean-Squared Error: 645.47\n```\n:::\n:::\n\n\n## Using Nonlinear Features to Improve Model Performance\n\nWe know the underlying formula for drag-force is: \n\n$$ F_d = \\frac{1}{2} \\rho v^2 C_d A  $$ \n\nWhile this function doesn't depend on the angle of attack, we do see how the features are polynomially related to one another. Naturally, it is going to be difficult for a linear combination of features to replicate this. However, if we take the log of both sides, something interesting happens: \n\n$$ \\ln(F_d) = \\ln(\\frac{1}{2} \\rho v^2 C_d A) = \\ln(\\frac{1}{2}) + \\ln(\\rho) + 2\\ln(v) + \\ln(C_d) + \\ln(A) $$\n\nWhen our features and outputs are log-scaled, we see a friendly linear-combination of log-scaled versions features emerge! Let's alter our inputs and outputs accordingly, so our problem becomes: \n\n$$ \\mathbf{X} = \\begin{bmatrix} 1 & \\ln \\alpha_1 & \\ln \\rho_1 & \\ln v_1 \\\\ 1 & \\ln \\alpha_2 & \\ln \\rho_2 & \\ln v_2 \\\\ & & \\vdots & \\\\ 1 & \\ln \\alpha_N & \\ln \\rho_N & \\ln v_N \\end{bmatrix} , \\mathbf{Y} = \\begin{bmatrix} \\ln F_{d1} \\\\ \\ln F_{d2} \\\\ \\vdots \\\\ \\ln F_{d3} \\end{bmatrix} $$ \n\nLet's see how this works programmatically: \n\n::: {#55aa5561 .cell execution_count=8}\n``` {.python .cell-code}\nX_log = np.hstack((np.ones((N, 1)), np.log(df[['alpha', 'rho', 'velocity']].values)))\nY_log = np.log(df['f_drag'].values - df['f_drag'].min() + 1)\n\nbeta_log = np.linalg.inv(X_log.T @ X_log) @ (X_log.T @ Y_log)\n\nY_hat_log = np.exp(X_log @ beta_log)+ df['f_drag'].min() - 1\n\nplt.figure(figsize=(8,4))\nplt.scatter(Y, Y_hat_log)\nplt.grid()\nplt.xlabel(\"True Drag Force\")\nplt.ylabel(\"Model Predicted Drag Force\")\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nText(0, 0.5, 'Model Predicted Drag Force')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](11_ls_files/figure-html/cell-9-output-2.png){width=686 height=350}\n:::\n:::\n\n\nNow let's see if our MSE has changed: \n\n::: {#170f5314 .cell execution_count=9}\n``` {.python .cell-code}\nlog_mse = np.linalg.norm(Y_hat_log - Y, 2)/N\nprint(\"Logarithmically Scaled Features Mean-Squared Error: %.2f\" % (log_mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogarithmically Scaled Features Mean-Squared Error: 144.70\n```\n:::\n:::\n\n\nWe have nearly a 5x reduction in MSE just by log-scaling our features! If we choose the right features, linear regression can be an extremely powerful method for approximating complex functions. \n\n---\n\n",
    "supporting": [
      "11_ls_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}