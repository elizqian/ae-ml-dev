---
title: "LLS and the singular value decomposition"
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"

---

{{< include _macros.qmd >}}

In this section we will introduce a new matrix decomposition called the Singular Value Decomposition (SVD). The SVD is generally applicable to all matrices of any dimension or rank, and has many applications in machine learning and beyond. We will begin by defining the decomposition and then discuss some of it uses.

The intended learning outcomes of these notes are that students should be able to:


## SVD: the basics

The SVD decomposes the matrix $X\in\R^{N\times n}$ into the product of three matrices, $U\in\R^{N\times N}$, $\Sigma\in\R^{N \times n}$, and $V^\top \in\R^{n\times n}$, so that 

$$
X = U\Sigma V^\top
$$ {#eq-svd-product}

The matrices $U$ and $V$ are orthogonal (unitary), so that $U^\top U = I_N$ and $V^\top V = I_n$, and the matrix $\Sigma$ is diagonal. The diagonal entries of $\Sigma$ are denoted $\sigma_1,\sigma_2,\ldots,$ and are called the *singular values* of $X$, and are always non-negative and in non-increasing order, that is: $\sigma_1\geq \sigma_2\geq \cdots \geq 0$.  $\Sigma$ is called the singular value matrix, and the number of nonzero singular values is equal to the rank of the matrix $X$. The matrix $U$ is called the left singular vector matrix, and its columns are called the *left singular vectors*. The matrix $V$ is called the right singular vector matrix, and its columns are called the *right singular vectors*. 

<!-- pseudocode in the future? -->

To compute the SVD in python, you would use <tt>numpy.linalg.svd</tt> or <tt>scipy.linalg.svd</tt>. Like other matrix decompositions, the time complexity to compute the SVD is third-order, specifically $\mathcal{O}(\min(Nn^2, N^2n))$. (Note that when $N>n$ this is the same time complexity as the QR decomposition for tall matrices.) The storage complexity is $\mathcal{O}(\max(N,n)^2)$.

Above, we have described the *full SVD*. Another version of the SVD is the *reduced SVD* or the *thin SVD*[^1]. 
Let $k = \min(N,n)$. The reduced SVD decomposes $X$ into the product $X=U\Sigma V^\top$ where $U\in\R^{N\times k}$, $\Sigma\in\R^{k\times k}$, and $V\in\R^{n\times k}$. The matrices $U$ and $V$ are still unitary, so that $U^\top U = I_k$ and $V^\top V = I_k$ and $\Sigma$ is square and diagonal. When one of the dimensions of $X$ is much larger than the other, this can lead to signficant cost savings in both time and memory. Note that the reduced SVD is subtly different from the truncated SVD which we describe next. 

[^1]: As with many of the terms we have encountered and will encounter in this course, the term "SVD" is used in the wild to mean either the full SVD or the reduced SVD -- often the distinction is not super important, but when it is you need to be able to discern from context what is meant.

<!-- add eigenvalue decomposition of square product? -->

<!-- add the operator interpretation? rotation/stretching/scaling -->

## The truncated SVD

Let $k=\min(N,n)$ as before. Then, we can rewrite @eq-svd-product as follows:

$$
X = \sum_{i=1}^k u_i \sigma_i v_i^\top = \sum_{i=1}^k \sigma_i (u_i v_i^\top)
$$ {#eq-svd-sum}

where $u_1,\ldots,u_N\in\R^N$ are the left singular vectors (columns of $U$) and $v_1,\ldots,v_n\in\R^n$ are the right singular vectors (columns of $V$), and $\sigma_i$ are again the diagonal entries in $\Sigma$. The matrices $u_1v_1^\top,\ldots,u_kv_k^\top\in\R^{N\times n}$ are rank-1 matrices formed by taking the *outer product* of $u_i$ with $v_i$.[^2] 
Recall that the number of nonzero singular values is equal to the matrix rank and let $r = \textsf{rank}(X)$. Thus, we can also write

$$
X = \sum_{i=1}^{r} \sigma_i (u_i v_i^\top)
$$ {#eq-svd-sum-rank}

[^2]: Recall that the rank of a matrix describes the number of linearly independent rows/columns of the matrix; to see that $ab^\top$ has rank 1 when $a,b$ are vectors, note that the $j$th row of $ab^\top$ is given by $a_jb^\top$, where $a_j\in\R$ is the $j$th element of $a$, so that $ab^\top$ is just the same row multiplied by different constants. 


We say a matrix is *low rank* when its rank is less than its maximum possible rank $k$. A low-rank matrix can be stored more efficiently than a full rank matrix: note that $X\in\R^{N\times n}$ generally has $Nn$ elements that must be stored, but if the the matrix has rank $r$, then from @eq-svd-sum-rank you can see that it would be more efficient to store $r$ sets of $(\sigma_i, u_i, v_i)$, for a total of $r(N+n+1)$ values. (In fact if storage is your only goal, you can just store the product $\sigma_iu_i$ and ditch that last 1 in the parentheses.) Low-rank matrices are also more efficient to compute with: while a standard matrix vector multiplication $X\beta$ where $\beta\in\R^n$ costs $\mathcal{O}(Nn)$ FLOPs, we can do a matrix vector multiplication using the expression in @eq-svd-sum-rank in just $\mathcal{O}(r(N+n))$ FLOPs (see Exercises).

Because low-rank matrices are more efficient to work with and to store, it is often useful to *approximate* a full-rank matrix with a low-rank matrix (or approximate a low-rank matrix with an even lower-rank matrix). By far the most common way to do this is using the **truncated SVD**, which just truncates the sum @eq-svd-sum at some $\ell < k$, as follows:

$$
X_\ell = \sum_{i=1}^\ell \sigma_i(u_iv_i^\top) = U_\ell \Sigma_\ell V_\ell^\top
$$

where $U_\ell\in\R^{N\times \ell}$ contains the left-most $\ell$ columns of $U$, and $V_\ell\in\R^{n\times \ell}$ contains the left-most $\ell$ columns of $V$, and $\Sigma_\ell$ contains the upper-left $\ell$-by-$\ell$ block of $\Sigma$ and thus has $\ell$ diagonal entries. The truncated SVD $X_\ell$ is the best rank-$\ell$ approximation of the matrix $X$ --- this is a result called the Eckart-Young Theorem:

**Theorem (Eckart-Young).** *Let $X\in\R^{N\times n}$ and let $\mathcal{M}_\ell$ denote the class of matrices in $\R^{N\times n}$ with rank equal to or less than $\ell$. Then, *

$$
X_\ell = \arg\min_{M_\ell \in\mathcal{M}_\ell} ||X - M_\ell||_F^2
$$ {#eq-eckart-young}

That is, the truncated SVD is the rank-$\ell$ matrix that is closest to the original matrix $X$. Above, we have stated the theorem in the *Frobenius* norm[^3], but it also holds for the matrix $2$-norm. 
This optimality result is the reason the SVD is one of the most powerful and broadly used matrix decompositions there is. The minimum of @eq-eckart-young gives the error incurred by the approximation, and is given by what's called the "tail sum" of the square of the singular values, that is,

$$
\min_{M_\ell \in\mathcal{M}_\ell} ||X - M_\ell||_F^2 = \|X_\ell - X||_F^2 = \sum_{i=\ell+1}^k \sigma_i^2
$$

[^3]: the Frobenius norm of a matrix $A$, denoted $\|A\|_F$, is the natural generalization of the Euclidean norm from vectors to matrices -- it is given by the square root of the sum of squares of all matrix elements.

### Applications of the truncated SVD in scientific machine learning

One application of the SVD is for data compression. If you have a very large data set $Z\in\R^{N\times d}$ with either/both $N,d$, very large, you can quickly have too much data to store easily on whatever machine is available to you. This is a common challenge in scientific simulation of partial differential equations (e.g., in computational fluid dynamics or finite element analysis of structures) where $d$ generally scales with the size of the computational mesh, which can easily have hundreds of millions of cells. Instead of storing the full data matrix, we can store the singular vectors and values needed to represent the truncated SVD. A similar idea is used to compress images for efficient storage. 


<!-- add the idea of PCA features, add something about choosing the rank of compression? -->


## the pseudo inverse and solving least squares problems
plus truncated SVD regularization?

## utility in understanding numerical conditioning

next ste of notes: machine precision, round off error, SVD and conditioning.


python examples showing nonzero residual for LU and conditioning error for QR?


introduce the SVD in math, singular values vs rank/conditioning, explain the concept of conditioning in terms of being essentially non-invertible from the computer's point of view, cond/rank commands

show that normal equations are terribly conditioned.

## Exercises 

1. Show that @eq-svd-product and @eq-svd-sum are equivalent (nothing fancy, just write it out using what you know about the structure of $U$, $\Sigma$, and $V$).

2. Let $X\in\R^{N\times n}$ have rank $r$ and let $\beta\in\R^n$. Show that the matrix multiplication $X\beta$ can be computed using the expression @eq-svd-sum-rank in $\mathcal{O}(r(N+n))$ FLOPs.