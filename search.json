[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AE 4803 AIM: Course Notes",
    "section": "",
    "text": "Preface\nThis is the class website where all relevant notes and class information will be stored for AE4803 AIM. To get started, click the Introduction button to the left.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "1¬† Introduction",
    "section": "",
    "text": "1.1 What‚Äôs in a name? Artificial intelligence, machine learning, data science, and the scope of this course.\nI googled ‚Äúartificial intelligence‚Äù (often abbreviated AI) and the first result comes from Google‚Äôs new ‚ÄúAI overview‚Äù feature1:\nBecause I closed the tab with my first search and wanted to go back to the webpage to copy the text for the image alt-text above, I repeated my search, and got a somewhat different answer:\nThese are probably both reasonable definitions for casual conversation, but that‚Äôs not what we‚Äôre here for. Instead, we‚Äôre here to really learn deeply about what AI is, and for that we‚Äôre going to need precision ‚Äì that is, of our definitions.\nI‚Äôm not going to provide a definition of AI for now and instead I‚Äôm going to throw two more terms into the mix that you may have heard. The first is ‚Äúmachine learning‚Äù (often abbreviated ML). Rather than get an AI definition for this too, I decided to go to the dictionary Merriam-Webster, which provides the following primary definition (Merriam-Webster Dictionary 2024):\nAnd finally I‚Äôll add the term ‚Äúdata science‚Äù (curiously, rarely abbreviated DS). I wanted to give you a Merriam-Webster definition here, but the term isn‚Äôt in their dictionary as of writing this on October 28, 2024. So instead I‚Äôm going to use Cambridge Dictionary‚Äôs definition (Cambridge Dictionary 2024):\nClearly, different people/entities may have different ideas of what AI, ML, and data science may mean. Some people may use these terms to describe generative tools like ChatGPT, GitHub copilot, and Dall-E (or similar products developed by other entities). Others use these terms to refer to more purpose-built algorithms like AlphaGo (for playing Go) or GraphCast (for weather prediction).2 Many academics use these terms to describe the study of the underlying mathematical and programming ideas on which such products are built.\nMy goal is not to give you a single definition of any of these terms and then to argue that my definition is more correct than any other definition. The point I want to make is that it‚Äôs worth being clear about how we define these terms in any given context, whether it be in a textbook, a news article, or perhaps a spirited discussion between friends. To that end, I now want to make clear what it is that we will and will not cover in this class, which is a ‚Äúfoundations‚Äù-level course in the College of Engineering‚Äôs AI minor.\nThe focus of this class will be the mathematical and programming foundations of scientific machine learning, which I define as the study of algorithms which use scientific data to define computational tools that perform useful scientific tasks. The main scientific task that we will focus on in this course is the task of predictive simulation, which seeks to predict the behavior or outcomes of scientific or engineering systems. Predictive simulation is a key task in engineering disciplines like design and control, where we seek to predict design outcomes and control responses in order to make decisions about design parameters and control inputs. The class of machine learning methods that we will focus on in this class will therefore be regression methods, which use data to define relationships between inputs and outputs that allow us to take a specified input and issue a prediction for the output.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#whats-in-a-name-artificial-intelligence-machine-learning-data-science-and-the-scope-of-this-course.",
    "href": "01_intro.html#whats-in-a-name-artificial-intelligence-machine-learning-data-science-and-the-scope-of-this-course.",
    "title": "1¬† Introduction",
    "section": "",
    "text": "Exercise\n\n\n\nConsider the two different definitions of ‚ÄúAI‚Äù above carefully. In what (if any) senses are they (a) exactly the same, (b) similar, (c) somewhat different, (d) very different? What consequences might these differences have (a) developing AI algorithms, (b) evaluating AI impacts, (c) creating AI policies?\n\n\n\n\na computational method that is a subfield of artificial intelligence and that enables a computer to learn to perform tasks by analyzing a large dataset without being explicitly programmed\n\n\n\nthe use of scientific methods to obtain useful information from computer data, especially large amounts of data\n\n\n\n\n\n\n\nExercise\n\n\n\nFor both the terms ‚Äúmachine learning‚Äù and ‚Äúdata science‚Äù, find an alternative definition from a source that is not a generative AI. What differences exist between the new definitions you‚Äôve found and the ones I‚Äôve cited above?",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#regression-methods-an-overview",
    "href": "01_intro.html#regression-methods-an-overview",
    "title": "1¬† Introduction",
    "section": "1.2 Regression methods: an overview",
    "text": "1.2 Regression methods: an overview\n\n1.2.1 Motivation and problem description\nWe use the notation \\(z\\in \\mathbb{R}^d\\) to denote a real-valued vector of \\(d\\) input variables, and the notation \\(y \\in\\mathbb{R}\\) to denote a real-valued scalar output variable. Our goal is to be able to predict the value of \\(y\\) if we know the value(s) of the input variable \\(z\\). Some examples:\n\nin aerodynamic modeling, \\(z\\) could contain airfoil geometry parameters like camber and thickness, and \\(y\\) could represent the lift coefficient. Being able to predict \\(y\\) from \\(z\\) enables engineers to choose more aerodynamically efficient designs.\nin orbital dynamics, \\(z\\) could represent orbital parameters like altitude and inclination, and \\(y\\) could represent the total time a satellite spends outside the sun‚Äôs shadow in a given time period. Being able to predict \\(y\\) from \\(z\\) enables engineers to determine if a satellite‚Äôs solar panels will generate enough power to support the satellite‚Äôs mission.\nin chemical process engineering, \\(z\\) could represent conditions within a reactor like temperature and chemical mixture properties and \\(y\\) could represent the reaction rate. Being able to predict \\(y\\) from \\(z\\) enables engineers to design more efficient reactors.\n\n\n\n\n\n\n\nExercise\n\n\n\nPropose your own example scenario where it would be useful to predict real-valued outputs from inputs, drawing on your own experience, e.g.¬†in personal projects, previous coursework, work/internship/co-op experiences, or extracurriculars. What would \\(z\\) and \\(y\\) represent? What does predicting \\(y\\) from \\(z\\) enable in your scenario?\n\n\nMathematically, predicting \\(y\\) from \\(z\\) amounts to defining a function \\(f\\) that takes in a \\(d\\)-dimensional input and outputs a scalar. Mathematical notational shorthand for this is \\(f:\\mathbb{R}^d\\to\\mathbb{R}\\). This function \\(f\\) is often called a model. The question at hand is: how do we choose \\(f\\)? There are many ways to do so:\n\nAt a baseline, you could just make up a model: let‚Äôs just say \\(y = f(z) = \\|z\\|^2\\). This is mathematically valid, but it‚Äôs probably a bad model for the three example scenarios above, because this model probably issues predictions that are very different from the true outputs.\nAlternatively, you could develop a model based on physical principles (a ‚Äúphysics-based‚Äù model) ‚Äì if you have taken classes in (aero/thermo)dynamics, then you would have learned some ways to calculate \\(y\\) from \\(z\\) in the above examples, e.g.¬†based on potential flow theory, rigid body dynamics, or the Arrhenius equation. These models are likely to be more accurate than our made-up model above, although they are often imperfectly accurate because they make simplifying assumptions. One drawback of physics-based models is that they may be computationally expensive (some computational fluid dynamics simulations require many thousands of hours of supercomputing time). Additionally, fully physics-based models may not even exist in some applications (e.g., there are many aspects of plasma physics for which we currently lack a complete theoretical understanding).\nFinally, our focus will be on using data to define a model (a ‚Äúdata-driven‚Äù model or a ‚Äúlearned‚Äù model). We assume that we have a data set consisting of \\(N\\) pairs of input and output data, \\((z_i,y_i)\\) for \\(i = 1,\\ldots,N\\). To define our model, we want to choose an \\(f\\) so that the predicted output \\(f(z_i)\\) is close to the output data \\(y_i\\) for all the data in our data set. This is sometimes called ‚Äúfitting‚Äù the model to data. The advantages of data-driven models are that they may be significantly cheaper than physics-based models, and they can be fit to experimental data even when we lack scientific theory for developing physics-based models. The disadvantages are that data-driven models require data ‚Äì and the amount of data that would be ‚Äúenough‚Äù to ensure that the learned model is accurate or useful is highly dependent on the application.\n\nLearning how to fit models to data and assess their accuracy and usefulness is the focus of this course.\n\n\n1.2.2 Mathematical setting and problem formulation\nHot take: Machine learning methods, at their core, are simply very complicated calculators.\nOur goal is to understand the calculations that these methods are carrying out, which is going to enable us to understand and assess both the successes and failures of the methods. This means we want to be precise about the mathematical problem formulation, that is, the characterization of the math problem that is being solved by the methods.\nFormulating a regression problem requires three ingredients:\n\nPaired input and output data: let \\(N\\) be the number of pairs, and let \\(z_1,\\ldots,z_N\\) denote the \\(N\\) input data and \\(y_1,\\ldots,y_N\\) denote the \\(N\\) corresponding outputs. It is common to notate the set of all these data pairs as \\(\\{(z_i,y_i)\\}_{i=1}^N\\).\nThe definition of a parametrized model class ‚Äì more details on this in a moment, but you should think of this as a set of possible functions that map \\(z\\) to \\(y\\).\nA method for choosing a model from within the chosen model class ‚Äì in regression problems this is most frequently done by solving an appropriate optimization problem to pick the ‚Äúbest‚Äù possible model from within the class.\n\nLet‚Äôs focus on the second ingredient, the parametrized model class. We use the mathematical term ‚Äúclass‚Äù in a similar way to the mathematical term ‚Äúset‚Äù. For example, perhaps you will have heard before that the numbers \\(1,2,3,\\ldots\\) all the way to infinity form the ‚Äúset of natural numbers‚Äù (often denoted \\(\\mathbb{N}\\)). As another example, \\(\\{H,T\\}\\) can denote the set of possible outcomes of flipping a two-sided coin (H for heads, T for tails). Note that it is common to use curly braces \\(\\{\\cdot\\}\\) to enclose a set (notice where it appears in our shorthand notation for the data set!). Similarly, we can define a set of possible functions that map \\(z\\) to \\(y\\), for example (assuming \\(d=1\\) so that \\(z\\) is a scalar for the moment):\n\\[\n\\mathcal{F}:= \\{z^2, \\sin(z), \\exp(z)\\}\n\\]\nIn the above expression, the notation \\(:=\\) indicates that \\(\\mathcal{F}\\) is being defined as the expression that follows \\(:=\\), that is, \\(\\mathcal{F}\\) is the set of three possible functions: \\(f(z)=z^2\\), \\(f(z)=\\sin(z)\\), and \\(f(z)=\\exp(z)\\). You see that it might be cumbersome to write the ‚Äú\\(f(z)=\\)‚Äù part of every function, so that part often gets dropped.\nBut so far I have still only been saying ‚Äúset‚Äù ‚Äî what distinguishes a ‚Äúclass‚Äù of potential models from a ‚Äúset‚Äù of potential models? Usually we use the term ‚Äúclass‚Äù to denote a set of functions that share certain properties or functional forms, for example (still assuming \\(z\\) is scalar for the moment) the class of all quadratic functions:\n\\[\n\\{c_0 + c_1 z + c_2 z^2 : c_0,c_1,c_2\\in\\mathbb{R}\\}\n\\qquad(1.1)\\]\nThe way to read this notation above is as the set of all functions of the form \\(f(z) = c_0 + c_1 z + c_2 z^2\\) that can be obtained when \\(c_0,c_1,c_2\\) are allowed to take on any real values.3 We call this the class of all quadratic functions because by varying \\(c_0,c_1,c_2\\) we always get a quadratic function (noting that with our mathematics hats on, linear functions are just special cases of quadratic functions where the second-order coefficient is zero). Thus, functions within the class share a functional form, unlike the set \\(\\mathcal{F}\\) we defined above. However, we could use the functions in the set \\(\\mathcal{F}\\) to construct an alternative model class:\n\\[\n\\{a z^2 + b\\sin(z) + c\\exp(z) : a,b,c\\in\\mathbb{R}\\}\n\\qquad(1.2)\\]\nor, allowing \\(z\\) to be a \\(d\\)-dimensional vector again, we could consider the following model class:\n\\[\n\\{\\max(0,a^\\top (Wz + b)): W\\in\\mathbb{R}^{d\\times d}, \\,a,b\\in\\mathbb{R}^d\\}\n\\qquad(1.3)\\]\n\n\n\n\n\n\nExercise\n\n\n\nHow would you read and understand the two classes of functions notated above?\n\n\nNotice that the notation we have been using to define function classes follows a pattern: within the brackets, before the colon comes the form of the function, followed by a list of some variables that appear in the function definition, together with a specification of what values those variables are allowed to take on. We call the post-colon variables parameters, and the set of all values that the parameters are allowed to be chosen from is called the parameter space. Any of the classes of functions we have defined above can be described as a parametrized model class, because they define sets of functions with a shared form, where the individual functions within the set arise from varying the values of the parameters.\nLet me describe some common notational conventions that I want you to be familiar with.45 It is common to use a single lowercase letter (usually a Greek one) to denote the set of all parameters ‚Äì e.g., \\(\\beta = \\{c_0,c_1,c_2\\}\\) or \\(\\theta = \\{a,b,W\\}\\), and the corresponding capital letter to denote the corresponding parameter space, i.e., \\(B = \\mathbb{R}^3\\) and \\(\\Theta = \\mathbb{R}\\times \\mathbb{R}\\times \\mathbb{R}^{d\\times d}\\). Where we might write \\(y = f(z)\\) for a general abstract model \\(f\\), we will typically write \\(y = f(z;\\beta)\\) or \\(y = f(z;\\theta)\\) to indicate a model parametrized by \\(\\beta\\) or \\(\\theta\\) (note that we separate inputs from parameters using a semicolon).\n\nIf you‚Äôve heard the term ‚Äúmodel architecture‚Äù used to describe a machine learning model, what this means is the specification of the parametrized model class. As the term ‚Äúarchitecture‚Äù suggests, choosing the model class is a design choice, and different model classes have varying advantages and disadvantages in terms of best possible performance, difficulty to train (more on that shortly), computational cost, and more. We‚Äôll cover many possible choices of model class, and explore their pros and cons, throughout the course.\nNow we will turn our attention to the third ingredient, the method of selecting a model from within the chosen class. Because parametrized models are defined by their parameters, the choice of model amounts to choosing the parameter values. Again, you could just make up the parameter values (choose your favorite numbers), but this is unlikely to match the data well and unlikely to yield accurate predictions. In some cases you could also derive the parameter values based on physical principles (this is possible when models in the chosen class have some physical interpretation). But our focus will be on choosing parameters that define a model that ‚Äúbest matches‚Äù the available data. A basic and standard version of this parameter selection task is to solve the following minimization problem:\n\\[\n\\theta^* = \\arg\\min_{\\theta\\in\\Theta}\\frac 1N \\sum_{i=1}^N (f(z_i;\\theta)-y_i)^2\n\\]\nNote that for the above expression to be well-defined, we need our first two ingredients ‚Äì that is, we need our data set, and we need to have already chosen the parametrized model class, which gives us an expression for the parametrized function \\(f(z;\\theta)\\). Once we have those ingredients, this expression defines \\(\\theta^*\\) as the parameter that minimizes the average square error of the learned model over the available data. In this sense, this parameter \\(\\theta^*\\) defines a learned model \\(f(z;\\theta^*)\\) that ‚Äúbest fits‚Äù the data (it is better than all other models in its class).\nAs we progress through this course, we will learn more about how we can solve the above optimization (it is different for different model architectures), and about alternative ways to select the parameter based on alternative minimization problems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#a-foundation-for-learning-about-ml-beyond-parametrized-regression",
    "href": "01_intro.html#a-foundation-for-learning-about-ml-beyond-parametrized-regression",
    "title": "1¬† Introduction",
    "section": "1.3 A foundation for learning about ML beyond parametrized regression",
    "text": "1.3 A foundation for learning about ML beyond parametrized regression\nI want to emphasize that this class is not and indeed cannot (realistically) be an exhaustive introduction to machine learning, artificial intelligence, or data science. Even the community of professionals who describe themselves as working in ‚Äúscientific machine learning‚Äù is quite broad in scope and would include folks working on topics that I do not intend to cover in this class. There are many topics within AI/ML/data science we will not cover (beyond perhaps at most a very surface level), including generative modeling, decision trees, unsupervised learning methods like clustering, and many more. The philosophy of this course is to empower you to learn about these other methods through future coursework or independent study, by providing a foundation in what I term the `three pillars of artificial intelligence‚Äô:\n\nMathematics: all machine learning methods are fundamentally solving mathematical problems at their core. The first pillar of AI is to be able to mathematically define the problem one seeks to solve.\nComputation: once a mathematical problem is defined, an ML method must define an algorithm that carries out the calculations to solve the problem. The second pillar of AI is to define an algorithm that (approximately) solves the mathematical problem.\nHuman intelligence: this is the crucial third pillar of AI. Human intelligence is vital for defining mathematical problem formulations, designing algorithms, and assessing the results and iteratively updating the mathematical problem formulation and computational algorithm to achieve desired outcomes.\n\nWe will seek to develop a firm foundation in these three pillars of AI throughout the course.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#course-structure-and-intended-learning-outcomes",
    "href": "01_intro.html#course-structure-and-intended-learning-outcomes",
    "title": "1¬† Introduction",
    "section": "1.4 Course structure and intended learning outcomes",
    "text": "1.4 Course structure and intended learning outcomes\n\nlearning outcomes\nproblem solving review - meant to develop your skills in demonstrating human intelligence\n\n\n\n\nCambridge Dictionary. 2024. ‚ÄúData Science.‚Äù 2024. https://dictionary.cambridge.org/dictionary/english/data-science.\n\n\nMerriam-Webster Dictionary. 2024. ‚ÄúMachine Learning.‚Äù 2024. https://www.merriam-webster.com/dictionary/machine%20learning.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#footnotes",
    "href": "01_intro.html#footnotes",
    "title": "1¬† Introduction",
    "section": "",
    "text": "It seems apt to use AI to define AI, and I‚Äôm quoting it for illustrative purposes here, but I do want to point out that Google‚Äôs AI Overview is not at all transparent about how these responses are generated and thus does not meet the standards for being cited as a source in most publishing venues.‚Ü©Ô∏é\nthis paragraph lists examples that came to me most quickly and thus reflects to some extent my own cognitive biases based on the media I‚Äôve consumed. I welcome suggestions of other examples of AI/ML/data science to include that are less well-known ‚Äì Email me!‚Ü©Ô∏é\nFollowing the same notational convention, we could write the data set as \\(\\{(z_i,y_i) : i =1,\\ldots,N\\}\\), which we would read as the set of all pairs \\((z_i,y_i)\\) for \\(i = 1,\\ldots,N\\), but the notation \\(\\{(z_i,y_i)\\}_{i=1}^N\\) is more compact and more common. I don‚Äôt make the rules. ü§∑‚Äç‚ôÄÔ∏è‚Ü©Ô∏é\nThese are common conventions, but the ML community is comprised of a diverse group of scientists and engineers across disciplines, and it‚Äôs very common to read things that use entirely different notation. This is unfortunately the reality of things, and one of the reasons I am going to push you in this class to be comfortable describing the underlying concepts precisely using different notations - that is, you are going to develop flexibility in re-assigning meaning to different symbols as we go along (I hope).‚Ü©Ô∏é\nI‚Äôm making an effort to explicitly define notation when I introduce it, and to give examples of how to read and understand it, but it‚Äôs easy for me to miss things, so please speak up or write us a message if you have a question.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "11_ls.html",
    "href": "11_ls.html",
    "title": "2¬† Linear Least Squares Problems",
    "section": "",
    "text": "2.1 Classifying regression problems as linear vs nonlinear\nWe have previously introduced regression problems in a general way: recall that the three ingredients of (parametrized) regression problems are (1) paired input and output data, (2) the choice of a parametrized model class, and (3) a method for choosing a model from within that class. The classification of a regression problem as linear or nonlinear depends solely on ingredient (2), the parametrized model class: if the models in that class depend linearly on the model parameters, the regression problem is a linear regression problem.\nThe classification of regression problems as linear or nonlinear depends solely on the dependence of the functions in the parametrized model class on the parameters. That is, we can define functions that are linear in \\(\\theta\\) while being nonlinear in \\(z\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "11_ls.html#classifying-regression-problems-as-linear-vs-nonlinear",
    "href": "11_ls.html#classifying-regression-problems-as-linear-vs-nonlinear",
    "title": "2¬† Linear Least Squares Problems",
    "section": "",
    "text": "Check your knowledge: Do you remember what it means for a function to depend linearly on a variable?\n\n\n\n\n\nThe function \\(f(z;\\theta): \\mathbb{R}^d\\times\\Theta\\to\\mathbb{R}\\) is said to be linear in the parameters \\(\\theta\\) if, for all \\(a,b\\in\\mathbb{R}\\) and all \\(\\theta_1,\\theta_2\\in\\Theta\\), the following holds: \\(f(z; a\\theta_1 + b\\theta_2) = af(z;\\theta_1) + bf(z;\\theta_2)\\).\nNote that when we say a function is ‚Äúlinear‚Äù, we have to specify in what. That is, we can also say \\(f(z;\\theta)\\) is linear in the inputs if, for all \\(a,b\\in\\mathbb{R}\\) and all \\(z_1,z_2\\in\\mathbb{R}^d\\), the following holds: \\(f(az_1 + bz_2;\\theta) = af(z_1;\\theta)+bf(z_2;\\theta)\\).\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nConsider the model classes (Equation¬†1.1)-(Equation¬†1.3) introduced previously. Are these model classes linear or nonlinear in the parameters? In the inputs?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "11_ls.html#mathematical-problem-formulation",
    "href": "11_ls.html#mathematical-problem-formulation",
    "title": "2¬† Linear Least Squares Problems",
    "section": "2.2 Mathematical problem formulation",
    "text": "2.2 Mathematical problem formulation\nWe are now going to introduce an abstract mathematical problem formulation that can be used to describe many specific instances of linear regression problems. This is a theme of the course and throughout computational mathematics and engineering: abstraction using the language of mathematics lets us isolate the core essence of the problem we‚Äôre solving and develop powerful algorithms that can solve specific applications of those problems across a wide range of disciplines. I‚Äôll introduce the abstract formulation first, and follow it up with some specific examples.\nIngredient 1 (the data set): let \\(\\{(z_i,y_i)\\}_{i=1}^N\\) be a given data set of paired inputs \\(z_i\\in\\mathbb{R}^d\\) and outputs \\(y_i\\in\\mathbb{R}\\).\nIngredient 2 (the parametrized model class): let \\(x:\\mathbb{R}^d\\to\\mathbb{R}^n\\) be a function that maps the \\(d\\)-dimensional input to an \\(n\\)-dimensional feature vector. For a fixed \\(x\\), we will consider the following parametrized model class:\n\\[\n\\mathcal{F}_\\beta := \\{ x(z)^\\top \\beta : \\beta\\in\\mathbb{R}^n\\}\n\\qquad(2.1)\\]\nRecall that Equation¬†2.1 is read as ‚Äú\\(\\mathcal{F}_\\beta\\) is defined to be the set of all functions \\(f(z;\\beta) = x(z)^\\top\\beta\\) for all \\(\\beta\\in\\mathbb{R}^n\\).‚Äù This is an abstract way to define the model class for any linear regression problem, as we will describe in more detail shortly.\nIngredient 3 (the method of choosing the parameters): let \\(\\beta^*\\) be given by\n\\[\n\\begin{aligned}\n\\beta^* &= \\arg\\min_{\\beta\\in\\mathbb{R}^n} \\frac1N \\sum_{i=1}^N (f(z_i;\\beta) - y_i)^2 \\\\\n&= \\arg\\min_{\\beta\\in\\mathbb{R}^N} \\frac1N \\sum_{i=1}^N (x(z_i)^\\top\\beta - y_i)^2.\n\\end{aligned}\n\\qquad(2.2)\\] Then, we define the learned model to be \\(f(z;\\beta^*) = x(z)^\\top\\beta^*\\). We call \\(\\beta^*\\) defined this way the ‚Äúoptimal regression parameters‚Äù or just the ‚Äúoptimal parameters‚Äù, because they are the result of solving an optimization problem. Note that the objective of this optimization function is defined by the data, and represents the average squared error of the model over the data set.\nTaken together, the three ingredients I have defined above define a linear least squares problem, which is a subclass of linear regression problems. We‚Äôll now give several examples of specific instances of linear least squares problems to illustrate how broadly applicable this abstract framework is.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "11_ls.html#examples",
    "href": "11_ls.html#examples",
    "title": "2¬† Linear Least Squares Problems",
    "section": "2.3 Examples",
    "text": "2.3 Examples\n\n2.3.1 Example 1: Aerodynamic drag prediction\n\nAn important task in aerodynamic design is predicting lift and drag forces on a body moving through air. Let‚Äôs consider a simplified problem where we are given an fixed airfoil design and our goal is to predict the drag force \\(F_d\\) on the airfoil as a function of three parameters which describe its flight conditions:\n\nThe angle of attack \\(\\alpha\\)\nThe density of the fluid \\(\\rho\\)\nThe freestream velocity of the air \\(v\\)\n\nTo put this problem in our abstract framework, we define \\(z = (\\alpha,\\rho,v)^\\top\\) to be a three-dimensional input, and take the output to be the drag force \\(y= F_d\\). In order to define a regression problem, we require the existence of a data set \\(\\{(z_i,y_i)\\}_{i=1}^N\\). Note that in this case \\(z_i = (\\alpha_i,\\rho_i,v_i)\\). We assume that this data set is given.\nIn linear regression problems, defining the model class amounts to choosing a set of regression features by defining \\(x\\). A simple choice takes the inputs themselves to be features: \\(x^{(1)}(z) = z = (\\alpha,\\rho,v)^\\top\\). This leads to a class of parametrized models with a three-dimensional unknown parameter vector \\(\\beta\\in\\mathbb{R}^3\\). The models in this class have the following form:\n\\[\nf^{(1)}(z;\\beta) = x^{(1)}(z)^\\top\\beta = (\\alpha,\\rho,v) \\beta = \\beta_1\\alpha + \\beta_2\\rho + \\beta_3 v.\n\\]\nThere are many other possible choices. For example, consider \\(x^{(2)}(z) = (\\alpha,\\rho, v, \\alpha^2, \\rho^2, v^2, \\alpha\\rho, \\alpha v, \\rho v)^\\top\\). This leads to a parametrized model class with a nine-dimensional unknown parameter vector \\(\\beta\\in\\mathbb{R}^9\\):\n\\[\n\\begin{aligned}\nf^{(2)}(z;\\beta) &= x^{(2)}(z)^\\top\\beta = (\\alpha,\\rho, v, \\alpha^2, \\rho^2, v^2, \\alpha\\rho, \\alpha v, \\rho v)\\beta \\\\\n&= \\beta_1\\alpha + \\beta_2\\rho + \\beta_3 v + \\beta_4\\alpha^2 + \\beta_5\\rho^2 + \\beta_6 v^2 + \\beta_7\\alpha\\rho + \\beta_8\\alpha v + \\beta_9 \\rho v\n\\end{aligned}\n\\]\nFor either the above choices of features \\(x^{(1)}(z)\\) or \\(x^{(2)}(z)\\), we could then define and solve the minimization Equation¬†2.2 to find the optimal regression parameters and define our learned model \\(f^{(1)}(z;\\beta^*)\\) or \\(f^{(2)}(z;\\beta^*)\\).\n\n\n\n\n\n\nUnderstanding notation\n\n\n\nNote that we use \\(\\beta\\) to denote the unknown parameters in both \\(f^{(1)}\\) and \\(f^{(2)}\\) above despite \\(\\beta\\) referring to different quantities in the definition of the different functions. This is a common notational shortcut ‚Äî while we could use the notation \\(\\beta^{(1)}\\in\\mathbb{R}^3\\) and \\(\\beta^{(2)}\\in\\mathbb{R}^9\\) to specify the different \\(\\beta\\) for the different functions, this can be cumbersome if we are considering many different options for the choice of features \\(x(z)\\), and it‚Äôs standard to just use \\(\\beta\\), where the definition of \\(\\beta\\) is implied by the context. One of the challenges in learning about machine learning and computational mathematics more generally is getting used to similar notation meaning different things in different contexts. That‚Äôs one of the things that we‚Äôll practice in this course.\n\n\n\n\n2.3.2 Example 2: Monitoring the Maximum Displacement of a Bridge\nLeast-squares regression is a powerful tool for estimating unknown quantities in many scientific and engineering disciplines beyond Aerospace. Let‚Äôs instead say we are interested in accurately estimating the maximum displacement at the midpoint of a bridge under various input-conditions measured by sensors along the bridge. Suppose we can accurately gather the following inputs:\n\nLoad applied to the bridge, \\(L\\)\nTemperature at the midpoint of the bridge, \\(T\\)\nDominant frequency in the bridge, \\(f\\)\n\nWe are interested in quickly estimating the Maximum Displacement (mm), \\(D\\), from these measurements. As we can see, the context of our problem has changed entirely, but we can still fit it into our existing framework by defining the input vector \\(z=(L, T, f)^\\top\\) and output \\(y=D\\). If we take \\(N\\) measurements throughout the year under various environmental conditions, we can again form some training dataset \\(\\{(z_i, y_i)\\}_{i=1}^N\\) which we can use to define some feature-set \\(x(z)\\) and then solve for the optimal weights that map \\(x(z)\\) to \\(y\\) identically to how we did in Example 1.\n\n\n2.3.3 Example 3: Detecting Cardiac Arrhythmia using Health-Monitoring Devices\nMany health issues are often difficult to detect without extensive testing. However, with the increase in biometric data available from devices such as smartphones and smart-watches, it has become increasingly feasible to predict health conditions by leveraging this information. \\(V O_2\\) max is a measure of the maximum rate at which a person can use oxygen during exercising and is considered a standard measure of cardiovascular fitness. However, to measure this experimentally requires an extensive laboratory test that measures the volume of oxygen a patient breathes during exercising. Let‚Äôs now suppose we have developed a smart-watch that gathers the following health-data from its user:\n\nMedian daily Step-Count \\(s\\)\nResting Heart Rate \\(r\\)\nRespiratory Rate \\(b\\)\n\nFrom these measurements, we are interested in accurately estimating the \\(VO_2\\) max of a patient. Let‚Äôs say we have a large sample of many patients of various ages and demographics who have worn our smart-watch for a long period of time. For the \\(i\\) th patient, suppose we have \\(s_i\\), \\(r_i\\) and \\(b_i\\) collected from the device and we have measured each patient‚Äôs actual \\(VO_2\\) max in a laboratory setting, which we denote \\(y_i\\). We then form a training dataset, \\(\\{(z_i, y_i)\\}_{i=1}^N\\) and solve the least-squares regression problem using the discrete values for \\(y\\) exactly how we did in Example 1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "11_ls.html#solving-linear-least-squares-problems",
    "href": "11_ls.html#solving-linear-least-squares-problems",
    "title": "2¬† Linear Least Squares Problems",
    "section": "2.4 Solving linear least squares problems",
    "text": "2.4 Solving linear least squares problems\nLinear least squares problems are special because they have closed form solutions: that is, we can write an analytical expression for the optimum parameters \\(\\beta^*\\) in terms of the data. To do so, we are going to define a feature data matrix \\(X\\in\\mathbb{R}^{N\\times n}\\) and an output data vector \\(Y\\in\\mathbb{R}^N\\) as follows:\n\\[\nX = \\begin{pmatrix}\n- &x^\\top(z_1) & - \\\\\n& \\vdots & \\\\\n- & x^\\top(z_N) & -\n\\end{pmatrix},\n\\qquad\nY = \\begin{pmatrix}\ny_1 \\\\ \\vdots \\\\ y_N\n\\end{pmatrix},\n\\] where the rows of \\(X\\) and the elements of \\(Y\\) correspond to input-output pairs in the data set. Note that each column of \\(X\\) corresponds to a different feature defined by an element of the vector-valued function \\(x\\).\nUsing this notation, Equation¬†2.2 can be rewritten as\n\\[\n\\beta^* = \\arg\\min_{\\beta\\in\\mathbb{R}^n} \\frac1N\\|X \\beta - Y\\|^2\n\\qquad(2.3)\\]\nThis is the minimization of a multivariate function (because \\(\\beta\\) is a vector). Recall from calculus that to find minimizers of multivariate functions we first seek critical points that satisfy the first-order necessary conditions for optimality: that is, we look for points where the derivative of the objective function is 0. Let \\(\\mathcal{L}(\\beta) = \\frac1N\\|X\\beta-Y\\|^2 = \\frac1N(X\\beta - Y)^\\top (X\\beta-Y)\\). Then,\n\\[\n\\frac{\\partial\\mathcal{L}}{\\partial \\beta} = \\frac2N(X^\\top X\\beta - X^\\top Y)\n\\]\nSetting the derivative equal to 0 yields the standard normal equations1:\n\\[\nX^\\top X\\beta^* = X^\\top Y\n\\]\nIf \\((X^\\top X)\\) is invertible, then the unique solution to the normal equations is given by\n\\[\n\\beta^* = (X^\\top X)^{-1} X^\\top Y\n\\]\nand this choice of \\(\\beta\\) defines our final learned model: \\(f(z;\\beta^*) = x(z)^\\top\\beta^*\\).\n\n2.4.1 Using Python for Least-Squares Regression\nNow let‚Äôs explore how we can actually solve for our optimal weights, \\(\\beta^*\\), given some training data with code. First, we need to import the libraries we need to work with matrices/vectors (numpy), plot results (matplotlib), work with dataframes (pandas), and easily perform more math than base python allows like \\(\\pi\\), sines, cosines, and square-roots (math).\n\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport pandas as pd\nimport math\n\nNow let‚Äôs say we‚Äôve imported some pandas dataframe called df that contains a set of empirical data from Example 1 (Assume it comes from experimental trials or simulations). We can print the first few rows of the dataset using the df.head() method:\n\ndf.head()\n\n\n\n\n\n\n\n\nalpha\nrho\nvelocity\nf_drag\n\n\n\n\n0\n16.854305\n1.319114\n79.481130\n9425.204935\n\n\n1\n42.782144\n1.203298\n50.263709\n13722.379248\n\n\n2\n32.939727\n1.320528\n57.690388\n12863.441979\n\n\n3\n26.939632\n0.696729\n49.251769\n3644.076532\n\n\n4\n7.020839\n1.227098\n19.524299\n186.612122\n\n\n\n\n\n\n\nLet‚Äôs define our input features as \\(x(z) = (\\alpha, \\rho, v)^\\top\\). We can now form \\(X\\) (our training inputs) and \\(Y\\) (our training outputs) matrices by extracting the input features and outputs from the dataframe:\n\n# The hstack method concatenates column vectors into \n# an N x d matrix like [1 alpha rho velocity]\nX = np.hstack((np.ones((N, 1)), df[['alpha', 'rho', 'velocity']].values))\nY = df['f_drag'].values\n\nWe can solve for \\(\\beta^*\\) using the np.linalg.lstsq() function which efficiently solves the least-squares equation \\(X \\beta = Y\\). The rcond keyword argument specifies how ‚Äúwell-conditioned‚Äù we want our result to be, achieved by cutting off the smallest singular values of \\(X\\) (don‚Äôt worry too much about this for now). Setting it to None assumes \\(X\\) is well-conditioned i.e.¬†far from being rank-deficient. By default, the lstsq() function returns a list of the optimal weights, \\(\\beta^*\\), the residuals of the training data (\\(X \\beta - Y\\)), the rank of \\(X\\), and the singular values of \\(X\\). For now, we‚Äôre only interested in \\(\\beta^*\\), so we use [0] to select this value.\n\nbeta_star = np.linalg.lstsq(X, Y, rcond=None)[0]\n\nWe are now able to compute the model‚Äôs predictions at the training inputs:\n\nY_hat = X @ beta_star\n\nLet‚Äôs now plot \\(\\hat{{Y}}\\) and \\({Y}\\) to examine how closely correlated the two are. A straight line with slope 1 would mean the linear regression model exactly matched all of the training outputs:\n\nplt.figure(figsize=(8,4))\nplt.scatter(Y, Y_hat)\nplt.grid()\nplt.xlabel(\"True Drag Force\")\nplt.ylabel(\"Model Predicted Drag Force\")\n\nText(0, 0.5, 'Model Predicted Drag Force')\n\n\n\n\n\n\n\n\n\nThis plot shows that there is clearly a strong, but nonlinear relationship between the true and model-predicted drag-force. We observe that the linear regression model under-predicts the drag-force when the true force is very low or very high and the model over-predicts the drag-force when the true force is around 5000N. This shows that when the relationship between the input features and the output is nonlinear, linear regression may provide good estimates on average, but choosing features to better capture this nonlinearity may provide much more accurate outputs over the entire input domain.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "11_ls.html#assessing-the-learned-models",
    "href": "11_ls.html#assessing-the-learned-models",
    "title": "2¬† Linear Least Squares Problems",
    "section": "2.5 Assessing the learned models",
    "text": "2.5 Assessing the learned models\n\n2.5.1 Common Performance Metrics\nOnce we have trained a model (in the linear regression case, we have computed the optimal \\(\\beta\\) that maps our training inputs to our training outputs), it‚Äôs important to have some quantitative way to measure how ‚Äúwell‚Äù our model is able to do this. There are many, many ways to measure model performance. For regression problems, the popular sklearn python package for machine learning has thirteen different metrics to assess model performance. We will outline some of the most prominent metrics in this section.\nA natural way to assess model performance is to measure the error between the model‚Äôs predictions and actual outputs. One common way to do this is computed is with mean squared error:\n\\[\n\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2\n\\]\nwhere \\(\\hat{y}_i\\) is the model‚Äôs best approximation of \\(y_i\\), using inputs \\(x_i\\). Because of how linear regression is formulated, the estimator \\(\\beta^*\\) will be the set of weights that minimize the MSE of the training data. If \\(X\\) is full-rank, \\(\\beta^*\\) will be unique. However, many data-scientists dislike the MSE because it penalizes higher errors more than lower errors and in the context of physical experiments the squared error doesn‚Äôt have easily interpretable units. For instance, in Example 1 we sought to predict drag-force in Newtons. Using the MSE would produce an error with units of (Newtons)\\(^2\\) which doesn‚Äôt have an intuitive physical interpretation. For this reason, many scientists use the mean absolute error:\n\\[\n\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^N |\\hat{y}_i - y_i|\n\\]\nIn Example 1, this would give us an error with units of Newtons that we can interpret as ‚Äúthe linear regression model deviates from the true outputs by ___ Newtons on average‚Äù. If the output data significantly varies in magnitude over the domain of the input, then we may expect MSE and MAE to dominate in places where the magnitude of \\(y\\) is large. To combat this problem, we might choose to use mean relative error to assess the performance of our model. This scales the error at each input/output pair according to how large the output is:\n\\[\n\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N \\frac{|\\hat{y}_i - y_i|}{|y_i|}\n\\]\nAt the end of 2.4, we plotted each training output, \\(y_i\\), on the x-axis of a plot and the model‚Äôs best guess (\\(\\hat{y}_i\\)) on the y-axis. If the model was able to perfectly match each \\(y_i\\), then we should expect a perfect line with a slope of 1 (\\(\\hat{y}=y\\)). Using the pearson correlation coefficient, often denoted \\(R\\) or the coefficient of determination, \\(R^2\\), we can assess how accurately the model is able to match a given set of outputs. We can compute this as:\n\\[\nR = \\frac{\\sum (\\hat{y}_i -\\bar{\\hat{y}})(y_i - \\bar{y})}{\\sqrt{\\sum (\\hat{y}_i -\\bar{\\hat{y}})^2 \\sum (y_i - \\bar{y})^2}}\n\\]\nWhen \\(\\hat{y}\\) perfectly matches the true outputs, \\(y\\), we get a pearson correlation coefficient of \\(R=R^2=1.0\\). Anything lower indicates an imperfect match between the model‚Äôs predictions and the true outputs.\n\n\n2.5.2 Cross-Validation\nUp to now, we have only discussed model performance as it relates to the data used to train the model. However, the entire reason we go to the trouble of training machine learning models is so they can be used to make accurate predictions on unseen data. Famous LLMs like ChatGPT are so valuable because we can ask them questions they have never seen before and they can still provide coherent, useful answers.\nOftentimes, when machine learning models are highly expressive, (i.e.¬†can model complex functional relationships) they tend to overfit to their training data. This happens when the model is so good at matching its training data, it does not generalize well to data it hasn‚Äôt seen before. This can be especially problematic when our training data contains some sort of noise or uncertainty and our trained model begins to conform to the noise present in the training data.\nA straightforward method to assess how much a model is overfitting its training data is to form a train and test set. Usually around 80% of the training data is used to train the model and about 20% is kept out of the training process and used to evaluate the model‚Äôs ability to generalize to data it hasn‚Äôt seen before. A model that generalizes well should exhibit approximately the same performance on its testing data as its training data. When a model shows significantly worse performance on its test dataset, this indicates that the model might be overfitting its training data.\nLet‚Äôs refer back to Example 1, when we were trying to accurately predict the force of drag on an airfoil. Say we break up the training data (X and Y) into (Xtrain, Ytrain) and (Xtest, Ytest); one dataset to train the model and one to evaluate it. Suppose we receive the following values for Mean Absolute Error on the training and test predictions:\n\nTraining MAE: 61.456 N\nTesting  MAE: 595.328 N\n\nThis shows that the model exhibits significantly lower error on its training data, and much higher on data it hasn‚Äôt seen before. This is typically a dead giveaway of overfitting.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "11_ls.html#exercises",
    "href": "11_ls.html#exercises",
    "title": "2¬† Linear Least Squares Problems",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\n\nRecall that the loss-function for least-squares regression is given by \\(\\mathcal{L}(\\beta) = \\frac 1N|| X \\beta - Y||_2^2\\). Show that the gradient of \\(\\mathcal{L}\\) with respect to \\(\\beta\\) is given by \\(\\nabla_\\beta \\mathcal{L} = \\frac2N \\left( X^\\top X \\beta - X^\\top Y\\right)\\)\nHint: you may find the following identities from the Matrix Cookbook (Petersen and Pedersen 2012) useful. For \\(v,u\\in\\mathbb{R}^n\\) and \\(A\\in\\mathbb{R}^{n\\times n}\\), the following hold:\n\n\\(\\nabla_v (v^\\top A v) = 2Av\\)\n\\(\\nabla_v (v^\\top u) = \\nabla_v(u^\\top v) = u\\)\n\nFor an extra-thorough learning experience, try deriving the above identities from the definitions of matrix-vector and dot products.\n\nConsider a matrix \\(A \\in \\mathbb{R}^{n \\times m}\\). Which of the following are necessary conditions for \\(A\\) to be invertible? Which are sufficient?\n\n\\(A\\) is a square matrix, i.e.¬†\\(n = m\\)\n\nThe columns of \\(A\\) span \\(\\mathbb{R}^{n}\\)\nThe columns of \\(A\\) are linearly independent\n\\(A\\) has no linearly dependent columns\nThe null-space of \\(A\\) is empty\nThe only solution of \\(A X = 0\\) is \\(X = 0\\). \n\n\n\n\nShow that \\(X^\\top X\\) is invertible if and only if \\(X\\) has full column-rank. \n\n\n\nExplain why \\(\\beta^*\\) is the unique solution to the normal equations if \\((X^\\top X)\\) is invertible. If \\((X^\\top X)\\) is not invertible, describe all solutions to the normal equations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "11_ls.html#further-reading",
    "href": "11_ls.html#further-reading",
    "title": "2¬† Linear Least Squares Problems",
    "section": "2.7 Further reading",
    "text": "2.7 Further reading\nLinear Algebra Crash Course (Section A.1)\nThe Matrix Cookbook (Petersen and Pedersen (2012))\n\n\n\n\n\nPetersen, Kaare Brandt, and Michael Syskind Pedersen. 2012. ‚ÄúThe Matrix Cookbook.‚Äù https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "11_ls.html#footnotes",
    "href": "11_ls.html#footnotes",
    "title": "2¬† Linear Least Squares Problems",
    "section": "",
    "text": "note that the constant factor \\(\\frac2N\\) drops out. It‚Äôs common to play fast and loose with multiplicative constants in minimizations ‚Äì other sources define the minimization objective with a \\(\\frac12\\) multiplier, which leads to the derivative not having the factor of 2 in front and leads to the same critical point. Or you may see the objective function defined without the \\(1/N\\) in front, which still leads to the same critical point.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Linear Least Squares Problems</span>"
    ]
  },
  {
    "objectID": "00_linalg.html",
    "href": "00_linalg.html",
    "title": "Appendix A ‚Äî Linear algebra review",
    "section": "",
    "text": "A.1 Scalars, Vectors, and Matrices",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Linear algebra review</span>"
    ]
  },
  {
    "objectID": "00_linalg.html#sec-linalg",
    "href": "00_linalg.html#sec-linalg",
    "title": "Appendix A ‚Äî Linear algebra review",
    "section": "",
    "text": "A.1.1 Notation\nLet‚Äôs introduce some notation: \\(\\mathbb{R}\\) is the set of real numbers, \\(\\mathbb{C}\\) is the set of complex numbers, and the operator \\(\\in\\) designates that a variable belongs to a set. To define a real scalar (i.e.¬†just a number), \\(c\\), we write:\n\\[ c \\in \\mathbb{R}\\]\nYou can read this as ‚Äúthe variable \\(c\\) belongs to the set of 1-dimensional real numbers.‚Äù\nTo define a real \\(n\\) dimensional vector, \\(\\mathbf{x}\\), we write:\n\\[ \\mathbf{x}  = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\in \\mathbb{R}^n \\]\nTo define a \\(m \\times n\\) matrix \\(\\mathbf{A}\\) of real-valued entries, we write:\n\\[ \\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & & a_{2n} \\\\\n\\vdots & & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n }\n\\]\n\n\nA.1.2 The Transpose\nThe ‚Äútranspose‚Äù of a matrix or vector, swaps the rows and columns. In other words, the first row becomes the first column, the second row the second column and so-on:\n\\[ \\mathbf{x}^\\top = \\begin{bmatrix} x_1 & \\dots & x_n \\end{bmatrix} \\in \\mathbb{R}^{1 \\times n} \\]\n\\[ \\mathbf{A}^\\top = \\begin{bmatrix} a_{11} & a_{21} & \\dots & a_{m1} \\\\\na_{12} & a_{22} & & a_{m2} \\\\\n\\vdots & & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\dots & a_{mn}\n\\end{bmatrix} \\in \\mathbb{R}^{n \\times m } \\]\nTransposing the product of matrices reverses their order:\n\\[ \\left( \\mathbf{A} \\mathbf{B} \\mathbf{C} \\mathbf{D} \\right)^\\top = \\mathbf{D}^\\top \\mathbf{C}^\\top \\mathbf{B}^\\top \\mathbf{A}^\\top \\]\n\n\nA.1.3 Addition & subtraction\nTechnically, only objects of the same dimensionality can be added or subtracted with one another. So, scalars can only be added to scalars, vectors can only be added to vectors of the same dimension, and matrices can only be added to other matrices with the same numbers of rows and columns. Some software like MATLAB or NumPy might let you add scalars to vectors and matrices and under the hood they multiply that scalar by a vector/matrix of ones to make the dimensions match.\n\n\n\n\n\n\nExamples of numpy allowing mathematically invalid operations\n\n\n\n\n\nMathematically, none of the following are valid operations. However, numpy allows us to do them by secretly adding in a vector of ones so that the vector/matrix dimensions are valid. This can be confusing for new programmers.\nAdding a scalar to a vector\n\nimport numpy as np \n# Defining a vector \nu = np.array([\n  1, 2, 3\n])\n\n# Adding a scalar to a vector\nprint(u + 5)\n# What numpy does to make the dimensions match\nprint(u + 5*np.ones_like(u)) \n\n[6 7 8]\n[6 7 8]\n\n\nAdding a row-vector to a column-vector\n\n# Reshaping u to a column-vector \nu = u.reshape((3,1))\n\n# Defining a row-vector, v \nv = np.array([\n  4, 5, 6\n]).reshape((1,3)) \n\n# Adding the row-vector v to a column-vector u\nprint(v + u)\n# What numpy does to make the dimensions match\nprint(np.outer(u, np.ones(3)) + np.outer(np.ones(3), v)) \n\n[[5 6 7]\n [6 7 8]\n [7 8 9]]\n[[5. 6. 7.]\n [6. 7. 8.]\n [7. 8. 9.]]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Linear algebra review</span>"
    ]
  },
  {
    "objectID": "00_linalg.html#scalar-multiplication",
    "href": "00_linalg.html#scalar-multiplication",
    "title": "Appendix A ‚Äî Linear algebra review",
    "section": "A.2 Scalar Multiplication",
    "text": "A.2 Scalar Multiplication\nGenerally, scalars can multiply by any structure (scalar, vector, and matrix) and do not change its dimension. They only ‚Äúscale‚Äù its value by a certain amount. Hence, multiplying a scalar by a scalar returns a scalar, multiplying a scalar times a vector returns a vector, and multiplying a scalar by a matrix returns a matrix. Scalar multiplication is also commutative, i.e.¬†\\(c\\mathbf{A} = \\mathbf{A}c\\) for all scalars \\(c\\). Consider our scalar, \\(c\\), from the previous section, another scalar, \\(b\\), the vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\) and the matrix \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\)\n\nA.2.1 Scalar-Scalar Product\n\\[ b \\times c = bc \\in \\mathbb{R} \\]\n\n\nA.2.2 Scalar-Vector Product\n\\[ c \\mathbf{x} = \\mathbf{x} c = \\begin{bmatrix} c x_1 \\\\ \\vdots \\\\ c x_n \\end{bmatrix} \\in \\mathbb{R}^n \\]\n\n\nA.2.3 Scalar-Matrix Product\n\\[ c \\mathbf{A} = \\mathbf{A} c = \\begin{bmatrix} c a_{11} & c a_{12} & \\dots & c a_{1n} \\\\\nc a_{21} & c a_{22} & & c a_{2n} \\\\\n\\vdots & & \\ddots & \\vdots \\\\\nc a_{m1} & c a_{m2} & \\dots & c a_{mn}\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n } \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Linear algebra review</span>"
    ]
  },
  {
    "objectID": "00_linalg.html#matrix-vector-multiplication",
    "href": "00_linalg.html#matrix-vector-multiplication",
    "title": "Appendix A ‚Äî Linear algebra review",
    "section": "A.3 Matrix & Vector Multiplication",
    "text": "A.3 Matrix & Vector Multiplication\n\nA.3.1 Vector-Vector Multiplication\nTo multiply two matrices (or two vectors, or a matrix with a vector), their inner dimensions must always match. Hence, the only valid way to multiply two vectors is by ‚Äúinner‚Äù or ‚Äúouter‚Äù products. Consider two vectors, \\(\\mathbf{u} \\in \\mathbb{R}^n\\) and \\(x \\in \\mathbb{R}^n\\). An ‚ÄúInner Product‚Äù (sometimes called the Dot-Product) is defined as:\n\\[ \\mathbf{u}^\\top \\mathbf{x} = \\begin{bmatrix} u_1 & \\dots & u_n \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = u_1 x_1 + u_2 x_2 + \\dots u_n x_n \\in \\mathbb{R}\\]\nSome important notes about inner products:\n\nIf the dimensions of \\(\\mathbf{u}\\) and \\(\\mathbf{x}\\) do not match, we cannot multiply them this way as we need to multiply each entry of both vectors.\nThis operation returns a scalar value.\nInner Products are commutative, i.e.¬†\\(\\mathbf{u}^\\top \\mathbf{x} = \\mathbf{x}^\\top \\mathbf{u}\\), as they are sums of scalar-scalar multiplications which are also commutative.\n\nRecall \\(\\mathbf{x} \\in \\mathbb{R}^n\\). Now consider a new vector \\(\\mathbf{w} \\in \\mathbb{R}^m\\) where \\(m \\neq n\\). An ‚ÄúOuter Product‚Äù is defined as:\n\\[ \\mathbf{w} \\mathbf{x}^\\top = \\begin{bmatrix} w_1 \\\\ \\vdots \\\\ w_m \\end{bmatrix} \\mathbf{x}^\\top = \\begin{bmatrix} w_1 \\mathbf{x}^\\top  \\\\ \\vdots \\\\ w_m \\mathbf{x}^\\top  \\end{bmatrix} = \\begin{bmatrix} w_1 x_1 & w_1 x_2 & \\dots & w_1 x_n \\\\ w_2 x_1 & w_2 x_2 & & w_2 x_n \\\\ \\vdots & & \\ddots & \\vdots \\\\ w_m x_1 & w_m x_2 & \\dots & w_m x_n \\end{bmatrix} \\in \\mathbb{R}^{m \\times n} \\]\nImportant notes on outer products:\n\nOuter products return matrices with dimensions according to the vectors multiplied.\nAs long as \\(\\mathbf{w}\\) and \\(\\mathbf{x}\\) are vectors, outer products share an inner dimension of \\(1\\), which means that any two vectors have an outer product.\nLastly, this operation is non-commutative, meaning \\(\\mathbf{w} \\mathbf{x}^\\top \\neq \\mathbf{x} \\mathbf{w}^\\top\\). Rather, these two are transposes of each other.\n\n\n\nA.3.2 Matrix-Vector Multiplication\nMultiplying a matrix with a vector means the dimension of the vector must equal the number of columns of the matrix. Recall \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) and \\(\\mathbf{x} \\in \\mathbb{R}^n\\). Note that the columns of \\(\\mathbf{A}\\) match the dimension of \\(\\mathbf{x}\\), which are both size \\(n\\). The product \\(\\mathbf{A}\\mathbf{x}\\) can be written as:\n\\[ \\mathbf{A}\\mathbf{x} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & & a_{2n} \\\\\n\\vdots & & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix} \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} a_{11} x_1 + a_{12} x_2 + \\dots + a_{1n} x_n \\\\  a_{21} x_1 + a_{22} x_2 + \\dots + a_{2n} x_n \\\\ \\vdots \\\\a_{m1} x_1 + a_{m2} x_2 + \\dots + a_{mn} x_n  \\end{bmatrix} \\in \\mathbb{R}^m \\]\nPerhaps a more useful way to visualize this is to break \\(\\mathbf{A}\\) down into its constituent columns and show this as a sum of scalar-vector multiplications:\n\\[ \\mathbf{A}\\mathbf{x} = \\begin{bmatrix} | & | &  & | \\\\\n\\mathbf{a}_1 & \\mathbf{a}_2 & \\dots & \\mathbf{a}_n \\\\\n| & | & & | \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} | \\\\ \\mathbf{a}_1 \\\\ | \\end{bmatrix} x_1 + \\begin{bmatrix} | \\\\ \\mathbf{a}_2 \\\\ | \\end{bmatrix} x_2 + \\dots + \\begin{bmatrix} | \\\\ \\mathbf{a}_n \\\\ | \\end{bmatrix} x_n  \\in \\mathbb{R}^m \\]\nwhere \\(\\mathbf{a}_i\\) is the \\(i\\)th column of \\(\\mathbf{A}\\). Note that matrix-vector multiplication forms a linear map from one dimensional vector-space to another. In this case, \\(\\mathbf{A}\\mathbf{x}\\) maps a vector from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\). This linear transformation from one dimensionality to another is a core component of many machine learning algorithms.\n\n\nA.3.3 Matrix-Matrix Multiplication\nConsider two matrices, \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{n \\times k}\\), where \\(m \\neq k\\). Again, the inner dimensions must match to multiply matrices. Because \\(\\mathbf{A}\\) multiplied by \\(\\mathbf{B}\\) have an inner-dimension of \\(n\\), they can be multiplied. Similar to matrix-vector multiplication, we can visualize the product of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) by breaking up the columns and rows of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), respectively:\n\\[ \\mathbf{A}\\mathbf{B} = \\begin{bmatrix} | & | &  & | \\\\\n\\mathbf{a}_1 & \\mathbf{a}_2 & \\dots & \\mathbf{a}_n \\\\\n| & | & & | \\end{bmatrix} \\begin{bmatrix} - & \\mathbf{b}_1 & - \\\\ - & \\mathbf{b}_2 & - \\\\ & \\vdots & \\\\  - & \\mathbf{b}_n & - \\end{bmatrix} \\in \\mathbb{R}^{m \\times k}\\]\nwhere \\(\\mathbf{a}_i\\) is the \\(i\\)th column of \\(\\mathbf{A}\\) and \\(\\mathbf{b}_i\\) is the \\(i\\)th row of \\(\\mathbf{B}\\). We can expand this expression by multiplying each column of \\(\\mathbf{A}\\) with each row of \\(\\mathbf{B}\\):\n\\[ \\mathbf{A}\\mathbf{B} = \\begin{bmatrix} | \\\\ \\mathbf{a}_1 \\\\ | \\end{bmatrix} \\begin{bmatrix} - & \\mathbf{b}_1 & - \\end{bmatrix} +\\begin{bmatrix} | \\\\ \\mathbf{a}_2 \\\\ | \\end{bmatrix} \\begin{bmatrix} - & \\mathbf{b}_2 & - \\end{bmatrix} + \\dots + \\begin{bmatrix} | \\\\ \\mathbf{a}_n \\\\ | \\end{bmatrix} \\begin{bmatrix} - & \\mathbf{b}_n & - \\end{bmatrix}\\]\nAs we can see, this is a sum of outer products (vectors multiplied by transposed vectors). We know \\(\\mathbf{A}\\) has \\(m\\) rows and \\(\\mathbf{B}\\) has \\(k\\) columns, so each outer product will form a matrix of dimension \\(m \\times k\\):\n\\[ \\mathbf{A}\\mathbf{B} = \\sum_{i=1}^n \\mathbf{a}_i \\mathbf{b}_i \\in \\mathbb{R}^{m \\times k}\\]\nNote: matrix multiplication is generally non-commutative; The product \\(\\mathbf{A}\\mathbf{B}\\) is valid, because the number of columns of \\(\\mathbf{A}\\) matches the number of rows of \\(\\mathbf{B}\\). However, the product \\(\\mathbf{B}\\mathbf{A}\\) is not valid, because the number of columns of \\(\\mathbf{B}\\) does not equal the number of rows of \\(\\mathbf{A}\\). The product \\(\\mathbf{B}\\mathbf{A}\\) can be multiplied if \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are both square matrices of the same size. However, even if the dimensions do match, generally speaking, \\(\\mathbf{BA \\neq AB}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Linear algebra review</span>"
    ]
  },
  {
    "objectID": "00_linalg.html#vector-norms",
    "href": "00_linalg.html#vector-norms",
    "title": "Appendix A ‚Äî Linear algebra review",
    "section": "A.4 Vector Norms",
    "text": "A.4 Vector Norms\nOftentimes, it is useful to know how ‚Äúbig‚Äù a vector is. And there are many different ways of computing this. The ‚Äúnorm‚Äù of an object is a measure of how ‚Äúlarge‚Äù it is, according to some rule. Consider \\(\\mathbf{x} \\in \\mathbb{R}^n\\). We‚Äôll start with the most common norm:\n\nA.4.1 The 2-Norm\nThe 2-norm of \\(x\\) (which we will use most-often in this class) is defined as the squareroot of the squares of its entries:\n\\[ ||\\mathbf{x}||_2 = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2} \\]\nThe reason the 2-norm is so useful is the fact that the square of the \\(l_2\\) norm is an inner product of a vector with itself:\n\\[ \\mathbf{x}^\\top \\mathbf{x} = x_1^2 + x_2^2 + \\dots + x_3^2 = ||\\mathbf{x}||_2^2 \\]\nThis helps us write a norm in terms of matrix-vector operations, which is extremely useful for optimization. The 2-norm is also quite important because it is the first \\(l_p\\) norm with a continuous derivative, which cannot be said of the 1-norm since it uses absolute-value functions.\n\n\nA.4.2 The \\(l_p\\)-Norm\nWe can take the definition of the 2-Norm and generalize it to describe a class of norms known as \\(l_p\\) norms. The \\(l_p\\) norm of a vector is defined by the following:\n\\[ ||\\mathbf{x}||_p = \\left( \\sum_{i=1}^n |x_i|^p \\right)^{1/p} \\]\n\n\nA.4.3 The 1-Norm\nThe 1-norm of \\(\\mathbf{x}\\) is simply the sum of the absolute-value of its entries:\n\\[ ||\\mathbf{x}||_1 = |x_1| + |x_2| + \\dots + |x_n| \\]\n\n\nA.4.4 The Infinity-Norm\nThe infinity-norm takes the limit as \\(p \\rightarrow \\infty\\) and when this limit is solved, this norm simply becomes the entry of \\(x\\) with the largest absolute value:\n\\[ ||\\mathbf{x}||_\\infty = \\max \\{ |x_1|, |x_2|, \\dots, |x_n| \\} \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Linear algebra review</span>"
    ]
  },
  {
    "objectID": "00_linalg.html#rank-rangecolumn-space-and-null-spaces",
    "href": "00_linalg.html#rank-rangecolumn-space-and-null-spaces",
    "title": "Appendix A ‚Äî Linear algebra review",
    "section": "A.5 Rank, Range/Column-Space and Null-Spaces",
    "text": "A.5 Rank, Range/Column-Space and Null-Spaces\n\nA.5.1 Linear Combinations and Span\nMany of the following ideas hinge upon the idea of ‚ÄúLinear Combinations‚Äù of vectors. Consider two vectors \\(\\mathbf{u}, \\mathbf{v}\\in \\mathbb{R}^{d}\\). A linear combination of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is the vector produced by the following:\n\\[ \\mathbf{w}= c_1 \\mathbf{u}+ c_2 \\mathbf{v}\\]\nwhere \\(c_1\\) and \\(c_2\\) are scalars. We can generalize this idea by saying that all possible realizations of \\(\\mathbf{w}\\) live in the vector-space \\(\\mathcal{W}\\) defined by the ‚Äúspan‚Äù by \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). This is denoted:\n\\[ \\mathbf{w}\\in \\mathcal{W} = \\text{Span}\\left( \\mathbf{u}, \\mathbf{v}\\right) \\]\nWe can read this in plain english as ‚Äúthe vector \\(\\mathbf{w}\\) belongs to the vector-space \\(\\mathcal{W}\\) which is defined by all possible linear combinations of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\)‚Äù.\n\n\nA.5.2 Linear-Independence\nThis is a fundamental idea in Linear Algebra. A set of vectors is said to be ‚ÄúLinearly Independent‚Äù if no vector in this set can be reconstructed with any of the others. In other words, no one vector can be reproduced by scaling or combining any of the others. For example, consider the following three vectors:\n\\[ \\{ \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\} \\]\nBecause each vector in this set points in its own unique direction, it is impossible to reconstruct one of these vectors as a linear combination of any others. This may be true with more complex vectors that aren‚Äôt unit vectors along the dimensions of the vector-space.\n\n\nA.5.3 Rank\nTechnically, there are two types of rank: Column Rank and Row-Rank. In this class, when we say ‚Äúrank‚Äù, we will be referring to the column rank.\nThe column rank of a matrix is simply its number of linearly independent column-vectors (the row-rank is the number of linearly independent row-vectors). Consider the following matrix:\n\\[ \\mathbf{A} = \\begin{bmatrix} 1 & 2 & 1 & 4 \\\\ 0 & 1 & 1 & 2 \\\\ 0 & 0 & 2 & 0 \\end{bmatrix} \\]\nThe first-three columns of \\(\\mathbf{A}\\) are linearly independent because they cannot be reproduced using linear combinations of the other columns. However, the last column is a multiple of column 2, which means it can be reproduced using a linear combination of columns. Hence, this matrix has a rank of 3.\nTheorem: No matrix \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) where \\(m &lt; n\\) can be full-rank. This is because only \\(m\\) vectors can span \\(\\mathbb{R}^m\\), and since there are more than \\(m\\) columns, we must have some redundancy.\n\n\nA.5.4 Range/Column-Space\nThe Range, Column-Space or Image of a matrix, \\(\\mathbf{A}\\), is the span of all possible combinations of its columns. In human words, it describes the space of \\(\\mathbf{Ax}\\) for all possible \\(\\mathbf{x}\\). Consider the following matrix:\n\\[ \\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\\\ 2 & 4  \\end{bmatrix} \\]\nNote that the second column is a multiple of the first column, hence, the range of \\(\\mathbf{A}\\) is:\n\\[ \\text{Range}(\\mathbf{A}) = \\text{span} \\left( \\begin{bmatrix} 1 \\\\ 3 \\\\ 2 \\end{bmatrix}\\right) \\]\n\n\nA.5.5 Null-Space\nThe Null-Space of a matrix, \\(\\mathbf{A}\\) is the set of all nonzero vectors, \\(\\mathbf{x}\\) such that \\(\\mathbf{Ax=0}\\). Consider a simple example:\n\\[ \\mathbf{A} = \\begin{bmatrix} 1 & 0 & -1\\\\ 0 & 1 & 1   \\end{bmatrix} \\]\nWe can find the set of \\(\\mathbf{x}\\) that produce the zero-vector by solving the following equation:\n\\[ \\begin{bmatrix} 1 & 0 & -1\\\\ 0 & 1 & 1   \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\]\nThis produces the systems of equations:\n\\[ x_1 - x_3 = 0 \\] \\[ x_2 + x_3 = 0 \\] \\[ x_3 = x_3 \\]\nIf we isolate each entry of \\(\\mathbf{x}\\):\n\\[ x_1 = x_3 \\] \\[ x_2 = -x_3 \\] \\[ x_3 = x_3 \\]\nHence, we see that \\(x_3\\) can be any scalar and satisfy this systems of equations:\n\\[ \\mathbf{x} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\end{bmatrix} x_3 = \\text{span}\\left( \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\end{bmatrix} \\right) = \\text{Null}(\\mathbf{A})\\]\n\n\nA.5.6 The Identity Matrix\nThe identity matrix is a square, \\(n\\)-dimensional matrix consisting of ones on its diagonal and zeros elsewhere. It is called the identity matrix because it preserves the identity when multiplied by matrices and vectors.\n\\[ \\mathbf{I} = \\begin{bmatrix} 1 & 0 & \\dots & 0 \\\\ 0 & 1  & & 0 \\\\ \\vdots & & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & 1 \\end{bmatrix} \\]\nConsider \\(\\mathbf{I} \\in \\mathbb{R}^{n \\times n}\\) to be an \\(n \\times n\\) identity matrix. If we recall \\(\\mathbf{A} \\in \\mathbb{m \\times n}\\) and \\(\\mathbf{x} \\in \\mathbb{R}^n\\):\n\\[ \\mathbf{A I = I A = A} \\]\n\\[ \\mathbf{I x = x } \\]\n\\[ \\mathbf{x^\\top I = x^\\top} \\]\n\n\n\n\n\n\nWhy is the identity matrix useful?\n\n\n\nThe identity matrix simplifies expressions involving matrices. If we can transform a term into an identity matrix, then we can reduce the expression to only the remaining terms.\n\n\n\n\nA.5.7 Matrix Inverses\nYou may have noticed that, thus far, we have covered the addition, subtraction, and multiplication of matrices, which conspicuously leaves division. Well, unfortunately, matrix division is complex to say the least. For square matrices, division is accomplished via matrix inversion. However, not all matrices are invertible.\nDefinition: A matrix \\(\\mathbf{B}\\) is called the ‚Äúinverse‚Äù of a square matrix \\(\\mathbf{A}\\) if and only if the following condition holds:\n\\[ \\mathbf{A} \\mathbf{B} = \\mathbf{B} \\mathbf{A} = \\mathbf{I} \\]\nwhere \\(\\mathbf{I}\\) is the identity matrix. Such a \\(\\mathbf{B}\\) is denoted \\(\\mathbf{A}^{-1}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Linear algebra review</span>"
    ]
  },
  {
    "objectID": "00_linalg.html#further-reading",
    "href": "00_linalg.html#further-reading",
    "title": "Appendix A ‚Äî Linear algebra review",
    "section": "A.6 Further Reading",
    "text": "A.6 Further Reading\n\nA great read for a deeper dive into the relationship between Column, Row and Null-spaces of matrices: Gil Strang‚Äôs Fundamental Theorem of Linear Algebra",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Linear algebra review</span>"
    ]
  }
]