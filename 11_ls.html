<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Linear Least Squares Problems – AE 4803 AIM: Course Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./00_linalg.html" rel="next">
<link href="./01_intro.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./11_ls.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Least Squares Problems</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AE 4803 AIM: Course Notes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_ls.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Least Squares Problems</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00_linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Linear algebra review</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#classifying-regression-problems-as-linear-vs-nonlinear" id="toc-classifying-regression-problems-as-linear-vs-nonlinear" class="nav-link active" data-scroll-target="#classifying-regression-problems-as-linear-vs-nonlinear"><span class="header-section-number">2.1</span> Classifying regression problems as linear vs nonlinear</a></li>
  <li><a href="#mathematical-problem-formulation" id="toc-mathematical-problem-formulation" class="nav-link" data-scroll-target="#mathematical-problem-formulation"><span class="header-section-number">2.2</span> Mathematical problem formulation</a></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples"><span class="header-section-number">2.3</span> Examples</a>
  <ul class="collapse">
  <li><a href="#airfoil-example" id="toc-airfoil-example" class="nav-link" data-scroll-target="#airfoil-example"><span class="header-section-number">2.3.1</span> Example 1: Aerodynamic drag prediction</a></li>
  <li><a href="#example-2-monitoring-the-maximum-displacement-of-a-bridge" id="toc-example-2-monitoring-the-maximum-displacement-of-a-bridge" class="nav-link" data-scroll-target="#example-2-monitoring-the-maximum-displacement-of-a-bridge"><span class="header-section-number">2.3.2</span> Example 2: Monitoring the Maximum Displacement of a Bridge</a></li>
  <li><a href="#example-3-detecting-cardiac-arrhythmia-using-health-monitoring-devices" id="toc-example-3-detecting-cardiac-arrhythmia-using-health-monitoring-devices" class="nav-link" data-scroll-target="#example-3-detecting-cardiac-arrhythmia-using-health-monitoring-devices"><span class="header-section-number">2.3.3</span> Example 3: Detecting Cardiac Arrhythmia using Health-Monitoring Devices</a></li>
  </ul></li>
  <li><a href="#solving-linear-least-squares-problems" id="toc-solving-linear-least-squares-problems" class="nav-link" data-scroll-target="#solving-linear-least-squares-problems"><span class="header-section-number">2.4</span> Solving linear least squares problems</a>
  <ul class="collapse">
  <li><a href="#using-python-for-least-squares-regression" id="toc-using-python-for-least-squares-regression" class="nav-link" data-scroll-target="#using-python-for-least-squares-regression"><span class="header-section-number">2.4.1</span> Using Python for Least-Squares Regression</a></li>
  </ul></li>
  <li><a href="#assessing-the-learned-models" id="toc-assessing-the-learned-models" class="nav-link" data-scroll-target="#assessing-the-learned-models"><span class="header-section-number">2.5</span> Assessing the learned models</a>
  <ul class="collapse">
  <li><a href="#common-performance-metrics" id="toc-common-performance-metrics" class="nav-link" data-scroll-target="#common-performance-metrics"><span class="header-section-number">2.5.1</span> Common Performance Metrics</a></li>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation"><span class="header-section-number">2.5.2</span> Cross-Validation</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">2.6</span> Exercises</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">2.7</span> Further reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Least Squares Problems</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="hidden">
<!-- \renewcommand{\P}{\mathrm{P}} -->
<!-- $$
\DeclarePairedDelimiters{\set}{\{}{\}}
\DeclareMathOperator*{\argmax}{argmax}
$$

\definecolor{quarto-callout-note-color}{HTML}{4477AA} -->
</div>
<p>We begin our exploration of scientific machine learning methods in the fundamental setting of <em>linear regression problems</em>. The intended learning outcomes of these notes are that students should be able to:</p>
<ol type="1">
<li>Classify a regression problem as linear or nonlinear</li>
<li>Understand the abstract form of linear least squares problems
<ol type="a">
<li>Define the abstract form of linear least squares problems and be able to explain what its key ingredients are</li>
<li>Translate a description of a model learning problem in words into a specific instance of the abstract form</li>
<li>Construct examples of linear least squares problems, cast them in the abstract form, and explain their context</li>
</ol></li>
<li>Solve linear least-squares problems by deriving and solving the normal equations.</li>
<li>Evaluate linear regression models by computing and interpreting mean error, mean relative error, and <span class="math inline">\(R^2\)</span> values on both training and test data sets.</li>
</ol>
<section id="classifying-regression-problems-as-linear-vs-nonlinear" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="classifying-regression-problems-as-linear-vs-nonlinear"><span class="header-section-number">2.1</span> Classifying regression problems as linear vs nonlinear</h2>
<p>We have previously introduced regression problems in a general way: recall that the three ingredients of (parametrized) regression problems are (1) paired input and output data, (2) the choice of a parametrized model class, and (3) a method for choosing a model from within that class. The classification of a regression problem as <em>linear</em> or <em>nonlinear</em> depends solely on ingredient (2), the parametrized model class: if the models in that class depend linearly on the model parameters, the regression problem is a linear regression problem.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check your knowledge: Do you remember what it means for a function to depend linearly on a variable?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The function <span class="math inline">\(f(z;\theta): \mathbb{R}^d\times\Theta\to\mathbb{R}\)</span> is said to be <em>linear</em> in the parameters <span class="math inline">\(\theta\)</span> if, for all <span class="math inline">\(a,b\in\mathbb{R}\)</span> and all <span class="math inline">\(\theta_1,\theta_2\in\Theta\)</span>, the following holds: <span class="math inline">\(f(z; a\theta_1 + b\theta_2) = af(z;\theta_1) + bf(z;\theta_2)\)</span>.</p>
<p>Note that when we say a function is “linear”, we have to specify in <em>what</em>. That is, we can also say <span class="math inline">\(f(z;\theta)\)</span> is linear in the <em>inputs</em> if, for all <span class="math inline">\(a,b\in\mathbb{R}\)</span> and all <span class="math inline">\(z_1,z_2\in\mathbb{R}^d\)</span>, the following holds: <span class="math inline">\(f(az_1 + bz_2;\theta) = af(z_1;\theta)+bf(z_2;\theta)\)</span>.</p>
</div>
</div>
</div>
<p>The classification of regression problems as linear or nonlinear depends <strong>solely</strong> on the dependence of the functions in the parametrized model class on the <strong>parameters</strong>. That is, we can define functions that are <em>linear</em> in <span class="math inline">\(\theta\)</span> while being <em>nonlinear</em> in <span class="math inline">\(z\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider the model classes (<a href="01_intro.html#eq-quad-models" class="quarto-xref">Equation&nbsp;<span>1.1</span></a>)-(<a href="01_intro.html#eq-perceptron" class="quarto-xref">Equation&nbsp;<span>1.3</span></a>) introduced previously. Are these model classes linear or nonlinear in the parameters? In the inputs?</p>
</div>
</div>
</section>
<section id="mathematical-problem-formulation" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="mathematical-problem-formulation"><span class="header-section-number">2.2</span> Mathematical problem formulation</h2>
<p>We are now going to introduce an <em>abstract</em> mathematical problem formulation that can be used to describe many <em>specific instances</em> of linear regression problems. This is a theme of the course and throughout computational mathematics and engineering: abstraction using the language of mathematics lets us isolate the core essence of the problem we’re solving and develop powerful algorithms that can solve specific applications of those problems across a wide range of disciplines. I’ll introduce the abstract formulation first, and follow it up with some specific examples.</p>
<p>Ingredient 1 (the data set): let <span class="math inline">\(\{(z_i,y_i)\}_{i=1}^N\)</span> be a given data set of paired inputs <span class="math inline">\(z_i\in\mathbb{R}^d\)</span> and outputs <span class="math inline">\(y_i\in\mathbb{R}\)</span>.</p>
<p>Ingredient 2 (the parametrized model class): let <span class="math inline">\(x:\mathbb{R}^d\to\mathbb{R}^n\)</span> be a function that maps the <span class="math inline">\(d\)</span>-dimensional input to an <span class="math inline">\(n\)</span>-dimensional <em>feature vector</em>. For a fixed <span class="math inline">\(x\)</span>, we will consider the following parametrized model class:</p>
<p><span id="eq-linreg-model-class"><span class="math display">\[
\mathcal{F}_\beta := \{ x(z)^\top \beta : \beta\in\mathbb{R}^n\}
\qquad(2.1)\]</span></span></p>
<p>Recall that <a href="#eq-linreg-model-class" class="quarto-xref">Equation&nbsp;<span>2.1</span></a> is read as “<span class="math inline">\(\mathcal{F}_\beta\)</span> is defined to be the set of all functions <span class="math inline">\(f(z;\beta) = x(z)^\top\beta\)</span> for all <span class="math inline">\(\beta\in\mathbb{R}^n\)</span>.” This is an abstract way to define the model class for <em>any</em> linear regression problem, as we will describe in more detail shortly.</p>
<p>Ingredient 3 (the method of choosing the parameters): let <span class="math inline">\(\beta^*\)</span> be given by</p>
<p><span id="eq-linreg-minimization"><span class="math display">\[
\begin{aligned}
\beta^* &amp;= \arg\min_{\beta\in\mathbb{R}^n} \frac1N \sum_{i=1}^N (f(z_i;\beta) - y_i)^2 \\
&amp;= \arg\min_{\beta\in\mathbb{R}^N} \frac1N \sum_{i=1}^N (x(z_i)^\top\beta - y_i)^2.
\end{aligned}
\qquad(2.2)\]</span></span> Then, we define the <em>learned model</em> to be <span class="math inline">\(f(z;\beta^*) = x(z)^\top\beta^*\)</span>. We call <span class="math inline">\(\beta^*\)</span> defined this way the “optimal regression parameters” or just the “optimal parameters”, because they are the result of solving an optimization problem. Note that the objective of this optimization function is <em>defined by the data</em>, and represents the average squared error of the model over the data set.</p>
<p>Taken together, the three ingredients I have defined above define a <em>linear least squares problem</em>, which is a subclass of linear regression problems. We’ll now give several examples of specific instances of linear least squares problems to illustrate how broadly applicable this abstract framework is.</p>
</section>
<section id="examples" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="examples"><span class="header-section-number">2.3</span> Examples</h2>
<section id="airfoil-example" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="airfoil-example"><span class="header-section-number">2.3.1</span> Example 1: Aerodynamic drag prediction</h3>
<!-- ![Image Source: [@erauAirfoil2024]](images/AirfoilDrag.png){fig-align="center" width=80%} -->
<p>An important task in aerodynamic design is predicting lift and drag forces on a body moving through air. Let’s consider a simplified problem where we are given an fixed airfoil design and our goal is to predict the drag force <span class="math inline">\(F_d\)</span> on the airfoil as a function of three parameters which describe its flight conditions:</p>
<ul>
<li>The angle of attack <span class="math inline">\(\alpha\)</span></li>
<li>The density of the fluid <span class="math inline">\(\rho\)</span></li>
<li>The freestream velocity of the air <span class="math inline">\(v\)</span></li>
</ul>
<p>To put this problem in our abstract framework, we define <span class="math inline">\(z = (\alpha,\rho,v)^\top\)</span> to be a three-dimensional input, and take the output to be the drag force <span class="math inline">\(y= F_d\)</span>. In order to define a regression problem, we require the existence of a data set <span class="math inline">\(\{(z_i,y_i)\}_{i=1}^N\)</span>. Note that in this case <span class="math inline">\(z_i = (\alpha_i,\rho_i,v_i)\)</span>. We assume that this data set is given.</p>
<p>In linear regression problems, defining the model class amounts to choosing a set of regression <em>features</em> by defining <span class="math inline">\(x\)</span>. A simple choice takes the inputs themselves to be features: <span class="math inline">\(x^{(1)}(z) = z = (\alpha,\rho,v)^\top\)</span>. This leads to a class of parametrized models with a three-dimensional unknown parameter vector <span class="math inline">\(\beta\in\mathbb{R}^3\)</span>. The models in this class have the following form:</p>
<p><span class="math display">\[
f^{(1)}(z;\beta) = x^{(1)}(z)^\top\beta = (\alpha,\rho,v) \beta = \beta_1\alpha + \beta_2\rho + \beta_3 v.
\]</span></p>
<p>There are many other possible choices. For example, consider <span class="math inline">\(x^{(2)}(z) = (\alpha,\rho, v, \alpha^2, \rho^2, v^2, \alpha\rho, \alpha v, \rho v)^\top\)</span>. This leads to a parametrized model class with a <em>nine</em>-dimensional unknown parameter vector <span class="math inline">\(\beta\in\mathbb{R}^9\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
f^{(2)}(z;\beta) &amp;= x^{(2)}(z)^\top\beta = (\alpha,\rho, v, \alpha^2, \rho^2, v^2, \alpha\rho, \alpha v, \rho v)\beta \\
&amp;= \beta_1\alpha + \beta_2\rho + \beta_3 v + \beta_4\alpha^2 + \beta_5\rho^2 + \beta_6 v^2 + \beta_7\alpha\rho + \beta_8\alpha v + \beta_9 \rho v
\end{aligned}
\]</span></p>
<p>For either the above choices of features <span class="math inline">\(x^{(1)}(z)\)</span> or <span class="math inline">\(x^{(2)}(z)\)</span>, we could then define and solve the minimization <a href="#eq-linreg-minimization" class="quarto-xref">Equation&nbsp;<span>2.2</span></a> to find the optimal regression parameters and define our learned model <span class="math inline">\(f^{(1)}(z;\beta^*)\)</span> or <span class="math inline">\(f^{(2)}(z;\beta^*)\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Understanding notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that we use <span class="math inline">\(\beta\)</span> to denote the unknown parameters in both <span class="math inline">\(f^{(1)}\)</span> and <span class="math inline">\(f^{(2)}\)</span> above despite <span class="math inline">\(\beta\)</span> referring to different quantities in the definition of the different functions. This is a common notational shortcut — while we could use the notation <span class="math inline">\(\beta^{(1)}\in\mathbb{R}^3\)</span> and <span class="math inline">\(\beta^{(2)}\in\mathbb{R}^9\)</span> to specify the different <span class="math inline">\(\beta\)</span> for the different functions, this can be cumbersome if we are considering many different options for the choice of features <span class="math inline">\(x(z)\)</span>, and it’s standard to just use <span class="math inline">\(\beta\)</span>, where the definition of <span class="math inline">\(\beta\)</span> is implied by the context. One of the challenges in learning about machine learning and computational mathematics more generally is getting used to similar notation meaning different things in different contexts. That’s one of the things that we’ll practice in this course.</p>
</div>
</div>
</section>
<section id="example-2-monitoring-the-maximum-displacement-of-a-bridge" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="example-2-monitoring-the-maximum-displacement-of-a-bridge"><span class="header-section-number">2.3.2</span> Example 2: Monitoring the Maximum Displacement of a Bridge</h3>
<p>Least-squares regression is a powerful tool for estimating unknown quantities in many scientific and engineering disciplines beyond Aerospace. Let’s instead say we are interested in accurately estimating the maximum displacement at the midpoint of a bridge under various input-conditions measured by sensors along the bridge. Suppose we can accurately gather the following inputs:</p>
<ul>
<li>Load applied to the bridge, <span class="math inline">\(L\)</span></li>
<li>Temperature at the midpoint of the bridge, <span class="math inline">\(T\)</span></li>
<li>Dominant frequency in the bridge, <span class="math inline">\(f\)</span></li>
</ul>
<p>We are interested in quickly estimating the Maximum Displacement (mm), <span class="math inline">\(D\)</span>, from these measurements. As we can see, the context of our problem has changed entirely, but we can still fit it into our existing framework by defining the input vector <span class="math inline">\(z=(L, T, f)^\top\)</span> and output <span class="math inline">\(y=D\)</span>. If we take <span class="math inline">\(N\)</span> measurements throughout the year under various environmental conditions, we can again form some training dataset <span class="math inline">\(\{(z_i, y_i)\}_{i=1}^N\)</span> which we can use to define some feature-set <span class="math inline">\(x(z)\)</span> and then solve for the optimal weights that map <span class="math inline">\(x(z)\)</span> to <span class="math inline">\(y\)</span> identically to how we did in <a href="#airfoil-example">Example 1</a>.</p>
</section>
<section id="example-3-detecting-cardiac-arrhythmia-using-health-monitoring-devices" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="example-3-detecting-cardiac-arrhythmia-using-health-monitoring-devices"><span class="header-section-number">2.3.3</span> Example 3: Detecting Cardiac Arrhythmia using Health-Monitoring Devices</h3>
<p>Many health issues are often difficult to detect without extensive testing. However, with the increase in biometric data available from devices such as smartphones and smart-watches, it has become increasingly feasible to predict health conditions by leveraging this information. <span class="math inline">\(V O_2\)</span> max is a measure of the maximum rate at which a person can use oxygen during exercising and is considered a standard measure of cardiovascular fitness. However, to measure this experimentally requires an extensive laboratory test that measures the volume of oxygen a patient breathes during exercising. Let’s now suppose we have developed a smart-watch that gathers the following health-data from its user:</p>
<ul>
<li>Median daily Step-Count <span class="math inline">\(s\)</span></li>
<li>Resting Heart Rate <span class="math inline">\(r\)</span></li>
<li>Respiratory Rate <span class="math inline">\(b\)</span></li>
</ul>
<p>From these measurements, we are interested in accurately estimating the <span class="math inline">\(VO_2\)</span> max of a patient. Let’s say we have a large sample of many patients of various ages and demographics who have worn our smart-watch for a long period of time. For the <span class="math inline">\(i\)</span> th patient, suppose we have <span class="math inline">\(s_i\)</span>, <span class="math inline">\(r_i\)</span> and <span class="math inline">\(b_i\)</span> collected from the device and we have measured each patient’s actual <span class="math inline">\(VO_2\)</span> max in a laboratory setting, which we denote <span class="math inline">\(y_i\)</span>. We then form a training dataset, <span class="math inline">\(\{(z_i, y_i)\}_{i=1}^N\)</span> and solve the least-squares regression problem using the discrete values for <span class="math inline">\(y\)</span> exactly how we did in <a href="#airfoil-example">Example 1</a>.</p>
</section>
</section>
<section id="solving-linear-least-squares-problems" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="solving-linear-least-squares-problems"><span class="header-section-number">2.4</span> Solving linear least squares problems</h2>
<p>Linear least squares problems are special because they have closed form solutions: that is, we can write an analytical expression for the optimum parameters <span class="math inline">\(\beta^*\)</span> in terms of the data. To do so, we are going to define a feature data matrix <span class="math inline">\(X\in\mathbb{R}^{N\times n}\)</span> and an output data vector <span class="math inline">\(Y\in\mathbb{R}^N\)</span> as follows:</p>
<p><span class="math display">\[
X = \begin{pmatrix}
- &amp;x^\top(z_1) &amp; - \\
&amp; \vdots &amp; \\
- &amp; x^\top(z_N) &amp; -
\end{pmatrix},
\qquad
Y = \begin{pmatrix}
y_1 \\ \vdots \\ y_N
\end{pmatrix},
\]</span> where the <em>rows</em> of <span class="math inline">\(X\)</span> and the <em>elements</em> of <span class="math inline">\(Y\)</span> correspond to input-output pairs in the data set. Note that each <em>column</em> of <span class="math inline">\(X\)</span> corresponds to a different <em>feature</em> defined by an element of the vector-valued function <span class="math inline">\(x\)</span>.</p>
<p>Using this notation, <a href="#eq-linreg-minimization" class="quarto-xref">Equation&nbsp;<span>2.2</span></a> can be rewritten as</p>
<p><span id="eq-linreg-matrix-min"><span class="math display">\[
\beta^* = \arg\min_{\beta\in\mathbb{R}^n} \frac1N\|X \beta - Y\|^2
\qquad(2.3)\]</span></span></p>
<p>This is the minimization of a multivariate function (because <span class="math inline">\(\beta\)</span> is a vector). Recall from calculus that to find minimizers of multivariate functions we first seek <em>critical points</em> that satisfy the <em>first-order necessary conditions</em> for optimality: that is, we look for points where the derivative of the objective function is 0. Let <span class="math inline">\(\mathcal{L}(\beta) = \frac1N\|X\beta-Y\|^2 = \frac1N(X\beta - Y)^\top (X\beta-Y)\)</span>. Then,</p>
<p><span class="math display">\[
\frac{\partial\mathcal{L}}{\partial \beta} = \frac2N(X^\top X\beta - X^\top Y)
\]</span></p>
<p>Setting the derivative equal to 0 yields the standard <em>normal equations</em><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>:</p>
<p><span class="math display">\[
X^\top X\beta^* = X^\top Y
\]</span></p>
<p>If <span class="math inline">\((X^\top X)\)</span> is invertible, then the unique solution to the normal equations is given by</p>
<p><span class="math display">\[
\beta^* = (X^\top X)^{-1} X^\top Y
\]</span></p>
<p>and this choice of <span class="math inline">\(\beta\)</span> defines our final learned model: <span class="math inline">\(f(z;\beta^*) = x(z)^\top\beta^*\)</span>.</p>
<section id="using-python-for-least-squares-regression" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="using-python-for-least-squares-regression"><span class="header-section-number">2.4.1</span> Using Python for Least-Squares Regression</h3>
<p>Now let’s explore how we can actually solve for our optimal weights, <span class="math inline">\(\beta^*\)</span>, given some training data with code. First, we need to import the libraries we need to work with matrices/vectors (<code>numpy</code>), plot results (<code>matplotlib</code>), work with dataframes (<code>pandas</code>), and easily perform more math than base python allows like <span class="math inline">\(\pi\)</span>, sines, cosines, and square-roots (<code>math</code>).</p>
<div id="c6f2575e" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s say we’ve imported some pandas dataframe called <code>df</code> that contains a set of empirical data from <a href="#airfoil-example">Example 1</a> (Assume it comes from experimental trials or simulations). We can print the first few rows of the dataset using the <code>df.head()</code> method:</p>
<div id="21070da2" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">alpha</th>
<th data-quarto-table-cell-role="th">rho</th>
<th data-quarto-table-cell-role="th">velocity</th>
<th data-quarto-table-cell-role="th">f_drag</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>16.854305</td>
<td>1.319114</td>
<td>79.481130</td>
<td>9425.204935</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>42.782144</td>
<td>1.203298</td>
<td>50.263709</td>
<td>13722.379248</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>32.939727</td>
<td>1.320528</td>
<td>57.690388</td>
<td>12863.441979</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>26.939632</td>
<td>0.696729</td>
<td>49.251769</td>
<td>3644.076532</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>7.020839</td>
<td>1.227098</td>
<td>19.524299</td>
<td>186.612122</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Let’s define our input features as <span class="math inline">\(x(z) = (\alpha, \rho, v)^\top\)</span>. We can now form <span class="math inline">\(X\)</span> (our training inputs) and <span class="math inline">\(Y\)</span> (our training outputs) matrices by extracting the input features and outputs from the dataframe:</p>
<div id="391d02ab" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The hstack method concatenates column vectors into </span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># an N x d matrix like [1 alpha rho velocity]</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.hstack((np.ones((N, <span class="dv">1</span>)), df[[<span class="st">'alpha'</span>, <span class="st">'rho'</span>, <span class="st">'velocity'</span>]].values))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> df[<span class="st">'f_drag'</span>].values</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can solve for <span class="math inline">\(\beta^*\)</span> using the <code>np.linalg.lstsq()</code> function which efficiently solves the least-squares equation <span class="math inline">\(X \beta = Y\)</span>. The <code>rcond</code> keyword argument specifies how “well-conditioned” we want our result to be, achieved by cutting off the smallest singular values of <span class="math inline">\(X\)</span> (don’t worry too much about this for now). Setting it to <code>None</code> assumes <span class="math inline">\(X\)</span> is well-conditioned i.e.&nbsp;far from being rank-deficient. By default, the <code>lstsq()</code> function returns a list of the optimal weights, <span class="math inline">\(\beta^*\)</span>, the residuals of the training data (<span class="math inline">\(X \beta - Y\)</span>), the rank of <span class="math inline">\(X\)</span>, and the singular values of <span class="math inline">\(X\)</span>. For now, we’re only interested in <span class="math inline">\(\beta^*\)</span>, so we use <code>[0]</code> to select this value.</p>
<div id="491f3d66" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>beta_star <span class="op">=</span> np.linalg.lstsq(X, Y, rcond<span class="op">=</span><span class="va">None</span>)[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We are now able to compute the model’s predictions at the training inputs:</p>
<div id="f9c5bb11" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>Y_hat <span class="op">=</span> X <span class="op">@</span> beta_star</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s now plot <span class="math inline">\(\hat{{Y}}\)</span> and <span class="math inline">\({Y}\)</span> to examine how closely correlated the two are. A straight line with slope 1 would mean the linear regression model exactly matched all of the training outputs:</p>
<div id="ec8d2861" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">4</span>))</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(Y, Y_hat)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"True Drag Force"</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Model Predicted Drag Force"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>Text(0, 0.5, 'Model Predicted Drag Force')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="11_ls_files/figure-html/cell-8-output-2.png" width="697" height="356" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This plot shows that there is clearly a strong, but nonlinear relationship between the true and model-predicted drag-force. We observe that the linear regression model under-predicts the drag-force when the true force is very low or very high and the model over-predicts the drag-force when the true force is around 5000N. This shows that when the relationship between the input features and the output is nonlinear, linear regression may provide good estimates on average, but choosing features to better capture this nonlinearity may provide much more accurate outputs over the entire input domain.</p>
</section>
</section>
<section id="assessing-the-learned-models" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="assessing-the-learned-models"><span class="header-section-number">2.5</span> Assessing the learned models</h2>
<section id="common-performance-metrics" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="common-performance-metrics"><span class="header-section-number">2.5.1</span> Common Performance Metrics</h3>
<p>Once we have trained a model (in the linear regression case, we have computed the optimal <span class="math inline">\(\beta\)</span> that maps our training inputs to our training outputs), it’s important to have some quantitative way to measure how “well” our model is able to do this. There are many, many ways to measure model performance. For regression problems, the popular <code>sklearn</code> python package for machine learning has thirteen different metrics to assess model performance. We will outline some of the most prominent metrics in this section.</p>
<p>A natural way to assess model performance is to measure the error between the model’s predictions and actual outputs. One common way to do this is computed is with <em>mean squared error</em>:</p>
<p><span class="math display">\[
\text{MSE} = \frac{1}{N} \sum_{i=1}^N (\hat{y}_i - y_i)^2
\]</span></p>
<p>where <span class="math inline">\(\hat{y}_i\)</span> is the model’s best approximation of <span class="math inline">\(y_i\)</span>, using inputs <span class="math inline">\(x_i\)</span>. Because of how linear regression is formulated, the estimator <span class="math inline">\(\beta^*\)</span> will be the set of weights that minimize the MSE of the training data. If <span class="math inline">\(X\)</span> is full-rank, <span class="math inline">\(\beta^*\)</span> will be unique. However, many data-scientists dislike the MSE because it penalizes higher errors more than lower errors and in the context of physical experiments the squared error doesn’t have easily interpretable units. For instance, in <a href="#airfoil-example">Example 1</a> we sought to predict drag-force in Newtons. Using the MSE would produce an error with units of (Newtons)<span class="math inline">\(^2\)</span> which doesn’t have an intuitive physical interpretation. For this reason, many scientists use the <em>mean absolute error</em>:</p>
<p><span class="math display">\[
\text{MAE} = \frac{1}{N} \sum_{i=1}^N |\hat{y}_i - y_i|
\]</span></p>
<p>In <a href="#airfoil-example">Example 1</a>, this would give us an error with units of Newtons that we can interpret as “the linear regression model deviates from the true outputs by ___ Newtons on average”. If the output data significantly varies in magnitude over the domain of the input, then we may expect MSE and MAE to dominate in places where the magnitude of <span class="math inline">\(y\)</span> is large. To combat this problem, we might choose to use <em>mean relative error</em> to assess the performance of our model. This scales the error at each input/output pair according to how large the output is:</p>
<p><span class="math display">\[
\text{MSE} = \frac{1}{N} \sum_{i=1}^N \frac{|\hat{y}_i - y_i|}{|y_i|}
\]</span></p>
<p>At the end of <strong>2.4</strong>, we plotted each training output, <span class="math inline">\(y_i\)</span>, on the x-axis of a plot and the model’s best guess (<span class="math inline">\(\hat{y}_i\)</span>) on the y-axis. If the model was able to perfectly match each <span class="math inline">\(y_i\)</span>, then we should expect a perfect line with a slope of 1 (<span class="math inline">\(\hat{y}=y\)</span>). Using the <em>pearson correlation coefficient</em>, often denoted <span class="math inline">\(R\)</span> or the <em>coefficient of determination</em>, <span class="math inline">\(R^2\)</span>, we can assess how accurately the model is able to match a given set of outputs. We can compute this as:</p>
<p><span class="math display">\[
R = \frac{\sum (\hat{y}_i -\bar{\hat{y}})(y_i - \bar{y})}{\sqrt{\sum (\hat{y}_i -\bar{\hat{y}})^2 \sum (y_i - \bar{y})^2}}
\]</span></p>
<p>When <span class="math inline">\(\hat{y}\)</span> perfectly matches the true outputs, <span class="math inline">\(y\)</span>, we get a pearson correlation coefficient of <span class="math inline">\(R=R^2=1.0\)</span>. Anything lower indicates an imperfect match between the model’s predictions and the true outputs.</p>
</section>
<section id="cross-validation" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="cross-validation"><span class="header-section-number">2.5.2</span> Cross-Validation</h3>
<p>Up to now, we have only discussed model performance as it relates to the data used to train the model. However, the entire reason we go to the trouble of training machine learning models is so they can be used to make accurate predictions on unseen data. Famous LLMs like ChatGPT are so valuable because we can ask them questions they have never seen before and they can still provide coherent, useful answers.</p>
<p>Oftentimes, when machine learning models are highly expressive, (i.e.&nbsp;can model complex functional relationships) they tend to <em>overfit</em> to their training data. This happens when the model is so good at matching its training data, it does not generalize well to data it hasn’t seen before. This can be especially problematic when our training data contains some sort of noise or uncertainty and our trained model begins to conform to the noise present in the training data.</p>
<p>A straightforward method to assess how much a model is overfitting its training data is to form a <code>train</code> and <code>test</code> set. Usually around 80% of the training data is used to train the model and about 20% is kept out of the training process and used to evaluate the model’s ability to generalize to data it hasn’t seen before. A model that generalizes well should exhibit approximately the same performance on its testing data as its training data. When a model shows significantly worse performance on its <code>test</code> dataset, this indicates that the model might be overfitting its training data.</p>
<p>Let’s refer back to <a href="#airfoil-example">Example 1</a>, when we were trying to accurately predict the force of drag on an airfoil. Say we break up the training data (<code>X</code> and <code>Y</code>) into (<code>Xtrain</code>, <code>Ytrain</code>) and (<code>Xtest</code>, <code>Ytest</code>); one dataset to train the model and one to evaluate it. Suppose we receive the following values for Mean Absolute Error on the training and test predictions:</p>
<ul>
<li><code>Training MAE: 61.456 N</code></li>
<li><code>Testing  MAE: 595.328 N</code></li>
</ul>
<p>This shows that the model exhibits significantly lower error on its training data, and much higher on data it hasn’t seen before. This is typically a dead giveaway of overfitting.</p>
</section>
</section>
<section id="exercises" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="exercises"><span class="header-section-number">2.6</span> Exercises</h2>
<ol type="1">
<li><p>Recall that the loss-function for least-squares regression is given by <span class="math inline">\(\mathcal{L}(\beta) = \frac 1N|| X \beta - Y||_2^2\)</span>. Show that the gradient of <span class="math inline">\(\mathcal{L}\)</span> with respect to <span class="math inline">\(\beta\)</span> is given by <span class="math inline">\(\nabla_\beta \mathcal{L} = \frac2N \left( X^\top X \beta - X^\top Y\right)\)</span></p>
<p><em>Hint:</em> you may find the following identities from the Matrix Cookbook <span class="citation" data-cites="petersen2012matrix">(<a href="#ref-petersen2012matrix" role="doc-biblioref">Petersen and Pedersen 2012</a>)</span> useful. For <span class="math inline">\(v,u\in\mathbb{R}^n\)</span> and <span class="math inline">\(A\in\mathbb{R}^{n\times n}\)</span>, the following hold:</p>
<ol type="i">
<li><span class="math inline">\(\nabla_v (v^\top A v) = 2Av\)</span></li>
<li><span class="math inline">\(\nabla_v (v^\top u) = \nabla_v(u^\top v) = u\)</span></li>
</ol>
<p>For an extra-thorough learning experience, try deriving the above identities from the definitions of matrix-vector and dot products.</p>
<p><!-- ::: {.callout-note collapse="true"}
 ## Solution 

 $$ \mathcal{L}(\beta) = ||X \beta - Y||_2^2 $$ 

 $$ \mathcal{L}(\beta) = (X \beta - Y)^\top (X \beta - Y) $$

 $$ \mathcal{L}(\beta) = (\beta^\top X^\top - Y^\top) (X \beta - Y) $$

 $$ \mathcal{L}(\beta) = \beta^\top X^\top X \beta - \beta^\top X^\top Y - Y^\top X \beta - Y^\top Y  $$

 $$ \mathcal{L}(\beta) = \beta^\top X^\top X \beta - 2 \beta^\top X^\top Y  - Y^\top Y  $$

 $$ \nabla (\beta^\top X^\top X \beta) = 2 X^\top X \beta $$ 

 $$ \nabla (2 \beta^\top X^\top Y) = 2 X^\top Y $$ 

 $$ \nabla (Y^\top Y) = 0 $$ 

 $$ \nabla \mathcal{L}_{\beta} = 2 X^\top X \beta - 2 X^\top Y + 0 = 2 \left( X^\top X \beta - X^\top Y  \right) $$ 


 :::  --></p></li>
<li><p>Consider a matrix <span class="math inline">\(A \in \mathbb{R}^{n \times m}\)</span>. Which of the following are necessary conditions for <span class="math inline">\(A\)</span> to be invertible? Which are sufficient?</p>
<ol type="a">
<li><span class="math inline">\(A\)</span> is a square matrix, i.e.&nbsp;<span class="math inline">\(n = m\)</span><br>
</li>
<li>The columns of <span class="math inline">\(A\)</span> span <span class="math inline">\(\mathbb{R}^{n}\)</span></li>
<li>The columns of <span class="math inline">\(A\)</span> are linearly independent</li>
<li><span class="math inline">\(A\)</span> has no linearly dependent columns</li>
<li>The null-space of <span class="math inline">\(A\)</span> is empty</li>
<li>The only solution of <span class="math inline">\(A X = 0\)</span> is <span class="math inline">\(X = 0\)</span>. <!-- What properties does $A$ need to satisfy in order for $A$ to be invertible? *Hint: there are many properties of a matrix that show it is uniquely invertible. Think dimensions, fundamental spaces, $A^{-1} A = ?$, etc.*  --></li>
</ol></li>
</ol>
<!-- :::{.callout-note collapse="true"}
## Solution 
The matrix $A$ must satisfy the following properties (many of these are different ways of saying the same thing): 

* $A$ is a square matrix, i.e. $n = m$ 
* A matrix $\bB$ exists such that $\bB A = A \bB = \bI$ where $\bI$ is an $n \times n$ identity matrix 
- The columns of $A$ span $\R^{n}$ 
- The columns of $A$ are linearly independent 
- $A$ has no linearly dependent columns 
- $A$ has no singular values equal to zero
- The null-space of $A$ is empty 
- The only solution of $A X = 0$ is $X = 0$. 
::: -->
<ol start="3" type="1">
<li>Show that <span class="math inline">\(X^\top X\)</span> is invertible if and only if <span class="math inline">\(X\)</span> has full column-rank. <!-- *Hint: a symmetric positive definite matrix is always invertible.* --></li>
</ol>
<!-- ::: {.callout-note collapse="true"}
## Solution 

Recall that a symmetric positive definite (SPD) matrix satisfies the following for all $\bv \neq 0$: 

$$ \bv^\top A \bv > 0 $$

If $X^\top X$ is SPD, then $X^\top X$ must be invertible. Hence, the following must hold for all $\bv \neq 0$: 

$$ \bv^\top X^\top X \bv > 0 $$ 

We can write this as the squared 2-norm of $X \bv$: 

$$ \bv^\top X^\top X \bv = || X \bv||_2^2 > 0$$ 

This is saying that any nonzero $\bv$ multiplied by $X$ must produce a vector with a norm greater than zero. In other words, $X \bv$ cannot produce the zero-vector when $\bv \neq 0$. This means that $X$ must have an empty null-space, which means that $X$ must have linearly independent columns which is another way of saying it has full column rank.
::: -->
<ol start="4" type="1">
<li>Explain why <span class="math inline">\(\beta^*\)</span> is the unique solution to the normal equations if <span class="math inline">\((X^\top X)\)</span> is invertible. If <span class="math inline">\((X^\top X)\)</span> is not invertible, describe all solutions to the normal equations.</li>
</ol>
<!-- $$ X^\top X \beta = X^\top Y $$

If $X^\top X$ is invertible (its columns are linearly independent, its null-space is empty, etc.), is it possible for more than one $\beta$ to satisfy this equation? Why? 

If $X^\top X$ is **not** invertible, how many values of $\beta$ satisfy this equation? Why? 

::: {.callout-note collapse="true"}
## Solution 

Solving $X^\top X \beta = X^\top Y$ can be simplified to solving the general equation: 

$$ A \beta = \bb$$ 

If $A$ is invertible (meaning it has full column-rank, its columns are linearly independent, it has an empty nullspace, etc.), then there exists no nonzero $\beta$ such that: 

$$ A \beta = 0 $$ 

However, if $A$ is not invertible, it has a non-empty null-space, which means there exist nonzero $\beta$ that **does** satisfy $A \beta = 0$. Let us refer to a $\beta$ in the null-space of $A$ as $\beta_0$. Because a null-space is defined by the linear combination of one or more vectors, there are infinitely many combinations of vectors in the null-space that can produce a $\beta_0$ such that $A \beta_0 = 0$. Let's say we find a $\beta$ in the column-space of $A$ that solves the least-squares equation: 

$$ A \beta = \bb $$ 

If $A$ has a non-empty null space, then we can take a $\beta_0$ in the null-space of $A$ and make the following substitution: 

$$ A (\beta + \beta_0) = A \beta + A \beta_0 = A \beta + 0 = A \beta $$ 

This means we can add in *any* $\beta_0$ from the null-space and still end up with a nonzero $\beta$ that minimizes the least squares equation. Because we can form infinitely many vectors from the null-space, this means there are infinitely many solutions to the least-squares equation when $A$ is not invertible. 
 -->
<!-- :::  -->
</section>
<section id="further-reading" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">2.7</span> Further reading</h2>
<p><em>Linear Algebra Crash Course (<a href="00_linalg.html#sec-linalg" class="quarto-xref"><span>Section A.1</span></a>)</em></p>
<p><em>The Matrix Cookbook (<span class="citation" data-cites="petersen2012matrix">Petersen and Pedersen (<a href="#ref-petersen2012matrix" role="doc-biblioref">2012</a>)</span>)</em></p>
<hr>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-petersen2012matrix" class="csl-entry" role="listitem">
Petersen, Kaare Brandt, and Michael Syskind Pedersen. 2012. <span>“The Matrix Cookbook.”</span> <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>note that the constant factor <span class="math inline">\(\frac2N\)</span> drops out. It’s common to play fast and loose with multiplicative constants in minimizations – other sources define the minimization objective with a <span class="math inline">\(\frac12\)</span> multiplier, which leads to the derivative not having the factor of 2 in front and leads to the same critical point. Or you may see the objective function defined without the <span class="math inline">\(1/N\)</span> in front, which still leads to the same critical point.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./01_intro.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./00_linalg.html" class="pagination-link" aria-label="Linear algebra review">
        <span class="nav-page-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Linear algebra review</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>